{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "outputs": [],
   "source": [
    "def g_sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def g_sigmoid_derivative(z):\n",
    "    return g_sigmoid(z) * (1 - g_sigmoid(z))\n",
    "\n",
    "def identity(x):\n",
    "    return x\n",
    "\n",
    "def hypothesis_neural(neural_model, input_data):\n",
    "    a_l = input_data\n",
    "    \n",
    "    for l in range(neural_model.shape[0]):\n",
    "        z_lp1 = neural_model[l].dot(a_l)\n",
    "        a_lp1 = g_sigmoid(z_lp1)\n",
    "        \n",
    "        if l != neural_model.shape[0] - 1:\n",
    "            a_lp1 = np.concatenate((np.array([1]), a_lp1))\n",
    "            \n",
    "        a_l = a_lp1\n",
    "        \n",
    "    return a_l[0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "outputs": [],
   "source": [
    "def unroll_matrix(matrix):\n",
    "    return matrix.ravel()\n",
    "\n",
    "def unroll_matrix_array(matrix_vec):\n",
    "    unrolled = np.array([])\n",
    "    for matrix in matrix_vec:\n",
    "        unrolled = np.concatenate((unrolled, unroll_matrix(matrix)), axis=None)\n",
    "        \n",
    "    return np.array(unrolled).ravel()\n",
    "\n",
    "def roll_vec_to_matrix(vec, matrix_size):\n",
    "    rows = matrix_size[0]\n",
    "    cols = matrix_size[1]\n",
    "    \n",
    "    if vec.shape[0] != rows * cols:\n",
    "        raise ValueError(f'Nekorektno razvijanje vektora velicine {vec.shape[0]} u matricu'\n",
    "                         f' dimenzija {rows}x{cols} (matrica od {rows*cols} elemenata)')\n",
    "    \n",
    "    matrix = np.zeros(matrix_size)\n",
    "    for i in range(rows):\n",
    "        matrix[i] = vec[i*cols : (i + 1)*cols]\n",
    "        \n",
    "    return matrix\n",
    "\n",
    "def roll_vec_to_matrix_array(long_vec, matrix_sizes):\n",
    "    matrix_array = []\n",
    "    prev_rows = 0\n",
    "    prev_cols = 0\n",
    "    \n",
    "    for matrix_size in matrix_sizes:\n",
    "        rows = matrix_size[0]\n",
    "        cols = matrix_size[1]\n",
    "        \n",
    "        i = prev_rows * prev_cols\n",
    "        j = rows * cols\n",
    "        matrix_array.append(roll_vec_to_matrix(long_vec[i : i+j], matrix_size))\n",
    "        \n",
    "        prev_rows = rows\n",
    "        prev_cols = cols\n",
    "        \n",
    "    return np.array(matrix_array)\n",
    "\n",
    "def get_matrix_sizes(matrix_array):\n",
    "    sizes = []\n",
    "    for i in range(matrix_array.shape[0]):\n",
    "        sizes.append(matrix_array[i].shape)\n",
    "        \n",
    "    return sizes"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "outputs": [],
   "source": [
    "def regularization(neural_model, N_set_size, lambda_param=0):\n",
    "    if lambda_param == 0:\n",
    "        return 0\n",
    "    \n",
    "    reg = 0\n",
    "    for l in range(neural_model.shape[0]):\n",
    "        for i in range(neural_model[l].shape[0]):\n",
    "            for j in range(1, neural_model[l].shape[1]):\n",
    "                reg += (neural_model[l][i][j])**2\n",
    "                \n",
    "    return lambda_param * reg / (2*N_set_size)\n",
    "\n",
    "def loss_logistic(X, y, neural_model, lambda_param=0):\n",
    "    N = y.shape[0]\n",
    "    loss = 0\n",
    "    \n",
    "    for i in range(N):\n",
    "        y_i = y[i][0]\n",
    "        y_i_predict = hypothesis_neural(neural_model, X[i])\n",
    "        \n",
    "        loss += y_i*np.log(y_i_predict) + (1 - y_i)*np.log(1 - y_i_predict)\n",
    "        \n",
    "    return -loss / N + regularization(neural_model, N, lambda_param)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def gradient_checking(X, y, neural_model, lambda_param=0, eps=10e-4):\n",
    "    w_unrolled = unroll_matrix_array(neural_model)\n",
    "    n = len(w_unrolled)\n",
    "    \n",
    "    grad_approx = np.zeros((n,))\n",
    "    matrix_sizes = get_matrix_sizes(neural_model)\n",
    "    \n",
    "    for i in range(n):\n",
    "        # OBAVEZNO MORA KOPIJA VEKTORA, inace imamo dve reference na isti vektor!!!\n",
    "        w_plus = w_unrolled.copy()\n",
    "        w_minus = w_unrolled.copy()\n",
    "        \n",
    "        w_plus[i] += eps\n",
    "        w_minus[i] -= eps\n",
    "        \n",
    "        w_plus_rolled = roll_vec_to_matrix_array(w_plus, matrix_sizes)\n",
    "        w_minus_rolled = roll_vec_to_matrix_array(w_minus, matrix_sizes)\n",
    "        \n",
    "        loss_plus = loss_logistic(X, y, w_plus_rolled, lambda_param)\n",
    "        loss_minus = loss_logistic(X, y, w_minus_rolled, lambda_param)\n",
    "        \n",
    "        grad_approx[i] = (loss_plus - loss_minus) / (2*eps)\n",
    "    \n",
    "    return grad_approx"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "outputs": [],
   "source": [
    "def gradient_descent(X, \n",
    "                     y, \n",
    "                     neural_network, \n",
    "                     alpha=0.01, \n",
    "                     num_iter=1000, \n",
    "                     lambda_param=0,\n",
    "                     plot=False):\n",
    "    \n",
    "    loss_history = np.zeros((num_iter, 1))\n",
    "    matrix_sizes = get_matrix_sizes(neural_network.model)\n",
    "    \n",
    "    # Podsetnik: neuralna mreza prilikom kreiranja vec ima inicijalni model.\n",
    "    # Ovde je dovoljno samo da ga razvijemo u vektor kako bi ga pripremili za\n",
    "    # algoritam gradijentnog spusta\n",
    "    w = neural_network.unroll_model()\n",
    "    for i in range(num_iter):\n",
    "        loss, gradient = neural_network.backward_propagation(X, y, lambda_param)\n",
    "        w = w - alpha * gradient\n",
    "        \n",
    "        # VRLO VAZAN KORAK: nakon sto je model azuriran, treba ga azurirati UNUTAR neuralne\n",
    "        # mreze kako bi backward propagation algoritam radio sa novim vrednostima modela.\n",
    "        neural_network.set_model(roll_vec_to_matrix_array(w, matrix_sizes))\n",
    "        \n",
    "        loss_history[i] = loss\n",
    "    \n",
    "    if plot:\n",
    "        plt.plot(range(num_iter), loss_history)\n",
    "\n",
    "        plt.xlabel('Iterations')\n",
    "        plt.ylabel('Loss')\n",
    "\n",
    "        plt.legend(['Loss_train'])\n",
    "        plt.title('Minimization of loss function via gradient descent')\n",
    "\n",
    "        plt.show()\n",
    "            \n",
    "    return loss_history, w"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self, input_layer_size, hidden_layer_options, output_layer_size, eps=10e-1):\n",
    "        self.input_layer_size = input_layer_size\n",
    "        self.hidden_layers_num = len(hidden_layer_options)\n",
    "        self.layers_num = self.hidden_layers_num + 2\n",
    "        self.output_layer_size = output_layer_size\n",
    "        self.layer_sizes = [input_layer_size] + hidden_layer_options + [output_layer_size]\n",
    "        self.layer_indices = range(self.layers_num)\n",
    "        \n",
    "        # NAPOMENA: velicina slojeva neuralne mreze (layer_sizes) se racuna kao broj jedinica\n",
    "        # BEZ bias jedinice, ali prilikom postavljanja slojeva mreza ocekuje bias jedinice.\n",
    "        # Dakle, svaki sloj je velicine za jedan vise, osim poslednjeg sloja.\n",
    "        layers = [np.ones(input_layer_size + 1)]\n",
    "        for i in range(self.hidden_layers_num):\n",
    "            layers.append(np.ones(hidden_layer_options[i] + 1))\n",
    "            \n",
    "        layers.append(np.ones(output_layer_size))\n",
    "        self.network = np.array(layers)\n",
    "        \n",
    "        # Cuvam dimenzije matrica Wij koje mapiraju slojeve j -> j + 1\n",
    "        # s(j+1) x (s(j) + 1)\n",
    "        mappers = {}\n",
    "        for l in range(self.layers_num - 1):\n",
    "           rows = self.layer_sizes[l + 1]\n",
    "           cols = self.layer_sizes[l] + 1\n",
    "           \n",
    "           mappers[l] = (rows, cols)\n",
    "        \n",
    "        self.layer_mapper_sizes = mappers\n",
    "        \n",
    "        # inicijalizacija modela, tj. matrica W_i za svaki sloj\n",
    "        ws = []\n",
    "        for l in range(self.layers_num - 1):\n",
    "            w_l = 2*eps * np.random.random(self.layer_mapper_sizes[l]) - eps\n",
    "            ws.append(w_l)\n",
    "            \n",
    "        self.model = np.array(ws)\n",
    "        self.model_trained = self.model\n",
    "        \n",
    "        # Analogno kao za slojeve, delte ocekuju bias jedinice, koje se NECE koristiti\n",
    "        # u proracunima backpropagation algoritma. Prvi sloj mora da ima\n",
    "        # podrazumevano za delte sve nule, jer ulazni podaci nemaju gresku.\n",
    "        deltas = []\n",
    "        for l in self.layer_indices:\n",
    "            if l == self.layers_num - 1:\n",
    "                delta_l = np.zeros(self.layer_sizes[l])\n",
    "            else:\n",
    "                delta_l = np.zeros(self.layer_sizes[l] + 1)\n",
    "                \n",
    "            deltas.append(delta_l)\n",
    "            \n",
    "        self.deltas = np.array(deltas)\n",
    "        \n",
    "    def __str__(self):\n",
    "        self.print_network()\n",
    "        \n",
    "    def __repr__(self):\n",
    "        self.print_network()\n",
    "        \n",
    "    def __layer_index_check(self, layer_index):\n",
    "        if layer_index < 0 or layer_index >= self.layers_num:\n",
    "            raise IndexError(f'Nekorektan indeks sloja neuralne mreze {layer_index}: '\n",
    "                             f'dostupni indeksi 0-{self.layers_num - 1}')\n",
    "        \n",
    "    def set_layer(self, layer_index, units_vec):\n",
    "        self.__layer_index_check(layer_index)\n",
    "        \n",
    "        # poslednji sloj NEMA bias unit\n",
    "        # -1 jer mreza inicijalno ne sadrzi bias unit, a ja ocekujem vektor koji sadrzi bias unit\n",
    "        if layer_index != self.layers_num - 1 and \\\n",
    "                units_vec.shape[0] != self.layer_sizes[layer_index] + 1:\n",
    "            error_message = f'Nekorektna dimenzija vektora {units_vec.shape[0]} za sloj {layer_index}: ' \\\n",
    "                f'ocekivana {self.layer_sizes[layer_index] + 1}'\n",
    "            \n",
    "            raise ValueError(error_message)\n",
    "        elif layer_index == self.layers_num - 1 and \\\n",
    "                units_vec.shape[0] != self.layer_sizes[layer_index]: # ovde je layer_index poslednji sloj\n",
    "            error_message = f'Nekorektna dimenzija vektora {units_vec.shape[0]} za izlazni sloj {layer_index}: ' \\\n",
    "                f'ocekivana {self.layer_sizes[layer_index]}'\n",
    "            \n",
    "            raise ValueError(error_message)\n",
    "        \n",
    "        self.network[layer_index] = units_vec\n",
    "        \n",
    "    def set_all_layers(self, all_layers):\n",
    "        for l in range(self.layers_num):\n",
    "            self.set_layer(l, all_layers[l])\n",
    "            \n",
    "    def set_mapper(self, layer_index, mapper):\n",
    "        self.__layer_index_check(layer_index)\n",
    "        \n",
    "        if mapper.shape != self.model[layer_index].shape:\n",
    "            error_message = f'Nekorektna dimenzija matrice {mapper.shape[0]}x{mapper.shape[1]} ' \\\n",
    "                f'za preslikavanje sloja {layer_index} -> {layer_index + 1}: ocekivana ' \\\n",
    "                f'{self.model[layer_index].shape[0]}x{self.model[layer_index].shape[1]}'\n",
    "            \n",
    "            raise ValueError(error_message)\n",
    "        \n",
    "        self.model[layer_index] = mapper\n",
    "        \n",
    "    # Funkcija po analogiji za slojeve, radi doslednosti\n",
    "    def set_all_mappers(self, all_mappers):\n",
    "        for l in range(self.layers_num - 1):\n",
    "            self.set_mapper(l, all_mappers[l])\n",
    "            \n",
    "    def set_model(self, all_mappers):\n",
    "        self.set_all_mappers(all_mappers)\n",
    "        \n",
    "    def set_delta(self, layer_index, delta_vec):\n",
    "        self.__layer_index_check(layer_index)\n",
    "        \n",
    "        if layer_index != self.layers_num - 1 and \\\n",
    "                delta_vec.shape[0] != self.layer_sizes[layer_index] + 1:\n",
    "            error_message = f'Nekorektna dimenzija vektora {delta_vec.shape[0]}:' \\\n",
    "                f'ocekivana {self.layer_sizes[layer_index] + 1}'\n",
    "            \n",
    "            raise ValueError(error_message)\n",
    "        elif layer_index == self.layers_num - 1 and \\\n",
    "                delta_vec.shape[0] != self.layer_sizes[layer_index]:\n",
    "            error_message = f'Nekorektna dimenzija vektora {delta_vec.shape[0]} za poslednji sloj: ' \\\n",
    "                f'ocekivana {self.layer_sizes[layer_index]}' \n",
    "            \n",
    "            raise ValueError(error_message)\n",
    "            \n",
    "        self.deltas[layer_index] = delta_vec\n",
    "        \n",
    "    def set_all_deltas(self, all_deltas):\n",
    "        for l in self.layer_indices:\n",
    "            self.set_delta(l, all_deltas[l])\n",
    "        \n",
    "    def unroll_mapper(self, layer_index):\n",
    "        self.__layer_index_check(layer_index)\n",
    "        \n",
    "        return self.model[layer_index].ravel()\n",
    "    \n",
    "    def unroll_model(self):\n",
    "        unrolled_model = np.array([])\n",
    "        for l in range(self.layers_num - 1):\n",
    "            W_l = self.unroll_mapper(l)\n",
    "            unrolled_model = np.concatenate((unrolled_model, W_l), axis=None)\n",
    "            \n",
    "        return np.array(unrolled_model).ravel()\n",
    "    \n",
    "    @staticmethod\n",
    "    def __activate_layer(z_vec, activation_func):\n",
    "        return activation_func(z_vec)\n",
    "    \n",
    "    def forward_propagation(self, input_layer_data):\n",
    "        a_l = input_layer_data\n",
    "        self.set_layer(0, a_l)\n",
    "        \n",
    "        for l in range(self.layers_num - 1):\n",
    "            z_lp1 = self.model[l].dot(a_l) # z(l+1) = W(l)*a(l)\n",
    "            a_l = g_sigmoid(z_lp1)\n",
    "            \n",
    "            # Dodavanje bias jedinice u a_l vektor\n",
    "            if l != self.layers_num - 2:\n",
    "                a_l = np.concatenate((np.array([1]), a_l))\n",
    "            \n",
    "            self.set_layer(l + 1, a_l)\n",
    "            \n",
    "    # Vrsi propagaciju na osnovu prosledjenog modela, a ne na osnovu internog modela.\n",
    "    def propagate(self, input_layer_data, neural_model):\n",
    "        a_l = input_layer_data\n",
    "        self.set_layer(0, a_l)\n",
    "        \n",
    "        for l in range(self.layers_num - 1):\n",
    "            z_lp1 = neural_model[l].dot(a_l)\n",
    "            a_l = g_sigmoid(z_lp1)\n",
    "            \n",
    "            # Dodavanje bias jedinice u a_l vektor\n",
    "            if l != self.layers_num - 2:\n",
    "                a_l = np.concatenate((np.array([1]), a_l))\n",
    "            \n",
    "            self.set_layer(l + 1, a_l)\n",
    "            \n",
    "    def backward_propagation_deltas(self, y_data):\n",
    "        delta_output = self.network[self.layers_num - 1] - y_data\n",
    "        self.set_delta(self.layers_num - 1, delta_output)\n",
    "        \n",
    "        for l in range(self.layers_num - 2, 0, -1):\n",
    "            z_l = self.model[l - 1].dot(self.network[l - 1]) # z(l) = W(l-1)*a(l-1)\n",
    "            g_prim_vec = np.concatenate((np.array([1]), g_sigmoid_derivative(z_l)))\n",
    "            \n",
    "            if l + 1 == self.layers_num - 1:\n",
    "                delta_lp1 = self.deltas[l + 1] # ne postoji bias jedinica za poslednji sloj\n",
    "            else:\n",
    "                delta_lp1 = self.deltas[l + 1][1:] # ignorisem bias jedinicu delta vektora\n",
    "                \n",
    "            delta_l = self.model[l].transpose().dot(delta_lp1) * g_prim_vec\n",
    "            self.set_delta(l, delta_l)\n",
    "            \n",
    "        # delta_0 je uvek nula-vektor, postavljen jos prilikom inicijalizacije same mreze\n",
    "        \n",
    "    # Funkcija koja racuna uporedo parcijalne izvode (gradijent) i funkciju gubitka\n",
    "    def backward_propagation(self, X_training, y_training, lambda_param=0):\n",
    "        accs = []\n",
    "        for l in range(self.layers_num - 1):\n",
    "            delta_acc_l = np.zeros(self.layer_mapper_sizes[l])\n",
    "            accs.append(delta_acc_l)\n",
    "            \n",
    "        # Delta_l akumulatori za parcijalne izvode i inicijalni parcijalni izvodi\n",
    "        delta_accumulators = np.array(accs)\n",
    "        gradient = np.array(accs)\n",
    "        loss_inner = 0\n",
    "        \n",
    "        N = X_training.shape[0]\n",
    "        for i in range(N):\n",
    "            self.forward_propagation(X_training[i]) # a_0 = X[i] ...\n",
    "            self.backward_propagation_deltas(y_training[i])\n",
    "            self.__accumulate_deltas(delta_accumulators) # radi dobro\n",
    "            \n",
    "            # print(f'Delta accumulators:')\n",
    "            # print(delta_accumulators)\n",
    "            \n",
    "            # Hocu da y_predict i y budu realne vrednosti a ne vektori velicine 1\n",
    "            y_i_predict = self.network[self.layers_num - 1][0]\n",
    "            y_i = y_training[i][0]\n",
    "            \n",
    "            loss_inner += y_i*np.log(y_i_predict) + (1 - y_i)*np.log(1 - y_i_predict)\n",
    "            \n",
    "            # print(f'y_predict = {y_predict}, y_{i} = {y_i}')\n",
    "            # print(f'Loss_{i}: {loss}')\n",
    "            \n",
    "        self.__set_partial_derivatives(gradient, delta_accumulators, lambda_param)\n",
    "        loss = -loss_inner / N + regularization(self.model, N, lambda_param)\n",
    "        \n",
    "        return loss, unroll_matrix_array(gradient)\n",
    "            \n",
    "    def __accumulate_deltas(self, delta_accumulators):\n",
    "        for l in range(self.layers_num - 1):\n",
    "            if l + 1 == self.layers_num - 1:\n",
    "                delta_lp1 = self.deltas[l + 1].reshape(-1, 1)\n",
    "            else:\n",
    "                delta_lp1 = self.deltas[l + 1][1:].reshape(-1, 1)\n",
    "                \n",
    "            a_l = self.network[l].reshape(-1, 1).transpose()\n",
    "\n",
    "            delta_accumulators[l] += delta_lp1.dot(a_l).copy()\n",
    "            \n",
    "    def __set_partial_derivatives(self, partial_derivatives, delta_accumulators, lambda_param):\n",
    "        for l in range(self.layers_num - 1):\n",
    "            for i in range(self.model[l].shape[0]):\n",
    "                for j in range(self.model[l].shape[1]):\n",
    "                    if j == 0:\n",
    "                        partial_derivatives[l][i][j] = delta_accumulators[l][i][j] / N\n",
    "                    else:\n",
    "                        partial_derivatives[l][i][j] = delta_accumulators[l][i][j] / N + \\\n",
    "                                                       lambda_param * self.model[l][i][j]\n",
    "                        \n",
    "    def fit(self, X, y, alpha=0.01, num_iter=1000, lambda_param=0, plot=False):\n",
    "        loss_history, model_trained = gradient_descent(X, y, \n",
    "                                                       self, \n",
    "                                                       alpha, \n",
    "                                                       num_iter, \n",
    "                                                       lambda_param,\n",
    "                                                       plot)\n",
    "        \n",
    "        matrix_sizes = get_matrix_sizes(self.model)\n",
    "        self.model_trained = roll_vec_to_matrix_array(model_trained, matrix_sizes)\n",
    "            \n",
    "    def predict(self, input_data):\n",
    "        return hypothesis_neural(self.model, input_data)\n",
    "        \n",
    "    def print_layer(self, layer_index):\n",
    "        self.__layer_index_check(layer_index)\n",
    "        \n",
    "        print(f'Layer {layer_index}:')\n",
    "        print(self.network[layer_index])\n",
    "        \n",
    "    def print_network(self):\n",
    "        print('~ Neural network ~')\n",
    "        \n",
    "        for i in range(self.network.shape[0]):\n",
    "            print(f'Layer {i + 1}:')\n",
    "            print(self.network[i])\n",
    "            \n",
    "    def print_layer_mapper_sizes(self):\n",
    "        print('Dimenzije matrica modela koji mapiraju slojeve:')\n",
    "        \n",
    "        for k, v in self.layer_mapper_sizes.items():\n",
    "            print(f'{k} -> {k + 1}: {v[0]} x {v[1]}')\n",
    "        \n",
    "    def print_mapper(self, layer_index):\n",
    "        self.__layer_index_check(layer_index)\n",
    "        \n",
    "        print(f'W_{layer_index}: {layer_index} -> {layer_index + 1}')\n",
    "        print(self.model[layer_index])           \n",
    "            \n",
    "    def print_model(self):\n",
    "        print('Model:')\n",
    "        \n",
    "        for l in range(self.layers_num - 1):\n",
    "            print(f'W_{l}: {l} -> {l + 1}')\n",
    "            print(self.model[l])\n",
    "            \n",
    "    def print_delta(self, layer_index):\n",
    "        self.__layer_index_check(layer_index)\n",
    "        \n",
    "        print(self.deltas[layer_index])\n",
    "            \n",
    "    def print_deltas(self):\n",
    "        print('Deltas:')\n",
    "        \n",
    "        for l in range(self.layers_num):\n",
    "            print(f'delta_{l}:')\n",
    "            print(self.deltas[l])\n",
    "# ---------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# UCITAVANJE PODATAKA I TESTIRANJE"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "outputs": [
    {
     "data": {
      "text/plain": "   sepal.length  sepal.width  petal.length  petal.width variety\n0           5.1          3.5           1.4          0.2  Setosa\n1           4.9          3.0           1.4          0.2  Setosa\n2           4.7          3.2           1.3          0.2  Setosa\n3           4.6          3.1           1.5          0.2  Setosa\n4           5.0          3.6           1.4          0.2  Setosa",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sepal.length</th>\n      <th>sepal.width</th>\n      <th>petal.length</th>\n      <th>petal.width</th>\n      <th>variety</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>5.1</td>\n      <td>3.5</td>\n      <td>1.4</td>\n      <td>0.2</td>\n      <td>Setosa</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>4.9</td>\n      <td>3.0</td>\n      <td>1.4</td>\n      <td>0.2</td>\n      <td>Setosa</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>4.7</td>\n      <td>3.2</td>\n      <td>1.3</td>\n      <td>0.2</td>\n      <td>Setosa</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4.6</td>\n      <td>3.1</td>\n      <td>1.5</td>\n      <td>0.2</td>\n      <td>Setosa</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5.0</td>\n      <td>3.6</td>\n      <td>1.4</td>\n      <td>0.2</td>\n      <td>Setosa</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 153
    }
   ],
   "source": [
    "df = pd.read_csv('../datasets/iris.csv')\n",
    "df.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Ukupan broj podataka: 150\nBroj atributa svakog podatka: 4\nAtributi: ['sepal.length', 'sepal.width', 'petal.length', 'petal.width']\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "N = df.shape[0]\n",
    "print(f'Ukupan broj podataka: {N}')\n",
    "\n",
    "m = df.shape[1] - 1\n",
    "print(f'Broj atributa svakog podatka: {m}')\n",
    "\n",
    "attributes = list(df.drop('variety', axis=True).columns)\n",
    "print(f'Atributi: {attributes}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "     sepal.length  sepal.width  petal.length  petal.width     variety\n50            7.0          3.2           4.7          1.4  Versicolor\n51            6.4          3.2           4.5          1.5  Versicolor\n52            6.9          3.1           4.9          1.5  Versicolor\n53            5.5          2.3           4.0          1.3  Versicolor\n54            6.5          2.8           4.6          1.5  Versicolor\n55            5.7          2.8           4.5          1.3  Versicolor\n56            6.3          3.3           4.7          1.6  Versicolor\n57            4.9          2.4           3.3          1.0  Versicolor\n58            6.6          2.9           4.6          1.3  Versicolor\n59            5.2          2.7           3.9          1.4  Versicolor\n60            5.0          2.0           3.5          1.0  Versicolor\n61            5.9          3.0           4.2          1.5  Versicolor\n62            6.0          2.2           4.0          1.0  Versicolor\n63            6.1          2.9           4.7          1.4  Versicolor\n64            5.6          2.9           3.6          1.3  Versicolor\n65            6.7          3.1           4.4          1.4  Versicolor\n66            5.6          3.0           4.5          1.5  Versicolor\n67            5.8          2.7           4.1          1.0  Versicolor\n68            6.2          2.2           4.5          1.5  Versicolor\n69            5.6          2.5           3.9          1.1  Versicolor\n70            5.9          3.2           4.8          1.8  Versicolor\n71            6.1          2.8           4.0          1.3  Versicolor\n72            6.3          2.5           4.9          1.5  Versicolor\n73            6.1          2.8           4.7          1.2  Versicolor\n74            6.4          2.9           4.3          1.3  Versicolor\n75            6.6          3.0           4.4          1.4  Versicolor\n76            6.8          2.8           4.8          1.4  Versicolor\n77            6.7          3.0           5.0          1.7  Versicolor\n78            6.0          2.9           4.5          1.5  Versicolor\n79            5.7          2.6           3.5          1.0  Versicolor\n..            ...          ...           ...          ...         ...\n120           6.9          3.2           5.7          2.3   Virginica\n121           5.6          2.8           4.9          2.0   Virginica\n122           7.7          2.8           6.7          2.0   Virginica\n123           6.3          2.7           4.9          1.8   Virginica\n124           6.7          3.3           5.7          2.1   Virginica\n125           7.2          3.2           6.0          1.8   Virginica\n126           6.2          2.8           4.8          1.8   Virginica\n127           6.1          3.0           4.9          1.8   Virginica\n128           6.4          2.8           5.6          2.1   Virginica\n129           7.2          3.0           5.8          1.6   Virginica\n130           7.4          2.8           6.1          1.9   Virginica\n131           7.9          3.8           6.4          2.0   Virginica\n132           6.4          2.8           5.6          2.2   Virginica\n133           6.3          2.8           5.1          1.5   Virginica\n134           6.1          2.6           5.6          1.4   Virginica\n135           7.7          3.0           6.1          2.3   Virginica\n136           6.3          3.4           5.6          2.4   Virginica\n137           6.4          3.1           5.5          1.8   Virginica\n138           6.0          3.0           4.8          1.8   Virginica\n139           6.9          3.1           5.4          2.1   Virginica\n140           6.7          3.1           5.6          2.4   Virginica\n141           6.9          3.1           5.1          2.3   Virginica\n142           5.8          2.7           5.1          1.9   Virginica\n143           6.8          3.2           5.9          2.3   Virginica\n144           6.7          3.3           5.7          2.5   Virginica\n145           6.7          3.0           5.2          2.3   Virginica\n146           6.3          2.5           5.0          1.9   Virginica\n147           6.5          3.0           5.2          2.0   Virginica\n148           6.2          3.4           5.4          2.3   Virginica\n149           5.9          3.0           5.1          1.8   Virginica\n\n[100 rows x 5 columns]",
      "\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "attr1 = 'petal.length'\n",
    "attr2 = 'petal.width'\n",
    "attributes = [attr1, attr2]\n",
    "m = len(attributes)\n",
    "\n",
    "df_short = df[df['variety'] != 'Setosa']\n",
    "print(df_short)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Velicina skupa podataka: 100\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "N = df_short.shape[0]\n",
    "print(f'Velicina skupa podataka: {N}')\n",
    "\n",
    "X = np.ones((N, m + 1))\n",
    "X[:, 1] = df_short[attr1]\n",
    "X[:, 2] = df_short[attr2]\n",
    "\n",
    "y = df_short['variety']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Mapiranje imena klasa u numericke vrednosti:\nVersicolor -> 0\nVirginica -> 1\n\nMapiranje numerickih vrednosti u odgovarajuce nazive klasa:\n0 -> Versicolor\n1 -> Virginica\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "classes = df_short['variety'].unique()\n",
    "n_classes = len(classes)\n",
    "\n",
    "name_to_class_dict = dict(zip(classes, range(n_classes)))\n",
    "class_to_name_dict = dict(zip(range(n_classes), classes))\n",
    "\n",
    "print(f'Mapiranje imena klasa u numericke vrednosti:')\n",
    "for name, cls in name_to_class_dict.items():\n",
    "    print(f'{name} -> {cls}')\n",
    "\n",
    "print()\n",
    "print(f'Mapiranje numerickih vrednosti u odgovarajuce nazive klasa:')\n",
    "for cls, name in class_to_name_dict.items():\n",
    "    print(f'{cls} -> {name}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "outputs": [
    {
     "data": {
      "text/plain": "<matplotlib.legend.Legend at 0xa076a90>"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 158
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "colors = ['red', 'blue']\n",
    "for c, cls in zip(colors, classes):\n",
    "    subsample = df.loc[df['variety'] == cls]\n",
    "    plt.scatter(subsample[attr1], subsample[attr2], color=c)\n",
    "    \n",
    "plt.xlabel(attr1)\n",
    "plt.ylabel(attr2)\n",
    "\n",
    "plt.legend(classes)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "outputs": [],
   "source": [
    "y = y.replace(name_to_class_dict)\n",
    "y = np.array(y).reshape(-1, 1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Podaci:\n[[1.  4.7 1.4]\n [1.  4.5 1.5]\n [1.  4.9 1.5]\n [1.  4.  1.3]\n [1.  4.6 1.5]\n [1.  4.5 1.3]\n [1.  4.7 1.6]\n [1.  3.3 1. ]\n [1.  4.6 1.3]\n [1.  3.9 1.4]]\n ...\nCiljne promenljive:\n[[0]\n [0]\n [0]\n [0]\n [0]\n [0]\n [0]\n [0]\n [0]\n [0]]\n ...\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "print(f'Podaci:')\n",
    "print(X[:10])\n",
    "print(' ...')\n",
    "\n",
    "print(f'Ciljne promenljive:')\n",
    "print(y[:10])\n",
    "print(' ...')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "~ Neural network ~\nLayer 1:\n[1. 1. 1.]\nLayer 2:\n[1. 1. 1.]\nLayer 3:\n[1. 1. 1.]\nLayer 4:\n[1.]\n\nDimenzije matrica modela koji mapiraju slojeve:\n0 -> 1: 2 x 3\n1 -> 2: 2 x 3\n2 -> 3: 1 x 3\n\nModel:\nW_0: 0 -> 1\n[[-0.54698157 -0.71928056 -0.54898896]\n [-0.16727609 -0.30885582 -0.18473612]]\nW_1: 1 -> 2\n[[-0.63376833 -0.62360295  0.60951821]\n [-0.22069206  0.9422377  -0.65853273]]\nW_2: 2 -> 3\n[[ 0.45972222  0.13506861 -0.01013308]]\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "# nn = NeuralNetwork(m, [3, 4], 1)\n",
    "nn = NeuralNetwork(m, [m, m], 1)\n",
    "nn.print_network()\n",
    "print()\n",
    "\n",
    "nn.print_layer_mapper_sizes()\n",
    "print()\n",
    "\n",
    "nn.print_model()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "gradient_0:\n[-7.88940976e-05 -2.99576294e-04 -8.77885909e-05  4.95108610e-04\n  1.65465269e-03  4.12001790e-04  3.91482356e-03  1.15345407e-04\n  9.25070793e-04 -3.03677173e-04 -9.06880794e-06 -7.24730924e-05\n  1.23489050e-01  4.63984096e-02  5.10625113e-02]\ngradient_1:\n[ 0.00027832  0.00076276  0.00014904  0.00095978  0.01060626  0.00486643\n  0.02071772 -0.00018158  0.00055527 -0.02186653  0.00018424 -0.00062411\n -0.14565894 -0.05121126 -0.06387155]\ngradient_2:\n[ 0.00027761  0.00075992  0.00014818  0.00095208  0.01053821  0.00483701\n  0.02074675 -0.00018102  0.00054983 -0.02191719  0.00018385 -0.0006187\n -0.14596682 -0.05124361 -0.06409381]\ngradient_3:\n[ 0.0002769   0.00075708  0.00014731  0.00094452  0.01047113  0.00480797\n  0.02077577 -0.00018045  0.0005445  -0.02196791  0.00018346 -0.0006134\n -0.14627527 -0.05127594 -0.06431673]\ngradient_4:\n[ 2.76190657e-04  7.54230879e-04  1.46437263e-04  9.37098432e-04\n  1.04049866e-02  4.77930936e-03  2.08047683e-02 -1.79890535e-04\n  5.39295222e-04 -2.20186881e-02  1.83070289e-04 -6.08221095e-04\n -1.46584293e-01 -5.13082223e-02 -6.45403175e-02]\ngradient_5:\n[ 2.75480106e-04  7.51381633e-04  1.45566197e-04  9.29808224e-04\n  1.03397699e-02  4.75101983e-03  2.08337580e-02 -1.79327855e-04\n  5.34200897e-04 -2.20695299e-02  1.82679321e-04 -6.03149302e-04\n -1.46893869e-01 -5.13404704e-02 -6.47645754e-02]\ngradient_6:\n[ 2.74768678e-04  7.48528756e-04  1.44693982e-04  9.22648498e-04\n  1.02754615e-02  4.72309358e-03  2.08627328e-02 -1.78765150e-04\n  5.29217562e-04 -2.21204305e-02  1.82287389e-04 -5.98185854e-04\n -1.47203998e-01 -5.13726741e-02 -6.49895059e-02]\ngradient_7:\n[ 2.74056384e-04  7.45672298e-04  1.43820635e-04  9.15616682e-04\n  1.02120445e-02  4.69552404e-03  2.08916911e-02 -1.78202426e-04\n  5.24342675e-04 -2.21713887e-02  1.81894499e-04 -5.93328403e-04\n -1.47514674e-01 -5.14048284e-02 -6.52151114e-02]\ngradient_8:\n[ 2.73343233e-04  7.42812305e-04  1.42946172e-04  9.08710259e-04\n  1.01495024e-02  4.66830477e-03  2.09206316e-02 -1.77639693e-04\n  5.19573761e-04 -2.22224032e-02  1.81500657e-04 -5.88574662e-04\n -1.47825893e-01 -5.14369284e-02 -6.54413946e-02]\ngradient_9:\n[ 2.72629236e-04  7.39948827e-04  1.42070610e-04  9.01926769e-04\n  1.00878191e-02  4.64142947e-03  2.09495527e-02 -1.77076957e-04\n  5.14908408e-04 -2.22734727e-02  1.81105867e-04 -5.83922400e-04\n -1.48137649e-01 -5.14689692e-02 -6.56683577e-02]\ngradient_10:\n[ 2.71914402e-04  7.37081910e-04  1.41193966e-04  8.95263809e-04\n  1.00269788e-02  4.61489198e-03  2.09784530e-02 -1.76514227e-04\n  5.10344270e-04 -2.23245960e-02  1.80710136e-04 -5.79369443e-04\n -1.48449937e-01 -5.15009462e-02 -6.58960033e-02]",
      "\ngradient_11:\n[ 2.71198742e-04  7.34211601e-04  1.40316254e-04  8.88719029e-04\n  9.96696604e-03  4.58868627e-03  2.10073312e-02 -1.75951509e-04\n  5.05879059e-04 -2.23757718e-02  1.80313468e-04 -5.74913671e-04\n -1.48762752e-01 -5.15328546e-02 -6.61243337e-02]\ngradient_12:\n[ 2.70482263e-04  7.31337945e-04  1.39437491e-04  8.82290131e-04\n  9.90776567e-03  4.56280645e-03  2.10361857e-02 -1.75388813e-04\n  5.01510547e-04 -2.24269989e-02  1.79915870e-04 -5.70553017e-04\n -1.49076089e-01 -5.15646899e-02 -6.63533512e-02]\ngradient_13:\n[ 2.69764977e-04  7.28460989e-04  1.38557693e-04  8.75974868e-04\n  9.84936291e-03  4.53724674e-03  2.10650153e-02 -1.74826144e-04\n  4.97236563e-04 -2.24782760e-02  1.79517346e-04 -5.66285466e-04\n -1.49389944e-01 -5.15964478e-02 -6.65830582e-02]\ngradient_14:\n[ 2.69046892e-04  7.25580776e-04  1.37676875e-04  8.69771043e-04\n  9.79174329e-03  4.51200150e-03  2.10938186e-02 -1.74263511e-04\n  4.93054993e-04 -2.25296019e-02  1.79117901e-04 -5.62109051e-04\n -1.49704312e-01 -5.16281238e-02 -6.68134568e-02]\ngradient_15:\n[ 2.68328017e-04  7.22697353e-04  1.36795053e-04  8.63676509e-04\n  9.73489266e-03  4.48706518e-03  2.11225942e-02 -1.73700922e-04\n  4.88963773e-04 -2.25809752e-02  1.78717540e-04 -5.58021856e-04\n -1.50019187e-01 -5.16597137e-02 -6.70445494e-02]\ngradient_16:\n[ 2.67608361e-04  7.19810763e-04  1.35912242e-04  8.57689165e-04\n  9.67879716e-03  4.46243239e-03  2.11513408e-02 -1.73138382e-04\n  4.84960896e-04 -2.26323946e-02  1.78316269e-04 -5.54022008e-04\n -1.50334566e-01 -5.16912134e-02 -6.72763380e-02]\ngradient_17:\n[ 2.66887934e-04  7.16921049e-04  1.35028457e-04  8.51806956e-04\n  9.62344323e-03  4.43809782e-03  2.11800571e-02 -1.72575899e-04\n  4.81044402e-04 -2.26838591e-02  1.77914091e-04 -5.50107683e-04\n -1.50650443e-01 -5.17226187e-02 -6.75088248e-02]\ngradient_18:\n[ 2.66166745e-04  7.14028256e-04  1.34143713e-04  8.46027872e-04\n  9.56881760e-03  4.41405628e-03  2.12087418e-02 -1.72013482e-04\n  4.77212381e-04 -2.27353671e-02  1.77511013e-04 -5.46277100e-04\n -1.50966815e-01 -5.17539256e-02 -6.77420119e-02]\ngradient_19:\n[ 2.65444801e-04  7.11132425e-04  1.33258025e-04  8.40349948e-04\n  9.51490728e-03  4.39030269e-03  2.12373936e-02 -1.71451136e-04\n  4.73462973e-04 -2.27869176e-02  1.77107039e-04 -5.42528520e-04\n -1.51283676e-01 -5.17851303e-02 -6.79759013e-02]\n",
      "gradient_20:\n[ 2.64722113e-04  7.08233599e-04  1.32371408e-04  8.34771259e-04\n  9.46169957e-03  4.36683209e-03  2.12660112e-02 -1.70888868e-04\n  4.69794361e-04 -2.28385091e-02  1.76702173e-04 -5.38860247e-04\n -1.51601022e-01 -5.18162289e-02 -6.82104952e-02]\ngradient_21:\n[ 2.63998688e-04  7.05331821e-04  1.31483876e-04  8.29289926e-04\n  9.40918201e-03  4.34363960e-03  2.12945934e-02 -1.70326687e-04\n  4.66204775e-04 -2.28901405e-02  1.76296420e-04 -5.35270625e-04\n -1.51918849e-01 -5.18472176e-02 -6.84457955e-02]\ngradient_22:\n[ 2.63274534e-04  7.02427133e-04  1.30595444e-04  8.23904105e-04\n  9.35734242e-03  4.32072045e-03  2.13231388e-02 -1.69764598e-04\n  4.62692487e-04 -2.29418104e-02  1.75889785e-04 -5.31758037e-04\n -1.52237152e-01 -5.18780927e-02 -6.86818041e-02]\ngradient_23:\n[ 2.62549662e-04  6.99519575e-04  1.29706126e-04  8.18611997e-04\n  9.30616889e-03  4.29806998e-03  2.13516464e-02 -1.69202608e-04\n  4.59255814e-04 -2.29935176e-02  1.75482272e-04 -5.28320905e-04\n -1.52555928e-01 -5.19088507e-02 -6.89185230e-02]\ngradient_24:\n[ 2.61824078e-04  6.96609188e-04  1.28815937e-04  8.13411839e-04\n  9.25564973e-03  4.27568362e-03  2.13801148e-02 -1.68640725e-04\n  4.55893111e-04 -2.30452607e-02  1.75073886e-04 -5.24957688e-04\n -1.52875171e-01 -5.19394879e-02 -6.91559542e-02]\ngradient_25:\n[ 2.61097791e-04  6.93696014e-04  1.27924891e-04  8.08301904e-04\n  9.20577350e-03  4.25355687e-03  2.14085429e-02 -1.68078956e-04\n  4.52602776e-04 -2.30970385e-02  1.74664631e-04 -5.21666880e-04\n -1.53194878e-01 -5.19700009e-02 -6.93940993e-02]\ngradient_26:\n[ 2.60370810e-04  6.90780093e-04  1.27033002e-04  8.03280505e-04\n  9.15652901e-03  4.23168536e-03  2.14369294e-02 -1.67517306e-04\n  4.49383244e-04 -2.31488497e-02  1.74254512e-04 -5.18447012e-04\n -1.53515045e-01 -5.20003864e-02 -6.96329604e-02]\ngradient_27:\n[ 2.59643142e-04  6.87861465e-04  1.26140283e-04  7.98345990e-04\n  9.10790530e-03  4.21006479e-03  2.14652732e-02 -1.66955782e-04\n  4.46232988e-04 -2.32006929e-02  1.73843532e-04 -5.15296648e-04\n -1.53835666e-01 -5.20306409e-02 -6.98725392e-02]\ngradient_28:\n[ 2.58914796e-04  6.84940170e-04  1.25246750e-04  7.93496739e-04\n  9.05989163e-03  4.18869094e-03  2.14935732e-02 -1.66394392e-04\n  4.43150517e-04 -2.32525669e-02  1.73431698e-04 -5.12214384e-04\n -1.54156740e-01 -5.20607612e-02 -7.01128374e-02]\ngradient_29:\n[ 2.58185779e-04  6.82016247e-04  1.24352415e-04  7.88731170e-04\n  9.01247747e-03  4.16755969e-03  2.15218280e-02 -1.65833142e-04\n  4.40134377e-04 -2.33044704e-02  1.73019011e-04 -5.09198851e-04\n -1.54478260e-01 -5.20907441e-02 -7.03538568e-02]",
      "\ngradient_30:\n[ 2.57456100e-04  6.79089736e-04  1.23457293e-04  7.84047732e-04\n  8.96565252e-03  4.14666700e-03  2.15500367e-02 -1.65272038e-04\n  4.37183149e-04 -2.33564021e-02  1.72605478e-04 -5.06248709e-04\n -1.54800224e-01 -5.21205864e-02 -7.05955992e-02]\ngradient_31:\n[ 2.56725766e-04  6.76160676e-04  1.22561397e-04  7.79444907e-04\n  8.91940669e-03  4.12600891e-03  2.15781981e-02 -1.64711087e-04\n  4.34295445e-04 -2.34083606e-02  1.72191102e-04 -5.03362649e-04\n -1.55122627e-01 -5.21502850e-02 -7.08380662e-02]\ngradient_32:\n[ 2.55994786e-04  6.73229105e-04  1.21664741e-04  7.74921208e-04\n  8.87373010e-03  4.10558154e-03  2.16063109e-02 -1.64150296e-04\n  4.31469912e-04 -2.34603447e-02  1.71775887e-04 -5.00539393e-04\n -1.55445465e-01 -5.21798370e-02 -7.10812594e-02]\ngradient_33:\n[ 2.55263168e-04  6.70295063e-04  1.20767338e-04  7.70475180e-04\n  8.82861304e-03  4.08538107e-03  2.16343742e-02 -1.63589670e-04\n  4.28705229e-04 -2.35123530e-02  1.71359837e-04 -4.97777691e-04\n -1.55768735e-01 -5.22092393e-02 -7.13251805e-02]\ngradient_34:\n[ 2.54530918e-04  6.67358588e-04  1.19869202e-04  7.66105397e-04\n  8.78404604e-03  4.06540379e-03  2.16623869e-02 -1.63029217e-04\n  4.26000105e-04 -2.35643843e-02  1.70942958e-04 -4.95076320e-04\n -1.56092433e-01 -5.22384890e-02 -7.15698310e-02]\ngradient_35:\n[ 2.53798044e-04  6.64419717e-04  1.18970347e-04  7.61810462e-04\n  8.74001979e-03  4.04564604e-03  2.16903477e-02 -1.62468942e-04\n  4.23353278e-04 -2.36164371e-02  1.70525251e-04 -4.92434086e-04\n -1.56416554e-01 -5.22675833e-02 -7.18152126e-02]\ngradient_36:\n[ 2.53064556e-04  6.61478489e-04  1.18070784e-04  7.57589008e-04\n  8.69652517e-03  4.02610424e-03  2.17182557e-02 -1.61908853e-04\n  4.20763517e-04 -2.36685102e-02  1.70106723e-04 -4.89849821e-04\n -1.56741095e-01 -5.22965193e-02 -7.20613268e-02]\ngradient_37:\n[ 2.52330459e-04  6.58534942e-04  1.17170529e-04  7.53439695e-04\n  8.65355325e-03  4.00677488e-03  2.17461098e-02 -1.61348954e-04\n  4.18229620e-04 -2.37206022e-02  1.69687376e-04 -4.87322382e-04\n -1.57066053e-01 -5.23252943e-02 -7.23081751e-02]",
      "\ngradient_38:\n[ 2.51595761e-04  6.55589113e-04  1.16269593e-04  7.49361211e-04\n  8.61109528e-03  3.98765451e-03  2.17739088e-02 -1.60789254e-04\n  4.15750410e-04 -2.37727118e-02  1.69267215e-04 -4.84850653e-04\n -1.57391423e-01 -5.23539057e-02 -7.25557589e-02]\ngradient_39:\n[ 2.50860471e-04  6.52641039e-04  1.15367991e-04  7.45352270e-04\n  8.56914267e-03  3.96873976e-03  2.18016518e-02 -1.60229757e-04\n  4.13324739e-04 -2.38248377e-02  1.68846244e-04 -4.82433541e-04\n -1.57717202e-01 -5.23823507e-02 -7.28040798e-02]\ngradient_40:\n[ 2.50124595e-04  6.49690758e-04  1.14465734e-04  7.41411613e-04\n  8.52768701e-03  3.95002733e-03  2.18293377e-02 -1.59670470e-04\n  4.10951485e-04 -2.38769786e-02  1.68424467e-04 -4.80069976e-04\n -1.58043386e-01 -5.24106268e-02 -7.30531391e-02]\ngradient_41:\n[ 2.49388142e-04  6.46738307e-04  1.13562838e-04  7.37538004e-04\n  8.48672006e-03  3.93151395e-03  2.18569655e-02 -1.59111399e-04\n  4.08629552e-04 -2.39291330e-02  1.68001887e-04 -4.77758915e-04\n -1.58369972e-01 -5.24387313e-02 -7.33029383e-02]\ngradient_42:\n[ 2.48651117e-04  6.43783723e-04  1.12659313e-04  7.33730234e-04\n  8.44623373e-03  3.91319647e-03  2.18845341e-02 -1.58552551e-04\n  4.06357867e-04 -2.39812996e-02  1.67578509e-04 -4.75499333e-04\n -1.58696956e-01 -5.24666619e-02 -7.35534786e-02]\ngradient_43:\n[ 2.47913530e-04  6.40827043e-04  1.11755174e-04  7.29987118e-04\n  8.40622008e-03  3.89507174e-03  2.19120426e-02 -1.57993931e-04\n  4.04135382e-04 -2.40334772e-02  1.67154337e-04 -4.73290231e-04\n -1.59024334e-01 -5.24944160e-02 -7.38047616e-02]\ngradient_44:\n[ 2.47175386e-04  6.37868302e-04  1.10850432e-04  7.26307494e-04\n  8.36667136e-03  3.87713672e-03  2.19394899e-02 -1.57435546e-04\n  4.01961074e-04 -2.40856642e-02  1.66729374e-04 -4.71130629e-04\n -1.59352103e-01 -5.25219913e-02 -7.40567885e-02]\ngradient_45:\n[ 2.46436695e-04  6.34907538e-04  1.09945102e-04  7.22690222e-04\n  8.32757994e-03  3.85938840e-03  2.19668749e-02 -1.56877402e-04\n  3.99833941e-04 -2.41378595e-02  1.66303625e-04 -4.69019568e-04\n -1.59680259e-01 -5.25493853e-02 -7.43095606e-02]\ngradient_46:\n[ 2.45697462e-04  6.31944787e-04  1.09039196e-04  7.19134188e-04\n  8.28893834e-03  3.84182384e-03  2.19941969e-02 -1.56319504e-04\n  3.97753003e-04 -2.41900616e-02  1.65877093e-04 -4.66956110e-04\n -1.60008799e-01 -5.25765957e-02 -7.45630791e-02]\n",
      "gradient_47:\n[ 2.44957695e-04  6.28980086e-04  1.08132726e-04  7.15638296e-04\n  8.25073924e-03  3.82444015e-03  2.20214546e-02 -1.55761859e-04\n  3.95717305e-04 -2.42422691e-02  1.65449782e-04 -4.64939338e-04\n -1.60337719e-01 -5.26036203e-02 -7.48173453e-02]\ngradient_48:\n[ 2.44217402e-04  6.26013470e-04  1.07225705e-04  7.12201473e-04\n  8.21297544e-03  3.80723449e-03  2.20486473e-02 -1.55204473e-04\n  3.93725909e-04 -2.42944807e-02  1.65021697e-04 -4.62968352e-04\n -1.60667016e-01 -5.26304569e-02 -7.50723605e-02]\ngradient_49:\n[ 2.43476589e-04  6.23044975e-04  1.06318147e-04  7.08822669e-04\n  8.17563990e-03  3.79020409e-03  2.20757738e-02 -1.54647351e-04\n  3.91777900e-04 -2.43466950e-02  1.64592840e-04 -4.61042272e-04\n -1.60996686e-01 -5.26571032e-02 -7.53281258e-02]\ngradient_50:\n[ 2.42735264e-04  6.20074638e-04  1.05410063e-04  7.05500851e-04\n  8.13872569e-03  3.77334622e-03  2.21028333e-02 -1.54090500e-04\n  3.89872382e-04 -2.43989106e-02  1.64163216e-04 -4.59160237e-04\n -1.61326727e-01 -5.26835570e-02 -7.55846423e-02]\ngradient_51:\n[ 2.41993434e-04  6.17102494e-04  1.04501467e-04  7.02235008e-04\n  8.10222603e-03  3.75665819e-03  2.21298248e-02 -1.53533926e-04\n  3.88008481e-04 -2.44511262e-02  1.63732829e-04 -4.57321402e-04\n -1.61657133e-01 -5.27098164e-02 -7.58419113e-02]\ngradient_52:\n[ 2.41251106e-04  6.14128579e-04  1.03592371e-04  6.99024149e-04\n  8.06613427e-03  3.74013740e-03  2.21567474e-02 -1.52977634e-04\n  3.86185338e-04 -2.45033403e-02  1.63301682e-04 -4.55524943e-04\n -1.61987903e-01 -5.27358791e-02 -7.60999338e-02]\ngradient_53:\n[ 2.40508287e-04  6.11152929e-04  1.02682787e-04  6.95867300e-04\n  8.03044387e-03  3.72378125e-03  2.21836001e-02 -1.52421630e-04\n  3.84402116e-04 -2.45555516e-02  1.62869780e-04 -4.53770048e-04\n -1.62319033e-01 -5.27617432e-02 -7.63587110e-02]\ngradient_54:\n[ 2.39764985e-04  6.08175578e-04  1.01772729e-04  6.92763509e-04\n  7.99514842e-03  3.70758722e-03  2.22103820e-02 -1.51865921e-04\n  3.82657994e-04 -2.46077588e-02  1.62437126e-04 -4.52055925e-04\n -1.62650519e-01 -5.27874066e-02 -7.66182438e-02]\ngradient_55:\n[ 2.39021206e-04  6.05196564e-04  1.00862208e-04  6.89711839e-04\n  7.96024164e-03  3.69155282e-03  2.22370922e-02 -1.51310511e-04\n  3.80952171e-04 -2.46599603e-02  1.62003724e-04 -4.50381799e-04\n -1.62982359e-01 -5.28128675e-02 -7.68785334e-02]\ngradient_56:\n[ 2.38276958e-04  6.02215921e-04  9.99512376e-05  6.86711372e-04\n  7.92571735e-03  3.67567563e-03  2.22637297e-02 -1.50755407e-04\n  3.79283860e-04 -2.47121548e-02  1.61569577e-04 -4.48746907e-04\n -1.63314548e-01 -5.28381238e-02 -7.71395808e-02]\n",
      "gradient_57:\n[ 2.37532248e-04  5.99233684e-04  9.90398299e-05  6.83761209e-04\n  7.89156949e-03  3.65995324e-03  2.22902938e-02 -1.50200615e-04\n  3.77652293e-04 -2.47643408e-02  1.61134691e-04 -4.47150505e-04\n -1.63647084e-01 -5.28631737e-02 -7.74013869e-02]\ngradient_58:\n[ 2.36787083e-04  5.96249889e-04  9.81279974e-05  6.80860465e-04\n  7.85779213e-03  3.64438331e-03  2.23167834e-02 -1.49646140e-04\n  3.76056718e-04 -2.48165171e-02  1.60699068e-04 -4.45591862e-04\n -1.63979963e-01 -5.28880153e-02 -7.76639527e-02]\ngradient_59:\n[ 2.36041469e-04  5.93264571e-04  9.72157526e-05  6.78008275e-04\n  7.82437943e-03  3.62896353e-03  2.23431978e-02 -1.49091988e-04\n  3.74496400e-04 -2.48686822e-02  1.60262713e-04 -4.44070263e-04\n -1.64313183e-01 -5.29126469e-02 -7.79272791e-02]\ngradient_60:\n[ 2.35295414e-04  5.90277766e-04  9.63031079e-05  6.75203787e-04\n  7.79132567e-03  3.61369165e-03  2.23695359e-02 -1.48538165e-04\n  3.72970616e-04 -2.49208345e-02  1.59825628e-04 -4.42585005e-04\n -1.64646739e-01 -5.29370666e-02 -7.81913672e-02]\ngradient_61:\n[ 2.34548926e-04  5.87289508e-04  9.53900757e-05  6.72446168e-04\n  7.75862521e-03  3.59856543e-03  2.23957970e-02 -1.47984677e-04\n  3.71478662e-04 -2.49729729e-02  1.59387819e-04 -4.41135401e-04\n -1.64980629e-01 -5.29612726e-02 -7.84562177e-02]\ngradient_62:\n[ 2.33802010e-04  5.84299832e-04  9.44766685e-05  6.69734599e-04\n  7.72627255e-03  3.58358270e-03  2.24219802e-02 -1.47431529e-04\n  3.70019848e-04 -2.50250957e-02  1.58949289e-04 -4.39720778e-04\n -1.65314849e-01 -5.29852634e-02 -7.87218315e-02]\ngradient_63:\n[ 2.33054675e-04  5.81308775e-04  9.35628987e-05  6.67068275e-04\n  7.69426226e-03  3.56874132e-03  2.24480847e-02 -1.46878726e-04\n  3.68593496e-04 -2.50772017e-02  1.58510042e-04 -4.38340474e-04\n -1.65649397e-01 -5.30090371e-02 -7.89882095e-02]\ngradient_64:\n[ 2.32306926e-04  5.78316370e-04  9.26487786e-05  6.64446408e-04\n  7.66258902e-03  3.55403918e-03  2.24741094e-02 -1.46326275e-04\n  3.67198945e-04 -2.51292893e-02  1.58070081e-04 -4.36993842e-04\n -1.65984269e-01 -5.30325922e-02 -7.92553526e-02]\ngradient_65:\n[ 2.31558772e-04  5.75322653e-04  9.17343206e-05  6.61868225e-04\n  7.63124762e-03  3.53947421e-03  2.25000538e-02 -1.45774182e-04\n  3.65835546e-04 -2.51813571e-02  1.57629411e-04 -4.35680247e-04\n -1.66319461e-01 -5.30559270e-02 -7.95232614e-02]\ngradient_66:\n[ 2.30810219e-04  5.72327658e-04  9.08195371e-05  6.59332964e-04\n  7.60023292e-03  3.52504438e-03  2.25259168e-02 -1.45222451e-04\n  3.64502665e-04 -2.52334037e-02  1.57188035e-04 -4.34399065e-04\n -1.66654972e-01 -5.30790400e-02 -7.97919367e-02]\n",
      "gradient_67:\n[ 2.30061273e-04  5.69331421e-04  8.99044404e-05  6.56839881e-04\n  7.56953989e-03  3.51074771e-03  2.25516977e-02 -1.44671088e-04\n  3.63199679e-04 -2.52854277e-02  1.56745958e-04 -4.33149686e-04\n -1.66990797e-01 -5.31019295e-02 -8.00613794e-02]\ngradient_68:\n[ 2.29311944e-04  5.66333977e-04  8.89890430e-05  6.54388243e-04\n  7.53916358e-03  3.49658223e-03  2.25773957e-02 -1.44120099e-04\n  3.61925980e-04 -2.53374276e-02  1.56303183e-04 -4.31931511e-04\n -1.67326934e-01 -5.31245940e-02 -8.03315901e-02]\ngradient_69:\n[ 2.28562236e-04  5.63335361e-04  8.80733570e-05  6.51977331e-04\n  7.50909913e-03  3.48254601e-03  2.26030098e-02 -1.43569490e-04\n  3.60680970e-04 -2.53894019e-02  1.55859714e-04 -4.30743953e-04\n -1.67663380e-01 -5.31470322e-02 -8.06025695e-02]\ngradient_70:\n[ 2.27812157e-04  5.60335607e-04  8.71573949e-05  6.49606438e-04\n  7.47934178e-03  3.46863718e-03  2.26285395e-02 -1.43019265e-04\n  3.59464066e-04 -2.54413493e-02  1.55415556e-04 -4.29586435e-04\n -1.68000131e-01 -5.31692424e-02 -8.08743183e-02]\ngradient_71:\n[ 2.27061715e-04  5.57334750e-04  8.62411690e-05  6.47274873e-04\n  7.44988683e-03  3.45485388e-03  2.26539837e-02 -1.42469431e-04\n  3.58274695e-04 -2.54932682e-02  1.54970711e-04 -4.28458391e-04\n -1.68337184e-01 -5.31912232e-02 -8.11468372e-02]\ngradient_72:\n[ 2.26310916e-04  5.54332826e-04  8.53246916e-05  6.44981954e-04\n  7.42072968e-03  3.44119427e-03  2.26793418e-02 -1.41919993e-04\n  3.57112295e-04 -2.55451572e-02  1.54525185e-04 -4.27359268e-04\n -1.68674537e-01 -5.32129733e-02 -8.14201267e-02]\ngradient_73:\n[ 2.25559767e-04  5.51329868e-04  8.44079751e-05  6.42727012e-04\n  7.39186580e-03  3.42765657e-03  2.27046129e-02 -1.41370957e-04\n  3.55976318e-04 -2.55970148e-02  1.54078980e-04 -4.26288521e-04\n -1.69012186e-01 -5.32344913e-02 -8.16941876e-02]\ngradient_74:\n[ 2.24808275e-04  5.48325913e-04  8.34910317e-05  6.40509393e-04\n  7.36329077e-03  3.41423902e-03  2.27297962e-02 -1.40822328e-04\n  3.54866225e-04 -2.56488396e-02  1.53632102e-04 -4.25245616e-04\n -1.69350128e-01 -5.32557758e-02 -8.19690202e-02]\ngradient_75:\n[ 2.24056448e-04  5.45320995e-04  8.25738737e-05  6.38328449e-04\n  7.33500020e-03  3.40093989e-03  2.27548911e-02 -1.40274111e-04\n  3.53781488e-04 -2.57006301e-02  1.53184553e-04 -4.24230029e-04\n -1.69688360e-01 -5.32768255e-02 -8.22446253e-02]",
      "\ngradient_76:\n[ 2.23304291e-04  5.42315149e-04  8.16565135e-05  6.36183550e-04\n  7.30698982e-03  3.38775747e-03  2.27798967e-02 -1.39726311e-04\n  3.52721590e-04 -2.57523848e-02  1.52736338e-04 -4.23241245e-04\n -1.70026880e-01 -5.32976390e-02 -8.25210033e-02]\ngradient_77:\n[ 2.22551814e-04  5.39308409e-04  8.07389634e-05  6.34074071e-04\n  7.27925540e-03  3.37469010e-03  2.28048122e-02 -1.39178935e-04\n  3.51686024e-04 -2.58041022e-02  1.52287462e-04 -4.22278761e-04\n -1.70365684e-01 -5.33182152e-02 -8.27981547e-02]\ngradient_78:\n[ 2.21799021e-04  5.36300811e-04  7.98212357e-05  6.31999403e-04\n  7.25179281e-03  3.36173613e-03  2.28296369e-02 -1.38631988e-04\n  3.50674294e-04 -2.58557810e-02  1.51837927e-04 -4.21342080e-04\n -1.70704769e-01 -5.33385527e-02 -8.30760800e-02]\ngradient_79:\n[ 2.21045921e-04  5.33292389e-04  7.89033427e-05  6.29958946e-04\n  7.22459798e-03  3.34889396e-03  2.28543701e-02 -1.38085475e-04\n  3.49685913e-04 -2.59074195e-02  1.51387739e-04 -4.20430717e-04\n -1.71044133e-01 -5.33586505e-02 -8.33547796e-02]\ngradient_80:\n[ 2.20292520e-04  5.30283179e-04  7.79852968e-05  6.27952109e-04\n  7.19766691e-03  3.33616198e-03  2.28790110e-02 -1.37539401e-04\n  3.48720403e-04 -2.59590163e-02  1.50936901e-04 -4.19544193e-04\n -1.71383772e-01 -5.33785071e-02 -8.36342541e-02]\ngradient_81:\n[ 2.19538826e-04  5.27273215e-04  7.70671101e-05  6.25978313e-04\n  7.17099566e-03  3.32353865e-03  2.29035589e-02 -1.36993772e-04\n  3.47777298e-04 -2.60105699e-02  1.50485417e-04 -4.18682041e-04\n -1.71723683e-01 -5.33981216e-02 -8.39145037e-02]\ngradient_82:\n[ 2.18784845e-04  5.24262532e-04  7.61487952e-05  6.24036989e-04\n  7.14458038e-03  3.31102242e-03  2.29280130e-02 -1.36448593e-04\n  3.46856138e-04 -2.60620789e-02  1.50033292e-04 -4.17843800e-04\n -1.72063864e-01 -5.34174927e-02 -8.41955289e-02]\ngradient_83:\n[ 2.18030585e-04  5.21251166e-04  7.52303642e-05  6.22127578e-04\n  7.11841727e-03  3.29861180e-03  2.29523727e-02 -1.35903869e-04\n  3.45956474e-04 -2.61135417e-02  1.49580529e-04 -4.17029018e-04\n -1.72404312e-01 -5.34366194e-02 -8.44773301e-02]\ngradient_84:\n[ 2.17276052e-04  5.18239151e-04  7.43118295e-05  6.20249529e-04\n  7.09250259e-03  3.28630530e-03  2.29766372e-02 -1.35359607e-04\n  3.45077864e-04 -2.61649568e-02  1.49127134e-04 -4.16237251e-04\n -1.72745023e-01 -5.34555005e-02 -8.47599074e-02]\ngradient_85:\n[ 2.16521254e-04  5.15226522e-04  7.33932034e-05  6.18402304e-04\n  7.06683268e-03  3.27410146e-03  2.30008058e-02 -1.34815810e-04\n  3.44219878e-04 -2.62163227e-02  1.48673109e-04 -4.15468064e-04\n -1.73085995e-01 -5.34741350e-02 -8.50432614e-02]",
      "\ngradient_86:\n[ 2.15766198e-04  5.12213315e-04  7.24744983e-05  6.16585370e-04\n  7.04140393e-03  3.26199885e-03  2.30248778e-02 -1.34272484e-04\n  3.43382092e-04 -2.62676380e-02  1.48218459e-04 -4.14721029e-04\n -1.73427225e-01 -5.34925219e-02 -8.53273922e-02]\ngradient_87:\n[ 2.15010891e-04  5.09199563e-04  7.15557265e-05  6.14798207e-04\n  7.01621279e-03  3.24999607e-03  2.30488524e-02 -1.33729635e-04\n  3.42564089e-04 -2.63189010e-02  1.47763188e-04 -4.13995725e-04\n -1.73768709e-01 -5.35106601e-02 -8.56123000e-02]\ngradient_88:\n[ 2.14255340e-04  5.06185303e-04  7.06369003e-05  6.13040301e-04\n  6.99125578e-03  3.23809172e-03  2.30727291e-02 -1.33187267e-04\n  3.41765462e-04 -2.63701104e-02  1.47307302e-04 -4.13291739e-04\n -1.74110446e-01 -5.35285486e-02 -8.58979852e-02]\ngradient_89:\n[ 2.13499552e-04  5.03170570e-04  6.97180321e-05  6.11311149e-04\n  6.96652947e-03  3.22628444e-03  2.30965072e-02 -1.32645386e-04\n  3.40985813e-04 -2.64212646e-02  1.46850803e-04 -4.12608666e-04\n -1.74452432e-01 -5.35461864e-02 -8.61844480e-02]\ngradient_90:\n[ 2.12743534e-04  5.00155398e-04  6.87991343e-05  6.09610254e-04\n  6.94203048e-03  3.21457291e-03  2.31201859e-02 -1.32103998e-04\n  3.40224748e-04 -2.64723621e-02  1.46393696e-04 -4.11946107e-04\n -1.74794665e-01 -5.35635727e-02 -8.64716884e-02]\ngradient_91:\n[ 2.11987294e-04  4.97139823e-04  6.78802191e-05  6.07937131e-04\n  6.91775552e-03  3.20295578e-03  2.31437645e-02 -1.31563107e-04\n  3.39481884e-04 -2.65234013e-02  1.45935986e-04 -4.11303671e-04\n -1.75137140e-01 -5.35807064e-02 -8.67597067e-02]\ngradient_92:\n[ 2.11230838e-04  4.94123880e-04  6.69612990e-05  6.06291300e-04\n  6.89370131e-03  3.19143179e-03  2.31672425e-02 -1.31022718e-04\n  3.38756843e-04 -2.65743808e-02  1.45477677e-04 -4.10680972e-04\n -1.75479857e-01 -5.35975867e-02 -8.70485030e-02]\ngradient_93:\n[ 2.10474174e-04  4.91107604e-04  6.60423864e-05  6.04672290e-04\n  6.86986465e-03  3.17999963e-03  2.31906192e-02 -1.30482837e-04\n  3.38049257e-04 -2.66252989e-02  1.45018773e-04 -4.10077633e-04\n -1.75822811e-01 -5.36142128e-02 -8.73380775e-02]",
      "\ngradient_94:\n[ 2.09717309e-04  4.88091030e-04  6.51234935e-05  6.03079639e-04\n  6.84624239e-03  3.16865808e-03  2.32138938e-02 -1.29943468e-04\n  3.37358762e-04 -2.66761543e-02  1.44559279e-04 -4.09493283e-04\n -1.76166001e-01 -5.36305836e-02 -8.76284301e-02]\ngradient_95:\n[ 2.08960250e-04  4.85074195e-04  6.42046329e-05  6.01512892e-04\n  6.82283143e-03  3.15740588e-03  2.32370658e-02 -1.29404618e-04\n  3.36685002e-04 -2.67269454e-02  1.44099199e-04 -4.08927556e-04\n -1.76509423e-01 -5.36466985e-02 -8.79195610e-02]\ngradient_96:\n[ 2.08203004e-04  4.82057132e-04  6.32858168e-05  5.99971601e-04\n  6.79962872e-03  3.14624183e-03  2.32601346e-02 -1.28866290e-04\n  3.36027629e-04 -2.67776705e-02  1.43638537e-04 -4.08380094e-04\n -1.76853074e-01 -5.36625565e-02 -8.82114701e-02]\ngradient_97:\n[ 2.07445579e-04  4.79039877e-04  6.23670578e-05  5.98455326e-04\n  6.77663126e-03  3.13516474e-03  2.32830994e-02 -1.28328491e-04\n  3.35386299e-04 -2.68283283e-02  1.43177298e-04 -4.07850543e-04\n -1.77196951e-01 -5.36781570e-02 -8.85041575e-02]\ngradient_98:\n[ 2.06687981e-04  4.76022467e-04  6.14483681e-05  5.96963636e-04\n  6.75383610e-03  3.12417342e-03  2.33059596e-02 -1.27791224e-04\n  3.34760678e-04 -2.68789172e-02  1.42715487e-04 -4.07338558e-04\n -1.77541053e-01 -5.36934990e-02 -8.87976232e-02]\ngradient_99:\n[ 2.05930219e-04  4.73004935e-04  6.05297602e-05  5.95496103e-04\n  6.73124034e-03  3.11326673e-03  2.33287148e-02 -1.27254496e-04\n  3.34150436e-04 -2.69294356e-02  1.42253108e-04 -4.06843797e-04\n -1.77885376e-01 -5.37085819e-02 -8.90918671e-02]\ngradient_100:\n[ 2.05172298e-04  4.69987319e-04  5.96112467e-05  5.94052311e-04\n  6.70884112e-03  3.10244353e-03  2.33513641e-02 -1.26718311e-04\n  3.33555248e-04 -2.69798819e-02  1.41790165e-04 -4.06365925e-04\n -1.78229917e-01 -5.37234050e-02 -8.93868892e-02]\ngradient_101:\n[ 2.04414228e-04  4.66969652e-04  5.86928397e-05  5.92631847e-04\n  6.68663564e-03  3.09170270e-03  2.33739070e-02 -1.26182674e-04\n  3.32974798e-04 -2.70302548e-02  1.41326664e-04 -4.05904614e-04\n -1.78574673e-01 -5.37379675e-02 -8.96826893e-02]",
      "\ngradient_102:\n[ 2.03656013e-04  4.63951971e-04  5.77745519e-05  5.91234306e-04\n  6.66462113e-03  3.08104315e-03  2.33963430e-02 -1.25647590e-04\n  3.32408775e-04 -2.70805525e-02  1.40862608e-04 -4.05459538e-04\n -1.78919642e-01 -5.37522687e-02 -8.99792673e-02]\ngradient_103:\n[ 2.02897663e-04  4.60934312e-04  5.68563957e-05  5.89859291e-04\n  6.64279488e-03  3.07046377e-03  2.34186714e-02 -1.25113064e-04\n  3.31856873e-04 -2.71307737e-02  1.40398003e-04 -4.05030381e-04\n -1.79264822e-01 -5.37663080e-02 -9.02766231e-02]\ngradient_104:\n[ 2.02139185e-04  4.57916709e-04  5.59383835e-05  5.88506411e-04\n  6.62115421e-03  3.05996352e-03  2.34408916e-02 -1.24579101e-04\n  3.31318792e-04 -2.71809166e-02  1.39932853e-04 -4.04616828e-04\n -1.79610208e-01 -5.37800847e-02 -9.05747566e-02]\ngradient_105:\n[ 2.01380585e-04  4.54899200e-04  5.50205278e-05  5.87175279e-04\n  6.59969648e-03  3.04954135e-03  2.34630030e-02 -1.24045706e-04\n  3.30794237e-04 -2.72309798e-02  1.39467162e-04 -4.04218572e-04\n -1.79955799e-01 -5.37935982e-02 -9.08736674e-02]\ngradient_106:\n[ 2.00621871e-04  4.51881818e-04  5.41028411e-05  5.85865517e-04\n  6.57841912e-03  3.03919621e-03  2.34850051e-02 -1.23512884e-04\n  3.30282921e-04 -2.72809618e-02  1.39000936e-04 -4.03835310e-04\n -1.80301592e-01 -5.38068479e-02 -9.11733555e-02]\ngradient_107:\n[ 1.99863050e-04  4.48864601e-04  5.31853359e-05  5.84576752e-04\n  6.55731956e-03  3.02892711e-03  2.35068973e-02 -1.22980640e-04\n  3.29784560e-04 -2.73308609e-02  1.38534179e-04 -4.03466744e-04\n -1.80647584e-01 -5.38198332e-02 -9.14738204e-02]\ngradient_108:\n[ 1.99104130e-04  4.45847584e-04  5.22680246e-05  5.83308618e-04\n  6.53639531e-03  3.01873302e-03  2.35286790e-02 -1.22448979e-04\n  3.29298875e-04 -2.73806757e-02  1.38066896e-04 -4.03112582e-04\n -1.80993773e-01 -5.38325535e-02 -9.17750621e-02]\ngradient_109:\n[ 1.98345118e-04  4.42830802e-04  5.13509198e-05  5.82060754e-04\n  6.51564389e-03  3.00861298e-03  2.35503497e-02 -1.21917905e-04\n  3.28825595e-04 -2.74304046e-02  1.37599092e-04 -4.02772535e-04\n -1.81340155e-01 -5.38450083e-02 -9.20770801e-02]\ngradient_110:\n[ 1.97586022e-04  4.39814292e-04  5.04340340e-05  5.80832804e-04\n  6.49506287e-03  2.99856602e-03  2.35719088e-02 -1.21387424e-04\n  3.28364449e-04 -2.74800459e-02  1.37130772e-04 -4.02446320e-04\n -1.81686728e-01 -5.38571970e-02 -9.23798741e-02]\n",
      "gradient_111:\n[ 1.96826849e-04  4.36798090e-04  4.95173796e-05  5.79624420e-04\n  6.47464987e-03  2.98859117e-03  2.35933558e-02 -1.20857540e-04\n  3.27915177e-04 -2.75295983e-02  1.36661940e-04 -4.02133659e-04\n -1.82033490e-01 -5.38691191e-02 -9.26834439e-02]\ngradient_112:\n[ 1.96067606e-04  4.33782231e-04  4.86009693e-05  5.78435258e-04\n  6.45440254e-03  2.97868751e-03  2.36146901e-02 -1.20328258e-04\n  3.27477519e-04 -2.75790600e-02  1.36192602e-04 -4.01834276e-04\n -1.82380437e-01 -5.38807741e-02 -9.29877889e-02]\ngradient_113:\n[ 1.95308301e-04  4.30766752e-04  4.76848156e-05  5.77264980e-04\n  6.43431855e-03  2.96885411e-03  2.36359112e-02 -1.19799582e-04\n  3.27051223e-04 -2.76284296e-02  1.35722762e-04 -4.01547903e-04\n -1.82727567e-01 -5.38921616e-02 -9.32929088e-02]\ngradient_114:\n[ 1.94548942e-04  4.27751688e-04  4.67689310e-05  5.76113252e-04\n  6.41439563e-03  2.95909005e-03  2.36570185e-02 -1.19271519e-04\n  3.26636039e-04 -2.76777055e-02  1.35252425e-04 -4.01274273e-04\n -1.83074878e-01 -5.39032811e-02 -9.35988032e-02]\ngradient_115:\n[ 1.93789535e-04  4.24737076e-04  4.58533280e-05  5.74979748e-04\n  6.39463153e-03  2.94939445e-03  2.36780116e-02 -1.18744071e-04\n  3.26231723e-04 -2.77268862e-02  1.34781597e-04 -4.01013126e-04\n -1.83422366e-01 -5.39141322e-02 -9.39054716e-02]\ngradient_116:\n[ 1.93030088e-04  4.21722952e-04  4.49380193e-05  5.73864146e-04\n  6.37502404e-03  2.93976642e-03  2.36988899e-02 -1.18217245e-04\n  3.25838036e-04 -2.77759700e-02  1.34310282e-04 -4.00764204e-04\n -1.83770030e-01 -5.39247144e-02 -9.42129135e-02]\ngradient_117:\n[ 1.92270610e-04  4.18709352e-04  4.40230174e-05  5.72766128e-04\n  6.35557100e-03  2.93020510e-03  2.37196529e-02 -1.17691045e-04\n  3.25454742e-04 -2.78249555e-02  1.33838485e-04 -4.00527256e-04\n -1.84117865e-01 -5.39350273e-02 -9.45211284e-02]\ngradient_118:\n[ 1.91511107e-04  4.15696313e-04  4.31083349e-05  5.71685381e-04\n  6.33627026e-03  2.92070962e-03  2.37403001e-02 -1.17165475e-04\n  3.25081611e-04 -2.78738410e-02  1.33366212e-04 -4.00302031e-04\n -1.84465871e-01 -5.39450705e-02 -9.48301158e-02]\ngradient_119:\n[ 1.90751586e-04  4.12683870e-04  4.21939844e-05  5.70621600e-04\n  6.31711970e-03  2.91127915e-03  2.37608311e-02 -1.16640540e-04\n  3.24718415e-04 -2.79226250e-02  1.32893468e-04 -4.00088285e-04\n -1.84814044e-01 -5.39548438e-02 -9.51398751e-02]",
      "\ngradient_120:\n[ 1.89992056e-04  4.09672060e-04  4.12799784e-05  5.69574481e-04\n  6.29811727e-03  2.90191287e-03  2.37812453e-02 -1.16116245e-04\n  3.24364932e-04 -2.79713060e-02  1.32420257e-04 -3.99885777e-04\n -1.85162381e-01 -5.39643466e-02 -9.54504058e-02]\ngradient_121:\n[ 1.89232524e-04  4.06660919e-04  4.03663296e-05  5.68543727e-04\n  6.27926092e-03  2.89260994e-03  2.38015422e-02 -1.15592594e-04\n  3.24020944e-04 -2.80198823e-02  1.31946585e-04 -3.99694270e-04\n -1.85510880e-01 -5.39735788e-02 -9.57617071e-02]\ngradient_122:\n[ 1.88472998e-04  4.03650484e-04  3.94530507e-05  5.67529046e-04\n  6.26054862e-03  2.88336958e-03  2.38217213e-02 -1.15069592e-04\n  3.23686235e-04 -2.80683525e-02  1.31472458e-04 -3.99513530e-04\n -1.85859538e-01 -5.39825399e-02 -9.60737785e-02]\ngradient_123:\n[ 1.87713485e-04  4.00640791e-04  3.85401541e-05  5.66530148e-04\n  6.24197841e-03  2.87419099e-03  2.38417823e-02 -1.14547244e-04\n  3.23360595e-04 -2.81167150e-02  1.30997879e-04 -3.99343327e-04\n -1.86208353e-01 -5.39912297e-02 -9.63866193e-02]\ngradient_124:\n[ 1.86953993e-04  3.97631877e-04  3.76276526e-05  5.65546750e-04\n  6.22354834e-03  2.86507339e-03  2.38617245e-02 -1.14025555e-04\n  3.23043818e-04 -2.81649681e-02  1.30522856e-04 -3.99183436e-04\n -1.86557322e-01 -5.39996478e-02 -9.67002287e-02]\ngradient_125:\n[ 1.86194530e-04  3.94623778e-04  3.67155587e-05  5.64578573e-04\n  6.20525648e-03  2.85601601e-03  2.38815476e-02 -1.13504528e-04\n  3.22735699e-04 -2.82131104e-02  1.30047393e-04 -3.99033634e-04\n -1.86906443e-01 -5.40077942e-02 -9.70146062e-02]\ngradient_126:\n[ 1.85435103e-04  3.91616530e-04  3.58038852e-05  5.63625342e-04\n  6.18710094e-03  2.84701809e-03  2.39012511e-02 -1.12984168e-04\n  3.22436039e-04 -2.82611403e-02  1.29571495e-04 -3.98893701e-04\n -1.87255713e-01 -5.40156684e-02 -9.73297508e-02]\ngradient_127:\n[ 1.84675719e-04  3.88610171e-04  3.48926446e-05  5.62686785e-04\n  6.16907987e-03  2.83807890e-03  2.39208346e-02 -1.12464480e-04\n  3.22144644e-04 -2.83090563e-02  1.29095167e-04 -3.98763421e-04\n -1.87605129e-01 -5.40232702e-02 -9.76456619e-02]\ngradient_128:\n[ 1.83916388e-04  3.85604737e-04  3.39818497e-05  5.61762637e-04\n  6.15119142e-03  2.82919770e-03  2.39402976e-02 -1.11945468e-04\n  3.21861320e-04 -2.83568567e-02  1.28618416e-04 -3.98642583e-04\n -1.87954689e-01 -5.40305995e-02 -9.79623386e-02]",
      "\ngradient_129:\n[ 1.83157115e-04  3.82600265e-04  3.30715130e-05  5.60852634e-04\n  6.13343380e-03  2.82037377e-03  2.39596396e-02 -1.11427137e-04\n  3.21585878e-04 -2.84045400e-02  1.28141247e-04 -3.98530977e-04\n -1.88304390e-01 -5.40376560e-02 -9.82797801e-02]\ngradient_130:\n[ 1.82397910e-04  3.79596791e-04  3.21616472e-05  5.59956519e-04\n  6.11580522e-03  2.81160639e-03  2.39788603e-02 -1.10909491e-04\n  3.21318133e-04 -2.84521047e-02  1.27663664e-04 -3.98428397e-04\n -1.88654230e-01 -5.40444395e-02 -9.85979856e-02]\ngradient_131:\n[ 1.81638779e-04  3.76594352e-04  3.12522651e-05  5.59074038e-04\n  6.09830393e-03  2.80289487e-03  2.39979592e-02 -1.10392535e-04\n  3.21057903e-04 -2.84995492e-02  1.27185674e-04 -3.98334640e-04\n -1.89004206e-01 -5.40509499e-02 -9.89169541e-02]\ngradient_132:\n[ 1.80879732e-04  3.73592986e-04  3.03433792e-05  5.58204939e-04\n  6.08092821e-03  2.79423852e-03  2.40169359e-02 -1.09876273e-04\n  3.20805010e-04 -2.85468719e-02  1.26707283e-04 -3.98249506e-04\n -1.89354316e-01 -5.40571871e-02 -9.92366848e-02]\ngradient_133:\n[ 1.80120774e-04  3.70592728e-04  2.94350023e-05  5.57348976e-04\n  6.06367636e-03  2.78563665e-03  2.40357900e-02 -1.09360709e-04\n  3.20559277e-04 -2.85940714e-02  1.26228495e-04 -3.98172800e-04\n -1.89704557e-01 -5.40631508e-02 -9.95571767e-02]\ngradient_134:\n[ 1.79361915e-04  3.67593616e-04  2.85271471e-05  5.56505907e-04\n  6.04654671e-03  2.77708859e-03  2.40545211e-02 -1.08845848e-04\n  3.20320533e-04 -2.86411460e-02  1.25749316e-04 -3.98104326e-04\n -1.90054926e-01 -5.40688410e-02 -9.98784288e-02]\ngradient_135:\n[ 1.78603161e-04  3.64595687e-04  2.76198262e-05  5.55675492e-04\n  6.02953760e-03  2.76859370e-03  2.40731288e-02 -1.08331693e-04\n  3.20088608e-04 -2.86880942e-02  1.25269752e-04 -3.98043896e-04\n -1.90405422e-01 -5.40742576e-02 -1.00200440e-01]\ngradient_136:\n[ 1.77844522e-04  3.61598977e-04  2.67130523e-05  5.54857496e-04\n  6.01264741e-03  2.76015130e-03  2.40916126e-02 -1.07818251e-04\n  3.19863336e-04 -2.87349145e-02  1.24789809e-04 -3.97991320e-04\n -1.90756041e-01 -5.40794005e-02 -1.00523210e-01]",
      "\ngradient_137:\n[ 1.77086004e-04  3.58603524e-04  2.58068381e-05  5.54051688e-04\n  5.99587455e-03  2.75176078e-03  2.41099723e-02 -1.07305524e-04\n  3.19644553e-04 -2.87816053e-02  1.24309492e-04 -3.97946415e-04\n -1.91106782e-01 -5.40842696e-02 -1.00846736e-01]\ngradient_138:\n[ 1.76327615e-04  3.55609364e-04  2.49011964e-05  5.53257838e-04\n  5.97921744e-03  2.74342148e-03  2.41282074e-02 -1.06793516e-04\n  3.19432100e-04 -2.88281650e-02  1.23828808e-04 -3.97908997e-04\n -1.91457641e-01 -5.40888648e-02 -1.01171019e-01]\ngradient_139:\n[ 1.75569364e-04  3.52616536e-04  2.39961398e-05  5.52475722e-04\n  5.96267453e-03  2.73513280e-03  2.41463175e-02 -1.06282233e-04\n  3.19225818e-04 -2.88745922e-02  1.23347761e-04 -3.97878888e-04\n -1.91808616e-01 -5.40931861e-02 -1.01496057e-01]\ngradient_140:\n[ 1.74811259e-04  3.49625075e-04  2.30916810e-05  5.51705119e-04\n  5.94624429e-03  2.72689413e-03  2.41643024e-02 -1.05771679e-04\n  3.19025554e-04 -2.89208853e-02  1.22866358e-04 -3.97855911e-04\n -1.92159704e-01 -5.40972335e-02 -1.01821849e-01]\ngradient_141:\n[ 1.74053306e-04  3.46635019e-04  2.21878327e-05  5.50945811e-04\n  5.92992520e-03  2.71870484e-03  2.41821615e-02 -1.05261857e-04\n  3.18831156e-04 -2.89670427e-02  1.22384604e-04 -3.97839892e-04\n -1.92510904e-01 -5.41010069e-02 -1.02148393e-01]\ngradient_142:\n[ 1.73295515e-04  3.43646405e-04  2.12846077e-05  5.50197583e-04\n  5.91371580e-03  2.71056436e-03  2.41998946e-02 -1.04752771e-04\n  3.18642474e-04 -2.90130629e-02  1.21902506e-04 -3.97830659e-04\n -1.92862213e-01 -5.41045064e-02 -1.02475689e-01]\ngradient_143:\n[ 1.72537893e-04  3.40659270e-04  2.03820186e-05  5.49460224e-04\n  5.89761461e-03  2.70247210e-03  2.42175013e-02 -1.04244427e-04\n  3.18459363e-04 -2.90589444e-02  1.21420069e-04 -3.97828044e-04\n -1.93213628e-01 -5.41077320e-02 -1.02803735e-01]\ngradient_144:\n[ 1.71780448e-04  3.37673652e-04  1.94800781e-05  5.48733525e-04\n  5.88162020e-03  2.69442748e-03  2.42349813e-02 -1.03736828e-04\n  3.18281677e-04 -2.91046856e-02  1.20937300e-04 -3.97831880e-04\n -1.93565146e-01 -5.41106837e-02 -1.03132531e-01]\ngradient_145:\n[ 1.71023187e-04  3.34689587e-04  1.85787991e-05  5.48017282e-04\n  5.86573115e-03  2.68642992e-03  2.42523342e-02 -1.03229977e-04\n  3.18109278e-04 -2.91502851e-02  1.20454203e-04 -3.97842004e-04\n -1.93916766e-01 -5.41133615e-02 -1.03462074e-01]\n",
      "gradient_146:\n[ 1.70266120e-04  3.31707113e-04  1.76781941e-05  5.47311293e-04\n  5.84994606e-03  2.67847888e-03  2.42695598e-02 -1.02723880e-04\n  3.17942025e-04 -2.91957413e-02  1.19970786e-04 -3.97858254e-04\n -1.94268485e-01 -5.41157655e-02 -1.03792363e-01]\ngradient_147:\n[ 1.69509254e-04  3.28726268e-04  1.67782758e-05  5.46615358e-04\n  5.83426355e-03  2.67057379e-03  2.42866575e-02 -1.02218541e-04\n  3.17779782e-04 -2.92410526e-02  1.19487054e-04 -3.97880472e-04\n -1.94620301e-01 -5.41178959e-02 -1.04123398e-01]\ngradient_148:\n[ 1.68752597e-04  3.25747088e-04  1.58790571e-05  5.45929282e-04\n  5.81868227e-03  2.66271411e-03  2.43036273e-02 -1.01713962e-04\n  3.17622417e-04 -2.92862176e-02  1.19003014e-04 -3.97908500e-04\n -1.94972210e-01 -5.41197525e-02 -1.04455176e-01]\ngradient_149:\n[ 1.67996157e-04  3.22769611e-04  1.49805506e-05  5.45252873e-04\n  5.80320087e-03  2.65489932e-03  2.43204687e-02 -1.01210149e-04\n  3.17469798e-04 -2.93312348e-02  1.18518671e-04 -3.97942185e-04\n -1.95324211e-01 -5.41213357e-02 -1.04787697e-01]\ngradient_150:\n[ 1.67239942e-04  3.19793874e-04  1.40827689e-05  5.44585939e-04\n  5.78781805e-03  2.64712886e-03  2.43371814e-02 -1.00707105e-04\n  3.17321795e-04 -2.93761026e-02  1.18034031e-04 -3.97981374e-04\n -1.95676302e-01 -5.41226454e-02 -1.05120959e-01]\ngradient_151:\n[ 1.66483961e-04  3.16819914e-04  1.31857249e-05  5.43928294e-04\n  5.77253250e-03  2.63940224e-03  2.43537651e-02 -1.00204835e-04\n  3.17178283e-04 -2.94208195e-02  1.17549101e-04 -3.98025918e-04\n -1.96028479e-01 -5.41236818e-02 -1.05454961e-01]\ngradient_152:\n[ 1.65728220e-04  3.13847770e-04  1.22894311e-05  5.43279753e-04\n  5.75734295e-03  2.63171893e-03  2.43702196e-02 -9.97033415e-05\n  3.17039137e-04 -2.94653840e-02  1.17063887e-04 -3.98075669e-04\n -1.96380741e-01 -5.41244451e-02 -1.05789701e-01]\ngradient_153:\n[ 1.64972729e-04  3.10877478e-04  1.13939004e-05  5.42640136e-04\n  5.74224813e-03  2.62407842e-03  2.43865445e-02 -9.92026292e-05\n  3.16904235e-04 -2.95097946e-02  1.16578395e-04 -3.98130482e-04\n -1.96733084e-01 -5.41249353e-02 -1.06125177e-01]\ngradient_154:\n[ 1.64217496e-04  3.07909076e-04  1.04991453e-05  5.42009262e-04\n  5.72724680e-03  2.61648022e-03  2.44027396e-02 -9.87027019e-05\n  3.16773458e-04 -2.95540498e-02  1.16092632e-04 -3.98190214e-04\n -1.97085508e-01 -5.41251528e-02 -1.06461389e-01]\ngradient_155:\n[ 1.63462528e-04  3.04942600e-04  9.60517859e-06  5.41386957e-04\n  5.71233774e-03  2.60892384e-03  2.44188046e-02 -9.82035635e-05\n  3.16646687e-04 -2.95981482e-02  1.15606604e-04 -3.98254724e-04\n -1.97438009e-01 -5.41250976e-02 -1.06798334e-01]",
      "\ngradient_156:\n[ 1.62707833e-04  3.01978090e-04  8.71201293e-06  5.40773046e-04\n  5.69751974e-03  2.60140878e-03  2.44347392e-02 -9.77052177e-05\n  3.16523808e-04 -2.96420882e-02  1.15120316e-04 -3.98323872e-04\n -1.97790585e-01 -5.41247699e-02 -1.07136011e-01]\ngradient_157:\n[ 1.61953421e-04  2.99015582e-04  7.81966100e-06  5.40167359e-04\n  5.68279162e-03  2.59393459e-03  2.44505431e-02 -9.72076684e-05\n  3.16404706e-04 -2.96858684e-02  1.14633776e-04 -3.98397523e-04\n -1.98143233e-01 -5.41241700e-02 -1.07474418e-01]\ngradient_158:\n[ 1.61199299e-04  2.96055113e-04  6.92813547e-06  5.39569727e-04\n  5.66815220e-03  2.58650077e-03  2.44662162e-02 -9.67109194e-05\n  3.16289271e-04 -2.97294872e-02  1.14146990e-04 -3.98475540e-04\n -1.98495952e-01 -5.41232980e-02 -1.07813555e-01]\ngradient_159:\n[ 1.60445474e-04  2.93096722e-04  6.03744900e-06  5.38979985e-04\n  5.65360033e-03  2.57910687e-03  2.44817581e-02 -9.62149743e-05\n  3.16177394e-04 -2.97729432e-02  1.13659964e-04 -3.98557791e-04\n -1.98848738e-01 -5.41221543e-02 -1.08153418e-01]\ngradient_160:\n[ 1.59691956e-04  2.90140445e-04  5.14761424e-06  5.38397969e-04\n  5.63913486e-03  2.57175244e-03  2.44971685e-02 -9.57198371e-05\n  3.16068966e-04 -2.98162350e-02  1.13172704e-04 -3.98644145e-04\n -1.99201590e-01 -5.41207390e-02 -1.08494007e-01]\ngradient_161:\n[ 1.58938753e-04  2.87186320e-04  4.25864385e-06  5.37823519e-04\n  5.62475469e-03  2.56443701e-03  2.45124473e-02 -9.52255113e-05\n  3.15963883e-04 -2.98593610e-02  1.12685218e-04 -3.98734473e-04\n -1.99554506e-01 -5.41190524e-02 -1.08835320e-01]\ngradient_162:\n[ 1.58185872e-04  2.84234385e-04  3.37055047e-06  5.37256476e-04\n  5.61045871e-03  2.55716015e-03  2.45275943e-02 -9.47320007e-05\n  3.15862041e-04 -2.99023198e-02  1.12197511e-04 -3.98828648e-04\n -1.99907482e-01 -5.41170947e-02 -1.09177354e-01]\ngradient_163:\n[ 1.57433323e-04  2.81284676e-04  2.48334673e-06  5.36696684e-04\n  5.59624581e-03  2.54992142e-03  2.45426091e-02 -9.42393088e-05\n  3.15763338e-04 -2.99451099e-02  1.11709591e-04 -3.98926544e-04\n -2.00260517e-01 -5.41148664e-02 -1.09520109e-01]\ngradient_164:\n[ 1.56681112e-04  2.78337233e-04  1.59704526e-06  5.36143989e-04\n  5.58211495e-03  2.54272038e-03  2.45574916e-02 -9.37474394e-05\n  3.15667676e-04 -2.99877299e-02  1.11221463e-04 -3.99028039e-04\n -2.00613608e-01 -5.41123676e-02 -1.09863582e-01]\n",
      "gradient_165:\n[ 1.55929249e-04  2.75392091e-04  7.11658662e-07  5.35598240e-04\n  5.56806505e-03  2.53555661e-03  2.45722416e-02 -9.32563960e-05\n  3.15574955e-04 -3.00301784e-02  1.10733135e-04 -3.99133010e-04\n -2.00966753e-01 -5.41095986e-02 -1.10207772e-01]\ngradient_166:\n[ 1.55177742e-04  2.72449290e-04 -1.72800455e-07  5.35059288e-04\n  5.55409507e-03  2.52842970e-03  2.45868587e-02 -9.27661822e-05\n  3.15485080e-04 -3.00724539e-02  1.10244613e-04 -3.99241337e-04\n -2.01319950e-01 -5.41065598e-02 -1.10552677e-01]\ngradient_167:\n[ 1.54426599e-04  2.69508865e-04 -1.05631950e-06  5.34526985e-04\n  5.54020399e-03  2.52133922e-03  2.46013430e-02 -9.22768015e-05\n  3.15397957e-04 -3.01145549e-02  1.09755903e-04 -3.99352903e-04\n -2.01673196e-01 -5.41032516e-02 -1.10898295e-01]\ngradient_168:\n[ 1.53675828e-04  2.66570855e-04 -1.93888589e-06  5.34001188e-04\n  5.52639079e-03  2.51428476e-03  2.46156940e-02 -9.17882576e-05\n  3.15313493e-04 -3.01564800e-02  1.09267013e-04 -3.99467590e-04\n -2.02026490e-01 -5.40996742e-02 -1.11244623e-01]\ngradient_169:\n[ 1.52925437e-04  2.63635298e-04 -2.82048706e-06  5.33481754e-04\n  5.51265448e-03  2.50726594e-03  2.46299117e-02 -9.13005538e-05\n  3.15231597e-04 -3.01982279e-02  1.08777949e-04 -3.99585286e-04\n -2.02379828e-01 -5.40958280e-02 -1.11591661e-01]\ngradient_170:\n[ 1.52175435e-04  2.60702230e-04 -3.70111044e-06  5.32968542e-04\n  5.49899407e-03  2.50028234e-03  2.46439959e-02 -9.08136936e-05\n  3.15152179e-04 -3.02397971e-02  1.08288718e-04 -3.99705875e-04\n -2.02733209e-01 -5.40917133e-02 -1.11939405e-01]\ngradient_171:\n[ 1.51425831e-04  2.57771689e-04 -4.58074351e-06  5.32461414e-04\n  5.48540860e-03  2.49333359e-03  2.46579463e-02 -9.03276805e-05\n  3.15075154e-04 -3.02811861e-02  1.07799327e-04 -3.99829248e-04\n -2.03086630e-01 -5.40873307e-02 -1.12287855e-01]\ngradient_172:\n[ 1.50676631e-04  2.54843713e-04 -5.45937373e-06  5.31960233e-04\n  5.47189711e-03  2.48641929e-03  2.46717628e-02 -8.98425180e-05\n  3.15000433e-04 -3.03223937e-02  1.07309782e-04 -3.99955294e-04\n -2.03440089e-01 -5.40826803e-02 -1.12637008e-01]\ngradient_173:\n[ 1.49927846e-04  2.51918338e-04 -6.33698859e-06  5.31464866e-04\n  5.45845865e-03  2.47953906e-03  2.46854453e-02 -8.93582093e-05\n  3.14927934e-04 -3.03634183e-02  1.06820090e-04 -4.00083905e-04\n -2.03793585e-01 -5.40777627e-02 -1.12986862e-01]",
      "\ngradient_174:\n[ 1.49179482e-04  2.48995604e-04 -7.21357558e-06  5.30975181e-04\n  5.44509230e-03  2.47269253e-03  2.46989935e-02 -8.88747578e-05\n  3.14857574e-04 -3.04042586e-02  1.06330258e-04 -4.00214975e-04\n -2.04147113e-01 -5.40725783e-02 -1.13337415e-01]\ngradient_175:\n[ 1.48431549e-04  2.46075546e-04 -8.08912223e-06  5.30491047e-04\n  5.43179714e-03  2.46587933e-03  2.47124074e-02 -8.83921670e-05\n  3.14789270e-04 -3.04449131e-02  1.05840293e-04 -4.00348400e-04\n -2.04500674e-01 -5.40671274e-02 -1.13688664e-01]\ngradient_176:\n[ 1.47684054e-04  2.43158203e-04 -8.96361606e-06  5.30012336e-04\n  5.41857228e-03  2.45909910e-03  2.47256867e-02 -8.79104400e-05\n  3.14722944e-04 -3.04853806e-02  1.05350203e-04 -4.00484074e-04\n -2.04854263e-01 -5.40614105e-02 -1.14040609e-01]\ngradient_177:\n[ 1.46937007e-04  2.40243612e-04 -9.83704463e-06  5.29538922e-04\n  5.40541682e-03  2.45235148e-03  2.47388313e-02 -8.74295802e-05\n  3.14658517e-04 -3.05256597e-02  1.04859993e-04 -4.00621897e-04\n -2.05207879e-01 -5.40554281e-02 -1.14393246e-01]\ngradient_178:\n[ 1.46190415e-04  2.37331810e-04 -1.07093955e-05  5.29070681e-04\n  5.39232989e-03  2.44563611e-03  2.47518411e-02 -8.69495908e-05\n  3.14595913e-04 -3.05657489e-02  1.04369671e-04 -4.00761769e-04\n -2.05561520e-01 -5.40491806e-02 -1.14746574e-01]\ngradient_179:\n[ 1.45444287e-04  2.34422834e-04 -1.15806563e-05  5.28607490e-04\n  5.37931063e-03  2.43895264e-03  2.47647159e-02 -8.64704751e-05\n  3.14535055e-04 -3.06056470e-02  1.03879243e-04 -4.00903590e-04\n -2.05915184e-01 -5.40426685e-02 -1.15100589e-01]\ngradient_180:\n[ 1.44698631e-04  2.31516723e-04 -1.24508145e-05  5.28149227e-04\n  5.36635817e-03  2.43230074e-03  2.47774557e-02 -8.59922362e-05\n  3.14475871e-04 -3.06453525e-02  1.03388718e-04 -4.01047263e-04\n -2.06268868e-01 -5.40358922e-02 -1.15455291e-01]\ngradient_181:\n[ 1.43953455e-04  2.28613512e-04 -1.33198579e-05  5.27695776e-04\n  5.35347169e-03  2.42568005e-03  2.47900602e-02 -8.55148773e-05\n  3.14418288e-04 -3.06848642e-02  1.02898102e-04 -4.01192692e-04\n -2.06622570e-01 -5.40288522e-02 -1.15810677e-01]",
      "\ngradient_182:\n[ 1.43208769e-04  2.25713241e-04 -1.41877741e-05  5.27247018e-04\n  5.34065036e-03  2.41909026e-03  2.48025294e-02 -8.50384016e-05\n  3.14362235e-04 -3.07241807e-02  1.02407401e-04 -4.01339782e-04\n -2.06976288e-01 -5.40215491e-02 -1.16166744e-01]\ngradient_183:\n[ 1.42464579e-04  2.22815945e-04 -1.50545507e-05  5.26802838e-04\n  5.32789337e-03  2.41253102e-03  2.48148632e-02 -8.45628121e-05\n  3.14307642e-04 -3.07633006e-02  1.01916624e-04 -4.01488441e-04\n -2.07330020e-01 -5.40139833e-02 -1.16523490e-01]\ngradient_184:\n[ 1.41720896e-04  2.19921661e-04 -1.59201754e-05  5.26363123e-04\n  5.31519990e-03  2.40600202e-03  2.48270614e-02 -8.40881120e-05\n  3.14254440e-04 -3.08022227e-02  1.01425777e-04 -4.01638576e-04\n -2.07683763e-01 -5.40061554e-02 -1.16880913e-01]\ngradient_185:\n[ 1.40977726e-04  2.17030429e-04 -1.67846359e-05  5.25927761e-04\n  5.30256917e-03  2.39950292e-03  2.48391240e-02 -8.36143042e-05\n  3.14202562e-04 -3.08409456e-02  1.00934867e-04 -4.01790097e-04\n -2.08037516e-01 -5.39980658e-02 -1.17239010e-01]\ngradient_186:\n[ 1.40235079e-04  2.14142283e-04 -1.76479200e-05  5.25496641e-04\n  5.29000040e-03  2.39303343e-03  2.48510509e-02 -8.31413920e-05\n  3.14151943e-04 -3.08794680e-02  1.00443902e-04 -4.01942915e-04\n -2.08391277e-01 -5.39897152e-02 -1.17597779e-01]\ngradient_187:\n[ 1.39492963e-04  2.11257262e-04 -1.85100154e-05  5.25069656e-04\n  5.27749282e-03  2.38659321e-03  2.48628419e-02 -8.26693781e-05\n  3.14102518e-04 -3.09177887e-02  9.99528880e-05 -4.02096943e-04\n -2.08745042e-01 -5.39811040e-02 -1.17957218e-01]\ngradient_188:\n[ 1.38751386e-04  2.08375402e-04 -1.93709099e-05  5.24646699e-04\n  5.26504566e-03  2.38018198e-03  2.48744971e-02 -8.21982657e-05\n  3.14054224e-04 -3.09559063e-02  9.94618333e-05 -4.02252093e-04\n -2.09098811e-01 -5.39722328e-02 -1.18317324e-01]\ngradient_189:\n[ 1.38010356e-04  2.05496742e-04 -2.02305913e-05  5.24227664e-04\n  5.25265819e-03  2.37379942e-03  2.48860162e-02 -8.17280576e-05\n  3.14006998e-04 -3.09938196e-02  9.89707448e-05 -4.02408281e-04\n -2.09452580e-01 -5.39631023e-02 -1.18678095e-01]\n",
      "gradient_190:\n[ 1.37269883e-04  2.02621316e-04 -2.10890474e-05  5.23812448e-04\n  5.24032967e-03  2.36744523e-03  2.48973993e-02 -8.12587567e-05\n  3.13960780e-04 -3.10315273e-02  9.84796298e-05 -4.02565423e-04\n -2.09806349e-01 -5.39537129e-02 -1.19039528e-01]\ngradient_191:\n[ 1.36529975e-04  1.99749164e-04 -2.19462661e-05  5.23400949e-04\n  5.22805937e-03  2.36111912e-03  2.49086463e-02 -8.07903661e-05\n  3.13915510e-04 -3.10690281e-02  9.79884954e-05 -4.02723435e-04\n -2.10160114e-01 -5.39440652e-02 -1.19401620e-01]\ngradient_192:\n[ 1.35790639e-04  1.96880320e-04 -2.28022353e-05  5.22993066e-04\n  5.21584657e-03  2.35482081e-03  2.49197571e-02 -8.03228884e-05\n  3.13871129e-04 -3.11063208e-02  9.74973491e-05 -4.02882238e-04\n -2.10513873e-01 -5.39341599e-02 -1.19764370e-01]\ngradient_193:\n[ 1.35051885e-04  1.94014823e-04 -2.36569428e-05  5.22588701e-04\n  5.20369057e-03  2.34855000e-03  2.49307317e-02 -7.98563266e-05\n  3.13827580e-04 -3.11434041e-02  9.70061981e-05 -4.03041751e-04\n -2.10867626e-01 -5.39239975e-02 -1.20127774e-01]\ngradient_194:\n[ 1.34313720e-04  1.91152710e-04 -2.45103767e-05  5.22187757e-04\n  5.19159067e-03  2.34230641e-03  2.49415700e-02 -7.93906834e-05\n  3.13784806e-04 -3.11802768e-02  9.65150498e-05 -4.03201894e-04\n -2.11221368e-01 -5.39135787e-02 -1.20491830e-01]\ngradient_195:\n[ 1.33576154e-04  1.88294016e-04 -2.53625249e-05  5.21790137e-04\n  5.17954619e-03  2.33608977e-03  2.49522720e-02 -7.89259617e-05\n  3.13742754e-04 -3.12169377e-02  9.60239115e-05 -4.03362590e-04\n -2.11575099e-01 -5.39029041e-02 -1.20856536e-01]\ngradient_196:\n[ 1.32839194e-04  1.85438778e-04 -2.62133753e-05  5.21395747e-04\n  5.16755646e-03  2.32989979e-03  2.49628376e-02 -7.84621641e-05\n  3.13701367e-04 -3.12533855e-02  9.55327907e-05 -4.03523763e-04\n -2.11928816e-01 -5.38919742e-02 -1.21221888e-01]\ngradient_197:\n[ 1.32102850e-04  1.82587034e-04 -2.70629160e-05  5.21004494e-04\n  5.15562080e-03  2.32373621e-03  2.49732668e-02 -7.79992934e-05\n  3.13660594e-04 -3.12896191e-02  9.50416947e-05 -4.03685336e-04\n -2.12282518e-01 -5.38807899e-02 -1.21587884e-01]\ngradient_198:\n[ 1.31367128e-04  1.79738820e-04 -2.79111352e-05  5.20616287e-04\n  5.14373856e-03  2.31759877e-03  2.49835596e-02 -7.75373523e-05\n  3.13620382e-04 -3.13256373e-02  9.45506310e-05 -4.03847237e-04\n -2.12636201e-01 -5.38693516e-02 -1.21954521e-01]\ngradient_199:\n[ 1.30632039e-04  1.76894173e-04 -2.87580207e-05  5.20231036e-04\n  5.13190910e-03  2.31148719e-03  2.49937160e-02 -7.70763434e-05\n  3.13580681e-04 -3.13614388e-02  9.40596070e-05 -4.04009391e-04\n -2.12989865e-01 -5.38576600e-02 -1.22321797e-01]\n",
      "Istrenirani model: \nModel:\nW_0: 0 -> 1\n[[-0.55105185 -0.72860214 -0.55017001]\n [-0.18003616 -0.4497094  -0.24954348]]\nW_1: 1 -> 2\n[[-1.09546198 -0.6210751   0.60219371]\n [ 0.31379878  0.93943492 -0.64979271]]\nW_2: 2 -> 3\n[[-1.09546198 -0.6210751   0.60219371]]\n"
     ],
     "output_type": "stream"
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "nn.fit(X, y, alpha=10e-2, num_iter=200, plot=True)\n",
    "\n",
    "print(f'Istrenirani model: ')\n",
    "nn.print_model()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "outputs": [],
   "source": [
    "petal_length = float(input('Unesite petal.length:'))\n",
    "petal_width = float(input('Unesite petal.width:'))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "0.4397556076768547\nVersicolor\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "x = np.array([1, petal_length, petal_width])\n",
    "y_predict = hypothesis_neural(nn.model, x)\n",
    "\n",
    "print(y_predict)\n",
    "\n",
    "if y_predict < 0.5:\n",
    "    print(class_to_name_dict[0])\n",
    "else:\n",
    "    print(class_to_name_dict[1])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "y_pred: 0.4397556076768547\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "y_pred = nn.predict(x)\n",
    "print(f'y_pred: {y_pred}')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}