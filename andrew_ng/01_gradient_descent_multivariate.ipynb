{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "def loss_function(X, y, w):\n",
    "    N = y.shape[0]\n",
    "    y_predict = X.dot(w)\n",
    "    diff = y_predict - y\n",
    "    \n",
    "    gradient = np.zeros(w.shape[0])\n",
    "    gradient = 1/N * (X.transpose().dot(diff))\n",
    "    \n",
    "    loss = 1/(2*N) * np.sum(diff ** 2)\n",
    "    \n",
    "    return loss, gradient"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [],
   "source": [
    "def gradient_descent(X, y, w, alpha=0.01, num_iter=100, eps=10e-5):\n",
    "    loss_history = np.zeros((num_iter, 1))\n",
    "    it_break = num_iter\n",
    "    \n",
    "    for i in range(num_iter):\n",
    "        loss, gradient = loss_function(X, y, w)        \n",
    "        w = w - alpha * gradient\n",
    "        \n",
    "        loss_history[i] = loss\n",
    "        # print(f'Iteration {i + 1}: Loss = {loss}')\n",
    "        \n",
    "        if i > 0 and np.abs(loss - loss_history[i-1]) <= eps:\n",
    "            it_break = i\n",
    "            break\n",
    "    \n",
    "    return w, loss_history, it_break"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "outputs": [],
   "source": [
    "def train_analytic(X, y):\n",
    "    X_T = X.transpose()\n",
    "    return np.linalg.pinv(X_T.dot(X)).dot(X_T).dot(y)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def show_model(model):\n",
    "    n = model.shape[0]\n",
    "    \n",
    "    print('f_w(x) = ', end='')\n",
    "    for i in range(n):\n",
    "        if i == 0:\n",
    "            print(f'{round(model[i][0], 3)} + ', end='')\n",
    "        elif i < n - 1:\n",
    "            print(f'{round(model[i][0], 3)}*x{i-1} + ', end='')\n",
    "        else:\n",
    "            print(f'{round(model[i][0], 3)}*x{i-1}', end='')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "def hypothesis(w, x):\n",
    "    return w.transpose().dot(x)[0][0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "    mpg  cylinders  displacement  horsepower  weight  acceleration  model-year\n0  18.0          8         307.0       130.0    3504          12.0          70\n1  15.0          8         350.0       165.0    3693          11.5          70\n2  18.0          8         318.0       150.0    3436          11.0          70\n3  16.0          8         304.0       150.0    3433          12.0          70\n4  17.0          8         302.0       140.0    3449          10.5          70",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>mpg</th>\n      <th>cylinders</th>\n      <th>displacement</th>\n      <th>horsepower</th>\n      <th>weight</th>\n      <th>acceleration</th>\n      <th>model-year</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>18.0</td>\n      <td>8</td>\n      <td>307.0</td>\n      <td>130.0</td>\n      <td>3504</td>\n      <td>12.0</td>\n      <td>70</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>15.0</td>\n      <td>8</td>\n      <td>350.0</td>\n      <td>165.0</td>\n      <td>3693</td>\n      <td>11.5</td>\n      <td>70</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>18.0</td>\n      <td>8</td>\n      <td>318.0</td>\n      <td>150.0</td>\n      <td>3436</td>\n      <td>11.0</td>\n      <td>70</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>16.0</td>\n      <td>8</td>\n      <td>304.0</td>\n      <td>150.0</td>\n      <td>3433</td>\n      <td>12.0</td>\n      <td>70</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>17.0</td>\n      <td>8</td>\n      <td>302.0</td>\n      <td>140.0</td>\n      <td>3449</td>\n      <td>10.5</td>\n      <td>70</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 7
    }
   ],
   "source": [
    "df = pd.read_csv('../datasets/auto-mpg.csv')\n",
    "df.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "df = df.replace('?', np.nan)\n",
    "df = df.dropna()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "0    18.0\n1    15.0\n2    18.0\n3    16.0\n4    17.0\nName: mpg, dtype: float64"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 9
    }
   ],
   "source": [
    "y = df['mpg']\n",
    "y.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "   cylinders  displacement  horsepower  weight  acceleration  model-year\n0          8         307.0       130.0    3504          12.0          70\n1          8         350.0       165.0    3693          11.5          70\n2          8         318.0       150.0    3436          11.0          70\n3          8         304.0       150.0    3433          12.0          70\n4          8         302.0       140.0    3449          10.5          70",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>cylinders</th>\n      <th>displacement</th>\n      <th>horsepower</th>\n      <th>weight</th>\n      <th>acceleration</th>\n      <th>model-year</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>8</td>\n      <td>307.0</td>\n      <td>130.0</td>\n      <td>3504</td>\n      <td>12.0</td>\n      <td>70</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>8</td>\n      <td>350.0</td>\n      <td>165.0</td>\n      <td>3693</td>\n      <td>11.5</td>\n      <td>70</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>8</td>\n      <td>318.0</td>\n      <td>150.0</td>\n      <td>3436</td>\n      <td>11.0</td>\n      <td>70</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>8</td>\n      <td>304.0</td>\n      <td>150.0</td>\n      <td>3433</td>\n      <td>12.0</td>\n      <td>70</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>8</td>\n      <td>302.0</td>\n      <td>140.0</td>\n      <td>3449</td>\n      <td>10.5</td>\n      <td>70</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 10
    }
   ],
   "source": [
    "X = df.drop('mpg', axis=1)\n",
    "X.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Broj instanci: 396\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "N = y.shape[0]\n",
    "print(f'Broj instanci: {N}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Broj atributa: 6\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "m = X.shape[1]\n",
    "print(f'Broj atributa: {m}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "y = np.array(y)\n",
    "y = y.reshape(-1, 1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Podaci u numpy formatu:\n[[1.000e+00 1.800e+01 8.000e+00 ... 1.300e+02 3.504e+03 1.200e+01]\n [1.000e+00 1.500e+01 8.000e+00 ... 1.650e+02 3.693e+03 1.150e+01]\n [1.000e+00 1.800e+01 8.000e+00 ... 1.500e+02 3.436e+03 1.100e+01]\n ...\n [1.000e+00 3.200e+01 4.000e+00 ... 8.400e+01 2.295e+03 1.160e+01]\n [1.000e+00 2.800e+01 4.000e+00 ... 7.900e+01 2.625e+03 1.860e+01]\n [1.000e+00 3.100e+01 4.000e+00 ... 8.200e+01 2.720e+03 1.940e+01]]\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "X = np.ones((N, m + 1))\n",
    "\n",
    "for i in range(m):\n",
    "    attr_i = df.iloc[:, i]\n",
    "    X[:, i + 1] = attr_i\n",
    "\n",
    "print('Podaci u numpy formatu:')\n",
    "print(X)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Inicijalizacija w:\n[[0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]]\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "w_init = np.zeros((m + 1, 1))\n",
    "print('Inicijalizacija w:')\n",
    "print(w_init)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Iteration 1: Loss = 307.1398484848485\nIteration 2: Loss = 274.8537439606008\nIteration 3: Loss = 247.35694147335607\nIteration 4: Loss = 223.93756668275984\nIteration 5: Loss = 203.98955895016346\nIteration 6: Loss = 186.9969430836322\nIteration 7: Loss = 172.52043894658857\nIteration 8: Loss = 160.18606142769266\nIteration 9: Loss = 149.6754149226601\nIteration 10: Loss = 140.71743045406032\nIteration 11: Loss = 133.08133099393845\nIteration 12: Loss = 126.57064242795286\nIteration 13: Loss = 121.01809473582706\nIteration 14: Loss = 116.28128106546454\nIteration 15: Loss = 112.23896204665272\nIteration 16: Loss = 108.78791943529293\nIteration 17: Loss = 105.84027743511623\nIteration 18: Loss = 103.32122218083214\nIteration 19: Loss = 101.1670601995952\nIteration 20: Loss = 99.32356546471148\nIteration 21: Loss = 97.74457214494389\nIteration 22: Loss = 96.39077652897562\nIteration 23: Loss = 95.22871703302613\nIteration 24: Loss = 94.22990582116019\nIteration 25: Loss = 93.37008950242782\nIteration 26: Loss = 92.6286197187243\nIteration 27: Loss = 91.98791728910352\nIteration 28: Loss = 91.43301600421958\nIteration 29: Loss = 90.95117423162093\nIteration 30: Loss = 90.53154425242431\nIteration 31: Loss = 90.16489074811783\nIteration 32: Loss = 89.84335113176864\nIteration 33: Loss = 89.560231503841\nIteration 34: Loss = 89.30983293734634\nIteration 35: Loss = 89.08730358414371\nIteration 36: Loss = 88.88851276430795\nIteration 37: Loss = 88.70994377098117\nIteration 38: Loss = 88.54860260881847\nIteration 39: Loss = 88.40194029764312\nIteration 40: Loss = 88.26778672496432\nIteration 41: Loss = 88.14429433072344\nIteration 42: Loss = 88.02989016279585\nIteration 43: Loss = 87.92323505901147\nIteration 44: Loss = 87.82318889640057\nIteration 45: Loss = 87.72878100582665\nIteration 46: Loss = 87.63918498421812\nIteration 47: Loss = 87.55369725073498\nIteration 48: Loss = 87.4717187903685\nIteration 49: Loss = 87.3927396111907\nIteration 50: Loss = 87.31632551189365\nIteration 51: Loss = 87.24210681621514\nIteration 52: Loss = 87.16976878189057\nIteration 53: Loss = 87.09904343522796\nIteration 54: Loss = 87.02970261939961\nIteration 55: Loss = 86.96155207604319\nIteration 56: Loss = 86.89442640657897\nIteration 57: Loss = 86.82818478248225\nIteration 58: Loss = 86.7627072931852\nIteration 59: Loss = 86.69789183683\nIteration 60: Loss = 86.63365147318375\nIteration 61: Loss = 86.56991217001861\nIteration 62: Loss = 86.50661088447222\nIteration 63: Loss = 86.44369392959689\nIteration 64: Loss = 86.38111558370599\nIteration 65: Loss = 86.3188369064291\nIteration 66: Loss = 86.25682473074936\nIteration 67: Loss = 86.19505080486572\nIteration 68: Loss = 86.1334910616095\nIteration 69: Loss = 86.0721249964555\nIteration 70: Loss = 86.01093513798627\nIteration 71: Loss = 85.9499065970671\nIteration 72: Loss = 85.88902668303182\nIteration 73: Loss = 85.82828457691949\nIteration 74: Loss = 85.76767105328109\nIteration 75: Loss = 85.70717824333752\nIteration 76: Loss = 85.64679943334156\nIteration 77: Loss = 85.58652889291196\nIteration 78: Loss = 85.52636172888371\nIteration 79: Loss = 85.46629376088237\nIteration 80: Loss = 85.40632141539295\nIteration 81: Loss = 85.34644163557468\nIteration 82: Loss = 85.28665180448081\nIteration 83: Loss = 85.22694967969119\nIteration 84: Loss = 85.16733333766086\nIteration 85: Loss = 85.10780112634119\nIteration 86: Loss = 85.04835162484285\nIteration 87: Loss = 84.9889836090949\nIteration 88: Loss = 84.92969602260818\nIteration 89: Loss = 84.87048795158442\nIteration 90: Loss = 84.81135860372527\nIteration 91: Loss = 84.75230729019083\nIteration 92: Loss = 84.69333341024034\nIteration 93: Loss = 84.63443643815533\nIteration 94: Loss = 84.57561591210705\nIteration 95: Loss = 84.51687142467811\nIteration 96: Loss = 84.45820261479336\nIteration 97: Loss = 84.39960916085002\nIteration 98: Loss = 84.34109077486873\nIteration 99: Loss = 84.2826471975144\nIteration 100: Loss = 84.22427819385672\nIteration 101: Loss = 84.1659835497611\nIteration 102: Loss = 84.10776306881581\nIteration 103: Loss = 84.04961656971602\nIteration 104: Loss = 83.99154388403664\nIteration 105: Loss = 83.93354485433609\nIteration 106: Loss = 83.87561933254194\nIteration 107: Loss = 83.81776717857656\nIteration 108: Loss = 83.75998825918693\nIteration 109: Loss = 83.70228244694839\nIteration 110: Loss = 83.64464961941657\nIteration 111: Loss = 83.58708965840513\nIteration 112: Loss = 83.529602449371\nIteration 113: Loss = 83.47218788089096\nIteration 114: Loss = 83.41484584421593\nIteration 115: Loss = 83.3575762328916\nIteration 116: Loss = 83.30037894243517\nIteration 117: Loss = 83.24325387006056\nIteration 118: Loss = 83.18620091444397\nIteration 119: Loss = 83.1292199755246\nIteration 120: Loss = 83.07231095433475\nIteration 121: Loss = 83.01547375285527\nIteration 122: Loss = 82.95870827389254\nIteration 123: Loss = 82.90201442097353\nIteration 124: Loss = 82.84539209825643\nIteration 125: Loss = 82.78884121045489\nIteration 126: Loss = 82.73236166277303\nIteration 127: Loss = 82.67595336085036\nIteration 128: Loss = 82.6196162107148\nIteration 129: Loss = 82.56335011874272\nIteration 130: Loss = 82.50715499162473\nIteration 131: Loss = 82.4510307363368\nIteration 132: Loss = 82.39497726011533\nIteration 133: Loss = 82.33899447043616\nIteration 134: Loss = 82.28308227499654\nIteration 135: Loss = 82.22724058169986\nIteration 136: Loss = 82.17146929864248\nIteration 137: Loss = 82.1157683341027\nIteration 138: Loss = 82.06013759653104\nIteration 139: Loss = 82.00457699454235\nIteration 140: Loss = 81.9490864369087\nIteration 141: Loss = 81.8936658325535\nIteration 142: Loss = 81.83831509054644\nIteration 143: Loss = 81.78303412009912\nIteration 144: Loss = 81.72782283056137\nIteration 145: Loss = 81.67268113141797\nIteration 146: Loss = 81.61760893228603\nIteration 147: Loss = 81.56260614291259\nIteration 148: Loss = 81.50767267317252\nIteration 149: Loss = 81.4528084330669\nIteration 150: Loss = 81.39801333272143\nIteration 151: Loss = 81.34328728238515\nIteration 152: Loss = 81.28863019242927\nIteration 153: Loss = 81.23404197334621\nIteration 154: Loss = 81.17952253574876\nIteration 155: Loss = 81.12507179036925\nIteration 156: Loss = 81.07068964805894\nIteration 157: Loss = 81.01637601978744\nIteration 158: Loss = 80.96213081664212\nIteration 159: Loss = 80.90795394982769\nIteration 160: Loss = 80.85384533066575\nIteration 161: Loss = 80.79980487059447\nIteration 162: Loss = 80.74583248116818\nIteration 163: Loss = 80.69192807405703\nIteration 164: Loss = 80.63809156104679\nIteration 165: Loss = 80.58432285403852\nIteration 166: Loss = 80.53062186504832\nIteration 167: Loss = 80.47698850620706\nIteration 168: Loss = 80.42342268976029\nIteration 169: Loss = 80.36992432806784\nIteration 170: Loss = 80.31649333360373\nIteration 171: Loss = 80.26312961895603\nIteration 172: Loss = 80.20983309682653\nIteration 173: Loss = 80.15660368003063\nIteration 174: Loss = 80.10344128149718\nIteration 175: Loss = 80.05034581426831\nIteration 176: Loss = 79.99731719149922\nIteration 177: Loss = 79.94435532645805\nIteration 178: Loss = 79.89146013252568\nIteration 179: Loss = 79.83863152319563\nIteration 180: Loss = 79.7858694120739\nIteration 181: Loss = 79.73317371287871\nIteration 182: Loss = 79.68054433944047\nIteration 183: Loss = 79.62798120570159\nIteration 184: Loss = 79.57548422571628\nIteration 185: Loss = 79.52305331365048\nIteration 186: Loss = 79.47068838378165\nIteration 187: Loss = 79.41838935049863\nIteration 188: Loss = 79.36615612830154\nIteration 189: Loss = 79.31398863180154\nIteration 190: Loss = 79.26188677572082\nIteration 191: Loss = 79.2098504748923\nIteration 192: Loss = 79.15787964425958\nIteration 193: Loss = 79.10597419887681\nIteration 194: Loss = 79.05413405390846\nIteration 195: Loss = 79.00235912462922\nIteration 196: Loss = 78.95064932642394\nIteration 197: Loss = 78.89900457478734\nIteration 198: Loss = 78.8474247853239\nIteration 199: Loss = 78.79590987374782\nIteration 200: Loss = 78.74445975588279\nIteration 201: Loss = 78.69307434766185\nIteration 202: Loss = 78.64175356512726\nIteration 203: Loss = 78.59049732443037\nIteration 204: Loss = 78.53930554183148\nIteration 205: Loss = 78.48817813369969\nIteration 206: Loss = 78.43711501651269\nIteration 207: Loss = 78.38611610685679\nIteration 208: Loss = 78.33518132142655\nIteration 209: Loss = 78.28431057702485\nIteration 210: Loss = 78.23350379056265\nIteration 211: Loss = 78.18276087905883\nIteration 212: Loss = 78.1320817596401\nIteration 213: Loss = 78.08146634954083\nIteration 214: Loss = 78.03091456610294\nIteration 215: Loss = 77.98042632677573\nIteration 216: Loss = 77.9300015491157\nIteration 217: Loss = 77.87964015078656\nIteration 218: Loss = 77.8293420495589\nIteration 219: Loss = 77.77910716331021\nIteration 220: Loss = 77.72893541002463\nIteration 221: Loss = 77.67882670779288\nIteration 222: Loss = 77.62878097481207\nIteration 223: Loss = 77.57879812938563\nIteration 224: Loss = 77.52887808992308\nIteration 225: Loss = 77.47902077494001\nIteration 226: Loss = 77.42922610305779\nIteration 227: Loss = 77.37949399300358\nIteration 228: Loss = 77.32982436361013\nIteration 229: Loss = 77.2802171338156\nIteration 230: Loss = 77.23067222266349\nIteration 231: Loss = 77.1811895493025\nIteration 232: Loss = 77.13176903298633\nIteration 233: Loss = 77.08241059307363\nIteration 234: Loss = 77.03311414902777\nIteration 235: Loss = 76.9838796204168\nIteration 236: Loss = 76.93470692691324\nIteration 237: Loss = 76.88559598829399\nIteration 238: Loss = 76.83654672444014\nIteration 239: Loss = 76.78755905533691\nIteration 240: Loss = 76.73863290107349\nIteration 241: Loss = 76.68976818184282\nIteration 242: Loss = 76.64096481794158\nIteration 243: Loss = 76.59222272977\nIteration 244: Loss = 76.54354183783168\nIteration 245: Loss = 76.49492206273356\nIteration 246: Loss = 76.44636332518571\nIteration 247: Loss = 76.3978655460012\nIteration 248: Loss = 76.349428646096\nIteration 249: Loss = 76.30105254648876\nIteration 250: Loss = 76.25273716830084\nIteration 251: Loss = 76.20448243275602\nIteration 252: Loss = 76.15628826118045\nIteration 253: Loss = 76.10815457500247\nIteration 254: Loss = 76.06008129575252\nIteration 255: Loss = 76.01206834506299\nIteration 256: Loss = 75.96411564466806\nIteration 257: Loss = 75.9162231164036\nIteration 258: Loss = 75.86839068220705\nIteration 259: Loss = 75.82061826411727\nIteration 260: Loss = 75.77290578427437\nIteration 261: Loss = 75.72525316491969\nIteration 262: Loss = 75.67766032839548\nIteration 263: Loss = 75.63012719714499\nIteration 264: Loss = 75.58265369371216\nIteration 265: Loss = 75.53523974074162\nIteration 266: Loss = 75.48788526097844\nIteration 267: Loss = 75.44059017726812\nIteration 268: Loss = 75.39335441255633\nIteration 269: Loss = 75.34617788988893\nIteration 270: Loss = 75.29906053241167\nIteration 271: Loss = 75.25200226337023\nIteration 272: Loss = 75.20500300610996\nIteration 273: Loss = 75.1580626840758\nIteration 274: Loss = 75.11118122081218\nIteration 275: Loss = 75.06435853996284\nIteration 276: Loss = 75.01759456527073\nIteration 277: Loss = 74.97088922057783\nIteration 278: Loss = 74.92424242982514\nIteration 279: Loss = 74.87765411705244\nIteration 280: Loss = 74.83112420639816\nIteration 281: Loss = 74.78465262209936\nIteration 282: Loss = 74.73823928849149\nIteration 283: Loss = 74.69188413000828\nIteration 284: Loss = 74.64558707118168\nIteration 285: Loss = 74.59934803664167\nIteration 286: Loss = 74.55316695111615\nIteration 287: Loss = 74.50704373943081\nIteration 288: Loss = 74.46097832650904\nIteration 289: Loss = 74.41497063737168\nIteration 290: Loss = 74.36902059713708\nIteration 291: Loss = 74.32312813102085\nIteration 292: Loss = 74.2772931643357\nIteration 293: Loss = 74.23151562249147\nIteration 294: Loss = 74.18579543099482\nIteration 295: Loss = 74.14013251544925\nIteration 296: Loss = 74.09452680155488\nIteration 297: Loss = 74.04897821510838\nIteration 298: Loss = 74.0034866820028\nIteration 299: Loss = 73.95805212822752\nIteration 300: Loss = 73.912674479868\nIteration 301: Loss = 73.86735366310582\nIteration 302: Loss = 73.82208960421838\nIteration 303: Loss = 73.7768822295789\nIteration 304: Loss = 73.73173146565625\nIteration 305: Loss = 73.68663723901484\nIteration 306: Loss = 73.64159947631445\nIteration 307: Loss = 73.59661810431018\nIteration 308: Loss = 73.55169304985232\nIteration 309: Loss = 73.5068242398861\nIteration 310: Loss = 73.46201160145176\nIteration 311: Loss = 73.41725506168423\nIteration 312: Loss = 73.37255454781324\nIteration 313: Loss = 73.3279099871629\nIteration 314: Loss = 73.28332130715187\nIteration 315: Loss = 73.23878843529307\nIteration 316: Loss = 73.19431129919356\nIteration 317: Loss = 73.1498898265545\nIteration 318: Loss = 73.10552394517092\nIteration 319: Loss = 73.06121358293173\nIteration 320: Loss = 73.01695866781945\nIteration 321: Loss = 72.97275912791027\nIteration 322: Loss = 72.92861489137368\nIteration 323: Loss = 72.88452588647264\nIteration 324: Loss = 72.8404920415632\nIteration 325: Loss = 72.79651328509453\nIteration 326: Loss = 72.75258954560877\nIteration 327: Loss = 72.70872075174086\nIteration 328: Loss = 72.6649068322185\nIteration 329: Loss = 72.62114771586195\nIteration 330: Loss = 72.57744333158399\nIteration 331: Loss = 72.53379360838969\nIteration 332: Loss = 72.49019847537645\nIteration 333: Loss = 72.44665786173364\nIteration 334: Loss = 72.40317169674277\nIteration 335: Loss = 72.35973990977719\nIteration 336: Loss = 72.31636243030196\nIteration 337: Loss = 72.27303918787382\nIteration 338: Loss = 72.22977011214098\nIteration 339: Loss = 72.18655513284313\nIteration 340: Loss = 72.14339417981118\nIteration 341: Loss = 72.1002871829672\nIteration 342: Loss = 72.05723407232433\nIteration 343: Loss = 72.01423477798667\nIteration 344: Loss = 71.97128923014907\nIteration 345: Loss = 71.92839735909706\nIteration 346: Loss = 71.88555909520679\nIteration 347: Loss = 71.84277436894482\nIteration 348: Loss = 71.8000431108681\nIteration 349: Loss = 71.75736525162377\nIteration 350: Loss = 71.71474072194904\nIteration 351: Loss = 71.67216945267117\nIteration 352: Loss = 71.62965137470724\nIteration 353: Loss = 71.5871864190641\nIteration 354: Loss = 71.5447745168382\nIteration 355: Loss = 71.50241559921561\nIteration 356: Loss = 71.46010959747166\nIteration 357: Loss = 71.41785644297106\nIteration 358: Loss = 71.3756560671677\nIteration 359: Loss = 71.33350840160443\nIteration 360: Loss = 71.29141337791317\nIteration 361: Loss = 71.24937092781451\nIteration 362: Loss = 71.20738098311791\nIteration 363: Loss = 71.16544347572129\nIteration 364: Loss = 71.12355833761112\nIteration 365: Loss = 71.08172550086219\nIteration 366: Loss = 71.03994489763757\nIteration 367: Loss = 70.99821646018846\nIteration 368: Loss = 70.95654012085406\nIteration 369: Loss = 70.91491581206147\nIteration 370: Loss = 70.8733434663256\nIteration 371: Loss = 70.83182301624902\nIteration 372: Loss = 70.79035439452187\nIteration 373: Loss = 70.74893753392173\nIteration 374: Loss = 70.70757236731353\nIteration 375: Loss = 70.66625882764941\nIteration 376: Loss = 70.62499684796862\nIteration 377: Loss = 70.58378636139739\nIteration 378: Loss = 70.54262730114884\nIteration 379: Loss = 70.50151960052285\nIteration 380: Loss = 70.46046319290602\nIteration 381: Loss = 70.41945801177135\nIteration 382: Loss = 70.37850399067845\nIteration 383: Loss = 70.33760106327308\nIteration 384: Loss = 70.29674916328734\nIteration 385: Loss = 70.25594822453932\nIteration 386: Loss = 70.21519818093317\nIteration 387: Loss = 70.17449896645888\nIteration 388: Loss = 70.13385051519218\nIteration 389: Loss = 70.09325276129447\nIteration 390: Loss = 70.05270563901267\nIteration 391: Loss = 70.01220908267918\nIteration 392: Loss = 69.97176302671161\nIteration 393: Loss = 69.93136740561286\nIteration 394: Loss = 69.89102215397091\nIteration 395: Loss = 69.85072720645869\nIteration 396: Loss = 69.810482497834\nIteration 397: Loss = 69.77028796293945\nIteration 398: Loss = 69.73014353670226\nIteration 399: Loss = 69.6900491541342\nIteration 400: Loss = 69.65000475033148\nIteration 401: Loss = 69.6100102604746\nIteration 402: Loss = 69.57006561982831\nIteration 403: Loss = 69.53017076374145\nIteration 404: Loss = 69.49032562764687\nIteration 405: Loss = 69.45053014706123\nIteration 406: Loss = 69.41078425758508\nIteration 407: Loss = 69.37108789490254\nIteration 408: Loss = 69.33144099478133\nIteration 409: Loss = 69.29184349307263\nIteration 410: Loss = 69.25229532571092\nIteration 411: Loss = 69.21279642871397\nIteration 412: Loss = 69.1733467381826\nIteration 413: Loss = 69.13394619030069\nIteration 414: Loss = 69.09459472133506\nIteration 415: Loss = 69.05529226763522\nIteration 416: Loss = 69.01603876563355\nIteration 417: Loss = 68.97683415184486\nIteration 418: Loss = 68.93767836286645\nIteration 419: Loss = 68.89857133537808\nIteration 420: Loss = 68.85951300614173\nIteration 421: Loss = 68.82050331200152\nIteration 422: Loss = 68.78154218988362\nIteration 423: Loss = 68.74262957679618\nIteration 424: Loss = 68.70376540982912\nIteration 425: Loss = 68.6649496261542\nIteration 426: Loss = 68.6261821630247\nIteration 427: Loss = 68.58746295777546\nIteration 428: Loss = 68.54879194782274\nIteration 429: Loss = 68.51016907066408\nIteration 430: Loss = 68.47159426387826\nIteration 431: Loss = 68.43306746512512\nIteration 432: Loss = 68.39458861214553\nIteration 433: Loss = 68.35615764276119\nIteration 434: Loss = 68.31777449487463\nIteration 435: Loss = 68.27943910646901\nIteration 436: Loss = 68.24115141560813\nIteration 437: Loss = 68.20291136043616\nIteration 438: Loss = 68.16471887917773\nIteration 439: Loss = 68.12657391013764\nIteration 440: Loss = 68.08847639170091\nIteration 441: Loss = 68.05042626233259\nIteration 442: Loss = 68.01242346057762\nIteration 443: Loss = 67.97446792506086\nIteration 444: Loss = 67.93655959448688\nIteration 445: Loss = 67.89869840763983\nIteration 446: Loss = 67.86088430338347\nIteration 447: Loss = 67.82311722066093\nIteration 448: Loss = 67.7853970984947\nIteration 449: Loss = 67.74772387598647\nIteration 450: Loss = 67.71009749231703\nIteration 451: Loss = 67.67251788674626\nIteration 452: Loss = 67.63498499861282\nIteration 453: Loss = 67.5974987673343\nIteration 454: Loss = 67.56005913240698\nIteration 455: Loss = 67.52266603340561\nIteration 456: Loss = 67.48531940998369\nIteration 457: Loss = 67.44801920187288\nIteration 458: Loss = 67.41076534888326\nIteration 459: Loss = 67.37355779090309\nIteration 460: Loss = 67.33639646789872\nIteration 461: Loss = 67.29928131991448\nIteration 462: Loss = 67.26221228707263\nIteration 463: Loss = 67.22518930957317\nIteration 464: Loss = 67.18821232769382\nIteration 465: Loss = 67.1512812817899\nIteration 466: Loss = 67.11439611229419\nIteration 467: Loss = 67.07755675971688\nIteration 468: Loss = 67.04076316464543\nIteration 469: Loss = 67.00401526774448\nIteration 470: Loss = 66.96731300975578\nIteration 471: Loss = 66.93065633149808\nIteration 472: Loss = 66.89404517386697\nIteration 473: Loss = 66.85747947783483\nIteration 474: Loss = 66.82095918445079\nIteration 475: Loss = 66.78448423484049\nIteration 476: Loss = 66.7480545702061\nIteration 477: Loss = 66.71167013182615\nIteration 478: Loss = 66.67533086105549\nIteration 479: Loss = 66.63903669932516\nIteration 480: Loss = 66.60278758814225\nIteration 481: Loss = 66.56658346908986\nIteration 482: Loss = 66.53042428382703\nIteration 483: Loss = 66.49430997408854\nIteration 484: Loss = 66.45824048168485\nIteration 485: Loss = 66.42221574850205\nIteration 486: Loss = 66.38623571650177\nIteration 487: Loss = 66.35030032772097\nIteration 488: Loss = 66.31440952427194\nIteration 489: Loss = 66.27856324834218\nIteration 490: Loss = 66.24276144219431\nIteration 491: Loss = 66.20700404816589\nIteration 492: Loss = 66.17129100866953\nIteration 493: Loss = 66.1356222661925\nIteration 494: Loss = 66.0999977632969\nIteration 495: Loss = 66.06441744261939\nIteration 496: Loss = 66.02888124687117\nIteration 497: Loss = 65.99338911883791\nIteration 498: Loss = 65.95794100137952\nIteration 499: Loss = 65.92253683743026\nIteration 500: Loss = 65.8871765699984\nIteration 501: Loss = 65.85186014216636\nIteration 502: Loss = 65.81658749709042\nIteration 503: Loss = 65.78135857800078\nIteration 504: Loss = 65.74617332820135\nIteration 505: Loss = 65.71103169106972\nIteration 506: Loss = 65.675933610057\nIteration 507: Loss = 65.64087902868783\nIteration 508: Loss = 65.60586789056018\nIteration 509: Loss = 65.57090013934528\nIteration 510: Loss = 65.53597571878757\nIteration 511: Loss = 65.50109457270457\nIteration 512: Loss = 65.46625664498679\nIteration 513: Loss = 65.43146187959762\nIteration 514: Loss = 65.39671022057327\nIteration 515: Loss = 65.36200161202264\nIteration 516: Loss = 65.32733599812723\nIteration 517: Loss = 65.29271332314111\nIteration 518: Loss = 65.2581335313907\nIteration 519: Loss = 65.2235965672748\nIteration 520: Loss = 65.18910237526444\nIteration 521: Loss = 65.15465089990275\nIteration 522: Loss = 65.12024208580497\nIteration 523: Loss = 65.08587587765824\nIteration 524: Loss = 65.05155222022158\nIteration 525: Loss = 65.01727105832579\nIteration 526: Loss = 64.98303233687331\nIteration 527: Loss = 64.94883600083818\nIteration 528: Loss = 64.91468199526595\nIteration 529: Loss = 64.88057026527352\nIteration 530: Loss = 64.84650075604911\nIteration 531: Loss = 64.81247341285216\nIteration 532: Loss = 64.77848818101322\nIteration 533: Loss = 64.74454500593384\nIteration 534: Loss = 64.71064383308654\nIteration 535: Loss = 64.67678460801464\nIteration 536: Loss = 64.6429672763323\nIteration 537: Loss = 64.60919178372417\nIteration 538: Loss = 64.5754580759456\nIteration 539: Loss = 64.54176609882238\nIteration 540: Loss = 64.50811579825064\nIteration 541: Loss = 64.47450712019686\nIteration 542: Loss = 64.44094001069767\nIteration 543: Loss = 64.40741441585982\nIteration 544: Loss = 64.37393028186008\nIteration 545: Loss = 64.34048755494517\nIteration 546: Loss = 64.30708618143157\nIteration 547: Loss = 64.27372610770557\nIteration 548: Loss = 64.24040728022311\nIteration 549: Loss = 64.20712964550964\nIteration 550: Loss = 64.17389315016013\nIteration 551: Loss = 64.14069774083892\nIteration 552: Loss = 64.10754336427964\nIteration 553: Loss = 64.07442996728507\nIteration 554: Loss = 64.04135749672722\nIteration 555: Loss = 64.00832589954699\nIteration 556: Loss = 63.97533512275429\nIteration 557: Loss = 63.94238511342784\nIteration 558: Loss = 63.90947581871515\nIteration 559: Loss = 63.87660718583235\nIteration 560: Loss = 63.84377916206416\nIteration 561: Loss = 63.81099169476379\nIteration 562: Loss = 63.77824473135283\nIteration 563: Loss = 63.745538219321205\nIteration 564: Loss = 63.712872106227046\nIteration 565: Loss = 63.68024633969656\nIteration 566: Loss = 63.64766086742412\nIteration 567: Loss = 63.61511563717192\nIteration 568: Loss = 63.582610596770074\nIteration 569: Loss = 63.550145694116495\nIteration 570: Loss = 63.51772087717676\nIteration 571: Loss = 63.48533609398404\nIteration 572: Loss = 63.45299129263902\nIteration 573: Loss = 63.420686421309824\nIteration 574: Loss = 63.38842142823188\nIteration 575: Loss = 63.356196261707915\nIteration 576: Loss = 63.32401087010779\nIteration 577: Loss = 63.29186520186844\nIteration 578: Loss = 63.25975920549379\nIteration 579: Loss = 63.22769282955465\nIteration 580: Loss = 63.19566602268867\nIteration 581: Loss = 63.16367873360024\nIteration 582: Loss = 63.13173091106036\nIteration 583: Loss = 63.09982250390655\nIteration 584: Loss = 63.067953461042876\nIteration 585: Loss = 63.036123731439744\nIteration 586: Loss = 63.00433326413384\nIteration 587: Loss = 62.9725820082281\nIteration 588: Loss = 62.940869912891515\nIteration 589: Loss = 62.90919692735921\nIteration 590: Loss = 62.87756300093217\nIteration 591: Loss = 62.845968082977286\nIteration 592: Loss = 62.81441212292723\nIteration 593: Loss = 62.78289507028036\nIteration 594: Loss = 62.75141687460064\nIteration 595: Loss = 62.71997748551756\nIteration 596: Loss = 62.68857685272605\nIteration 597: Loss = 62.657214925986395\nIteration 598: Loss = 62.62589165512415\nIteration 599: Loss = 62.59460699003004\nIteration 600: Loss = 62.56336088065989\nIteration 601: Loss = 62.53215327703455\nIteration 602: Loss = 62.500984129239825\nIteration 603: Loss = 62.469853387426305\nIteration 604: Loss = 62.43876100180939\nIteration 605: Loss = 62.407706922669114\nIteration 606: Loss = 62.376691100350165\nIteration 607: Loss = 62.34571348526168\nIteration 608: Loss = 62.31477402787724\nIteration 609: Loss = 62.2838726787348\nIteration 610: Loss = 62.25300938843653\nIteration 611: Loss = 62.222184107648786\nIteration 612: Loss = 62.19139678710202\nIteration 613: Loss = 62.16064737759071\nIteration 614: Loss = 62.129935829973206\nIteration 615: Loss = 62.099262095171774\nIteration 616: Loss = 62.06862612417235\nIteration 617: Loss = 62.03802786802462\nIteration 618: Loss = 62.00746727784185\nIteration 619: Loss = 61.97694430480077\nIteration 620: Loss = 61.94645890014159\nIteration 621: Loss = 61.916011015167825\nIteration 622: Loss = 61.8856006012463\nIteration 623: Loss = 61.855227609806995\nIteration 624: Loss = 61.82489199234296\nIteration 625: Loss = 61.794593700410324\nIteration 626: Loss = 61.76433268562807\nIteration 627: Loss = 61.734108899678105\nIteration 628: Loss = 61.703922294305116\nIteration 629: Loss = 61.67377282131641\nIteration 630: Loss = 61.643660432581925\nIteration 631: Loss = 61.61358508003419\nIteration 632: Loss = 61.58354671566809\nIteration 633: Loss = 61.55354529154094\nIteration 634: Loss = 61.523580759772294\nIteration 635: Loss = 61.49365307254395\nIteration 636: Loss = 61.463762182099785\nIteration 637: Loss = 61.433908040745735\nIteration 638: Loss = 61.40409060084972\nIteration 639: Loss = 61.37430981484154\nIteration 640: Loss = 61.34456563521272\nIteration 641: Loss = 61.314858014516616\nIteration 642: Loss = 61.28518690536812\nIteration 643: Loss = 61.25555226044376\nIteration 644: Loss = 61.2259540324815\nIteration 645: Loss = 61.19639217428074\nIteration 646: Loss = 61.16686663870214\nIteration 647: Loss = 61.13737737866767\nIteration 648: Loss = 61.107924347160385\nIteration 649: Loss = 61.07850749722451\nIteration 650: Loss = 61.04912678196519\nIteration 651: Loss = 61.01978215454855\nIteration 652: Loss = 60.990473568201516\nIteration 653: Loss = 60.96120097621179\nIteration 654: Loss = 60.93196433192777\nIteration 655: Loss = 60.90276358875848\nIteration 656: Loss = 60.87359870017344\nIteration 657: Loss = 60.84446961970257\nIteration 658: Loss = 60.815376300936286\nIteration 659: Loss = 60.786318697525175\nIteration 660: Loss = 60.757296763180115\nIteration 661: Loss = 60.72831045167209\nIteration 662: Loss = 60.69935971683213\nIteration 663: Loss = 60.67044451255127\nIteration 664: Loss = 60.64156479278042\nIteration 665: Loss = 60.61272051153034\nIteration 666: Loss = 60.583911622871526\nIteration 667: Loss = 60.55513808093412\nIteration 668: Loss = 60.52639983990788\nIteration 669: Loss = 60.497696854042054\nIteration 670: Loss = 60.46902907764536\nIteration 671: Loss = 60.44039646508587\nIteration 672: Loss = 60.41179897079089\nIteration 673: Loss = 60.38323654924698\nIteration 674: Loss = 60.354709154999796\nIteration 675: Loss = 60.32621674265407\nIteration 676: Loss = 60.2977592668735\nIteration 677: Loss = 60.269336682380676\nIteration 678: Loss = 60.24094894395701\nIteration 679: Loss = 60.212596006442645\nIteration 680: Loss = 60.184277824736434\nIteration 681: Loss = 60.15599435379576\nIteration 682: Loss = 60.1277455486366\nIteration 683: Loss = 60.09953136433327\nIteration 684: Loss = 60.07135175601852\nIteration 685: Loss = 60.0432066788834\nIteration 686: Loss = 60.01509608817712\nIteration 687: Loss = 59.98701993920707\nIteration 688: Loss = 59.95897818733866\nIteration 689: Loss = 59.93097078799533\nIteration 690: Loss = 59.90299769665838\nIteration 691: Loss = 59.875058868866994\nIteration 692: Loss = 59.84715426021809\nIteration 693: Loss = 59.81928382636625\nIteration 694: Loss = 59.79144752302374\nIteration 695: Loss = 59.763645305960274\nIteration 696: Loss = 59.73587713100308\nIteration 697: Loss = 59.70814295403672\nIteration 698: Loss = 59.680442731003154\nIteration 699: Loss = 59.65277641790148\nIteration 700: Loss = 59.62514397078803\nIteration 701: Loss = 59.59754534577619\nIteration 702: Loss = 59.569980499036376\nIteration 703: Loss = 59.5424493867959\nIteration 704: Loss = 59.51495196533903\nIteration 705: Loss = 59.487488191006726\nIteration 706: Loss = 59.46005802019671\nIteration 707: Loss = 59.43266140936338\nIteration 708: Loss = 59.40529831501763\nIteration 709: Loss = 59.37796869372693\nIteration 710: Loss = 59.35067250211513\nIteration 711: Loss = 59.32340969686242\nIteration 712: Loss = 59.29618023470529\nIteration 713: Loss = 59.26898407243644\nIteration 714: Loss = 59.241821166904685\nIteration 715: Loss = 59.214691475014895\nIteration 716: Loss = 59.187594953727945\nIteration 717: Loss = 59.16053156006059\nIteration 718: Loss = 59.13350125108545\nIteration 719: Loss = 59.106503983930935\nIteration 720: Loss = 59.07953971578108\nIteration 721: Loss = 59.05260840387561\nIteration 722: Loss = 59.02571000550978\nIteration 723: Loss = 58.9988444780343\nIteration 724: Loss = 58.9720117788553\nIteration 725: Loss = 58.94521186543426\nIteration 726: Loss = 58.91844469528792\nIteration 727: Loss = 58.89171022598816\nIteration 728: Loss = 58.86500841516206\nIteration 729: Loss = 58.83833922049169\nIteration 730: Loss = 58.81170259971409\nIteration 731: Loss = 58.78509851062127\nIteration 732: Loss = 58.75852691105998\nIteration 733: Loss = 58.7319877589318\nIteration 734: Loss = 58.705481012192976\nIteration 735: Loss = 58.679006628854374\nIteration 736: Loss = 58.65256456698141\nIteration 737: Loss = 58.62615478469398\nIteration 738: Loss = 58.5997772401664\nIteration 739: Loss = 58.57343189162725\nIteration 740: Loss = 58.54711869735948\nIteration 741: Loss = 58.520837615700174\nIteration 742: Loss = 58.49458860504056\nIteration 743: Loss = 58.46837162382589\nIteration 744: Loss = 58.44218663055542\nIteration 745: Loss = 58.41603358378235\nIteration 746: Loss = 58.38991244211366\nIteration 747: Loss = 58.36382316421018\nIteration 748: Loss = 58.337765708786364\nIteration 749: Loss = 58.311740034610345\nIteration 750: Loss = 58.28574610050382\nIteration 751: Loss = 58.25978386534199\nIteration 752: Loss = 58.233853288053425\nIteration 753: Loss = 58.20795432762011\nIteration 754: Loss = 58.18208694307732\nIteration 755: Loss = 58.15625109351349\nIteration 756: Loss = 58.13044673807024\nIteration 757: Loss = 58.10467383594229\nIteration 758: Loss = 58.078932346377336\nIteration 759: Loss = 58.05322222867602\nIteration 760: Loss = 58.027543442191856\nIteration 761: Loss = 58.00189594633118\nIteration 762: Loss = 57.97627970055305\nIteration 763: Loss = 57.95069466436917\nIteration 764: Loss = 57.92514079734389\nIteration 765: Loss = 57.89961805909406\nIteration 766: Loss = 57.87412640928898\nIteration 767: Loss = 57.84866580765038\nIteration 768: Loss = 57.82323621395224\nIteration 769: Loss = 57.797837588020926\nIteration 770: Loss = 57.772469889734865\nIteration 771: Loss = 57.74713307902467\nIteration 772: Loss = 57.72182711587299\nIteration 773: Loss = 57.696551960314494\nIteration 774: Loss = 57.6713075724357\nIteration 775: Loss = 57.64609391237506\nIteration 776: Loss = 57.620910940322744\nIteration 777: Loss = 57.59575861652067\nIteration 778: Loss = 57.57063690126239\nIteration 779: Loss = 57.54554575489307\nIteration 780: Loss = 57.520485137809345\nIteration 781: Loss = 57.49545501045934\nIteration 782: Loss = 57.47045533334252\nIteration 783: Loss = 57.4454860670097\nIteration 784: Loss = 57.42054717206296\nIteration 785: Loss = 57.39563860915548\nIteration 786: Loss = 57.37076033899166\nIteration 787: Loss = 57.345912322326846\nIteration 788: Loss = 57.321094519967446\nIteration 789: Loss = 57.29630689277074\nIteration 790: Loss = 57.27154940164488\nIteration 791: Loss = 57.24682200754879\nIteration 792: Loss = 57.22212467149213\nIteration 793: Loss = 57.19745735453517\nIteration 794: Loss = 57.172820017788816\nIteration 795: Loss = 57.14821262241446\nIteration 796: Loss = 57.123635129623956\nIteration 797: Loss = 57.09908750067957\nIteration 798: Loss = 57.07456969689384\nIteration 799: Loss = 57.050081679629635\nIteration 800: Loss = 57.02562341029994\nIteration 801: Loss = 57.001194850367945\nIteration 802: Loss = 56.97679596134682\nIteration 803: Loss = 56.952426704799805\nIteration 804: Loss = 56.928087042340024\nIteration 805: Loss = 56.9037769356305\nIteration 806: Loss = 56.87949634638403\nIteration 807: Loss = 56.85524523636319\nIteration 808: Loss = 56.8310235673802\nIteration 809: Loss = 56.80683130129686\nIteration 810: Loss = 56.7826684000246\nIteration 811: Loss = 56.75853482552424\nIteration 812: Loss = 56.73443053980606\nIteration 813: Loss = 56.710355504929716\nIteration 814: Loss = 56.68630968300408\nIteration 815: Loss = 56.66229303618733\nIteration 816: Loss = 56.63830552668673\nIteration 817: Loss = 56.61434711675869\nIteration 818: Loss = 56.590417768708626\nIteration 819: Loss = 56.56651744489094\nIteration 820: Loss = 56.542646107708926\nIteration 821: Loss = 56.51880371961473\nIteration 822: Loss = 56.49499024310925\nIteration 823: Loss = 56.47120564074215\nIteration 824: Loss = 56.447449875111666\nIteration 825: Loss = 56.423722908864704\nIteration 826: Loss = 56.400024704696634\nIteration 827: Loss = 56.37635522535133\nIteration 828: Loss = 56.352714433621045\nIteration 829: Loss = 56.32910229234635\nIteration 830: Loss = 56.305518764416135\nIteration 831: Loss = 56.28196381276745\nIteration 832: Loss = 56.258437400385525\nIteration 833: Loss = 56.23493949030366\nIteration 834: Loss = 56.2114700456032\nIteration 835: Loss = 56.18802902941341\nIteration 836: Loss = 56.16461640491148\nIteration 837: Loss = 56.14123213532245\nIteration 838: Loss = 56.11787618391908\nIteration 839: Loss = 56.09454851402192\nIteration 840: Loss = 56.071249088999096\n",
      "Iteration 841: Loss = 56.04797787226635\nIteration 842: Loss = 56.02473482728697\nIteration 843: Loss = 56.00151991757166\nIteration 844: Loss = 55.9783331066786\nIteration 845: Loss = 55.9551743582132\nIteration 846: Loss = 55.93204363582827\nIteration 847: Loss = 55.90894090322376\nIteration 848: Loss = 55.885866124146794\nIteration 849: Loss = 55.86281926239162\nIteration 850: Loss = 55.83980028179945\nIteration 851: Loss = 55.816809146258535\nIteration 852: Loss = 55.79384581970403\nIteration 853: Loss = 55.770910266117916\nIteration 854: Loss = 55.74800244952898\nIteration 855: Loss = 55.72512233401272\nIteration 856: Loss = 55.702269883691336\nIteration 857: Loss = 55.679445062733606\nIteration 858: Loss = 55.65664783535483\nIteration 859: Loss = 55.63387816581689\nIteration 860: Loss = 55.61113601842799\nIteration 861: Loss = 55.58842135754275\nIteration 862: Loss = 55.56573414756212\nIteration 863: Loss = 55.54307435293321\nIteration 864: Loss = 55.5204419381494\nIteration 865: Loss = 55.49783686775019\nIteration 866: Loss = 55.47525910632108\nIteration 867: Loss = 55.45270861849363\nIteration 868: Loss = 55.43018536894534\nIteration 869: Loss = 55.40768932239958\nIteration 870: Loss = 55.38522044362555\nIteration 871: Loss = 55.362778697438216\nIteration 872: Loss = 55.340364048698305\nIteration 873: Loss = 55.31797646231209\nIteration 874: Loss = 55.29561590323151\nIteration 875: Loss = 55.27328233645401\nIteration 876: Loss = 55.2509757270225\nIteration 877: Loss = 55.22869604002533\nIteration 878: Loss = 55.20644324059618\nIteration 879: Loss = 55.18421729391398\nIteration 880: Loss = 55.162018165202994\nIteration 881: Loss = 55.139845819732564\nIteration 882: Loss = 55.11770022281722\nIteration 883: Loss = 55.0955813398165\nIteration 884: Loss = 55.07348913613499\nIteration 885: Loss = 55.05142357722217\nIteration 886: Loss = 55.02938462857243\nIteration 887: Loss = 55.00737225572499\nIteration 888: Loss = 54.98538642426381\nIteration 889: Loss = 54.963427099817594\nIteration 890: Loss = 54.94149424805966\nIteration 891: Loss = 54.91958783470795\nIteration 892: Loss = 54.89770782552491\nIteration 893: Loss = 54.87585418631749\nIteration 894: Loss = 54.85402688293704\nIteration 895: Loss = 54.83222588127928\nIteration 896: Loss = 54.810451147284226\nIteration 897: Loss = 54.78870264693618\nIteration 898: Loss = 54.766980346263544\nIteration 899: Loss = 54.74528421133893\nIteration 900: Loss = 54.72361420827899\nIteration 901: Loss = 54.70197030324441\nIteration 902: Loss = 54.680352462439835\nIteration 903: Loss = 54.658760652113784\nIteration 904: Loss = 54.63719483855863\nIteration 905: Loss = 54.61565498811058\nIteration 906: Loss = 54.59414106714949\nIteration 907: Loss = 54.57265304209897\nIteration 908: Loss = 54.5511908794262\nIteration 909: Loss = 54.52975454564193\nIteration 910: Loss = 54.508344007300416\nIteration 911: Loss = 54.48695923099938\nIteration 912: Loss = 54.46560018337991\nIteration 913: Loss = 54.44426683112642\nIteration 914: Loss = 54.42295914096665\nIteration 915: Loss = 54.40167707967152\nIteration 916: Loss = 54.38042061405511\nIteration 917: Loss = 54.35918971097467\nIteration 918: Loss = 54.33798433733042\nIteration 919: Loss = 54.31680446006565\nIteration 920: Loss = 54.295650046166564\nIteration 921: Loss = 54.27452106266223\nIteration 922: Loss = 54.253417476624605\nIteration 923: Loss = 54.23233925516833\nIteration 924: Loss = 54.21128636545085\nIteration 925: Loss = 54.19025877467227\nIteration 926: Loss = 54.169256450075245\nIteration 927: Loss = 54.148279358945025\nIteration 928: Loss = 54.12732746860935\nIteration 929: Loss = 54.10640074643841\nIteration 930: Loss = 54.08549915984476\nIteration 931: Loss = 54.0646226762833\nIteration 932: Loss = 54.04377126325122\nIteration 933: Loss = 54.02294488828794\nIteration 934: Loss = 54.00214351897497\nIteration 935: Loss = 53.981367122936064\nIteration 936: Loss = 53.96061566783691\nIteration 937: Loss = 53.93988912138529\nIteration 938: Loss = 53.91918745133088\nIteration 939: Loss = 53.89851062546528\nIteration 940: Loss = 53.877858611621875\nIteration 941: Loss = 53.85723137767594\nIteration 942: Loss = 53.83662889154438\nIteration 943: Loss = 53.81605112118582\nIteration 944: Loss = 53.79549803460053\nIteration 945: Loss = 53.77496959983029\nIteration 946: Loss = 53.75446578495846\nIteration 947: Loss = 53.733986558109805\nIteration 948: Loss = 53.71353188745055\nIteration 949: Loss = 53.69310174118821\nIteration 950: Loss = 53.672696087571666\nIteration 951: Loss = 53.65231489489101\nIteration 952: Loss = 53.631958131477504\nIteration 953: Loss = 53.6116257657036\nIteration 954: Loss = 53.591317765982815\nIteration 955: Loss = 53.57103410076968\nIteration 956: Loss = 53.55077473855972\nIteration 957: Loss = 53.53053964788937\nIteration 958: Loss = 53.510328797335994\nIteration 959: Loss = 53.49014215551768\nIteration 960: Loss = 53.46997969109336\nIteration 961: Loss = 53.44984137276266\nIteration 962: Loss = 53.42972716926586\nIteration 963: Loss = 53.409637049383825\nIteration 964: Loss = 53.38957098193803\nIteration 965: Loss = 53.36952893579036\nIteration 966: Loss = 53.349510879843265\nIteration 967: Loss = 53.32951678303952\nIteration 968: Loss = 53.309546614362255\nIteration 969: Loss = 53.28960034283489\nIteration 970: Loss = 53.26967793752111\nIteration 971: Loss = 53.24977936752474\nIteration 972: Loss = 53.22990460198976\nIteration 973: Loss = 53.210053610100296\nIteration 974: Loss = 53.19022636108039\nIteration 975: Loss = 53.17042282419416\nIteration 976: Loss = 53.15064296874559\nIteration 977: Loss = 53.13088676407858\nIteration 978: Loss = 53.111154179576815\nIteration 979: Loss = 53.09144518466379\nIteration 980: Loss = 53.0717597488027\nIteration 981: Loss = 53.0520978414964\nIteration 982: Loss = 53.032459432287354\nIteration 983: Loss = 53.012844490757644\nIteration 984: Loss = 52.99325298652881\nIteration 985: Loss = 52.97368488926188\nIteration 986: Loss = 52.95414016865728\nIteration 987: Loss = 52.93461879445481\nIteration 988: Loss = 52.915120736433565\nIteration 989: Loss = 52.895645964411905\nIteration 990: Loss = 52.87619444824741\nIteration 991: Loss = 52.85676615783678\nIteration 992: Loss = 52.83736106311587\nIteration 993: Loss = 52.81797913405951\nIteration 994: Loss = 52.79862034068163\nIteration 995: Loss = 52.77928465303506\nIteration 996: Loss = 52.75997204121154\nIteration 997: Loss = 52.74068247534164\nIteration 998: Loss = 52.72141592559476\nIteration 999: Loss = 52.702172362179056\nIteration 1000: Loss = 52.68295175534135\nIteration 1001: Loss = 52.663754075367144\nIteration 1002: Loss = 52.644579292580524\nIteration 1003: Loss = 52.62542737734413\nIteration 1004: Loss = 52.606298300059116\nIteration 1005: Loss = 52.58719203116507\nIteration 1006: Loss = 52.568108541139964\nIteration 1007: Loss = 52.549047800500134\nIteration 1008: Loss = 52.530009779800245\nIteration 1009: Loss = 52.51099444963317\nIteration 1010: Loss = 52.49200178063001\nIteration 1011: Loss = 52.47303174346\nIteration 1012: Loss = 52.454084308830446\nIteration 1013: Loss = 52.43515944748679\nIteration 1014: Loss = 52.416257130212415\nIteration 1015: Loss = 52.39737732782866\nIteration 1016: Loss = 52.37852001119476\nIteration 1017: Loss = 52.35968515120785\nIteration 1018: Loss = 52.34087271880282\nIteration 1019: Loss = 52.32208268495235\nIteration 1020: Loss = 52.3033150206668\nIteration 1021: Loss = 52.284569696994204\nIteration 1022: Loss = 52.26584668502022\nIteration 1023: Loss = 52.24714595586802\nIteration 1024: Loss = 52.228467480698335\nIteration 1025: Loss = 52.20981123070932\nIteration 1026: Loss = 52.1911771771366\nIteration 1027: Loss = 52.172565291253086\nIteration 1028: Loss = 52.153975544369054\nIteration 1029: Loss = 52.135407907832075\nIteration 1030: Loss = 52.11686235302686\nIteration 1031: Loss = 52.09833885137538\nIteration 1032: Loss = 52.07983737433666\nIteration 1033: Loss = 52.06135789340685\nIteration 1034: Loss = 52.04290038011908\nIteration 1035: Loss = 52.02446480604348\nIteration 1036: Loss = 52.00605114278715\nIteration 1037: Loss = 51.98765936199399\nIteration 1038: Loss = 51.96928943534481\nIteration 1039: Loss = 51.950941334557164\nIteration 1040: Loss = 51.93261503138537\nIteration 1041: Loss = 51.91431049762043\nIteration 1042: Loss = 51.89602770508994\nIteration 1043: Loss = 51.87776662565819\nIteration 1044: Loss = 51.85952723122594\nIteration 1045: Loss = 51.84130949373049\nIteration 1046: Loss = 51.82311338514556\nIteration 1047: Loss = 51.80493887748132\nIteration 1048: Loss = 51.786785942784256\nIteration 1049: Loss = 51.7686545531372\nIteration 1050: Loss = 51.75054468065921\nIteration 1051: Loss = 51.73245629750563\nIteration 1052: Loss = 51.714389375867874\nIteration 1053: Loss = 51.696343887973555\nIteration 1054: Loss = 51.67831980608636\nIteration 1055: Loss = 51.66031710250597\nIteration 1056: Loss = 51.64233574956807\nIteration 1057: Loss = 51.624375719644256\nIteration 1058: Loss = 51.606436985142075\nIteration 1059: Loss = 51.58851951850483\nIteration 1060: Loss = 51.57062329221171\nIteration 1061: Loss = 51.55274827877758\nIteration 1062: Loss = 51.53489445075303\nIteration 1063: Loss = 51.51706178072432\nIteration 1064: Loss = 51.49925024131334\nIteration 1065: Loss = 51.481459805177494\nIteration 1066: Loss = 51.463690445009725\nIteration 1067: Loss = 51.445942133538466\nIteration 1068: Loss = 51.42821484352755\nIteration 1069: Loss = 51.410508547776196\nIteration 1070: Loss = 51.39282321911897\nIteration 1071: Loss = 51.37515883042571\nIteration 1072: Loss = 51.3575153546015\nIteration 1073: Loss = 51.33989276458664\nIteration 1074: Loss = 51.32229103335654\nIteration 1075: Loss = 51.30471013392173\nIteration 1076: Loss = 51.28715003932784\nIteration 1077: Loss = 51.269610722655464\nIteration 1078: Loss = 51.252092157020165\nIteration 1079: Loss = 51.234594315572444\nIteration 1080: Loss = 51.21711717149771\nIteration 1081: Loss = 51.19966069801615\nIteration 1082: Loss = 51.18222486838275\nIteration 1083: Loss = 51.16480965588726\nIteration 1084: Loss = 51.147415033854124\nIteration 1085: Loss = 51.130040975642395\nIteration 1086: Loss = 51.1126874546458\nIteration 1087: Loss = 51.095354444292575\nIteration 1088: Loss = 51.078041918045486\nIteration 1089: Loss = 51.06074984940177\nIteration 1090: Loss = 51.04347821189312\nIteration 1091: Loss = 51.02622697908556\nIteration 1092: Loss = 51.00899612457948\nIteration 1093: Loss = 50.991785622009544\nIteration 1094: Loss = 50.97459544504471\nIteration 1095: Loss = 50.95742556738811\nIteration 1096: Loss = 50.94027596277699\nIteration 1097: Loss = 50.923146604982804\nIteration 1098: Loss = 50.90603746781099\nIteration 1099: Loss = 50.888948525101064\nIteration 1100: Loss = 50.87187975072652\nIteration 1101: Loss = 50.85483111859476\nIteration 1102: Loss = 50.837802602647116\nIteration 1103: Loss = 50.820794176858755\nIteration 1104: Loss = 50.803805815238626\nIteration 1105: Loss = 50.78683749182947\nIteration 1106: Loss = 50.76988918070773\nIteration 1107: Loss = 50.75296085598355\nIteration 1108: Loss = 50.736052491800685\nIteration 1109: Loss = 50.719164062336446\nIteration 1110: Loss = 50.70229554180176\nIteration 1111: Loss = 50.685446904440944\nIteration 1112: Loss = 50.668618124531925\nIteration 1113: Loss = 50.651809176385896\nIteration 1114: Loss = 50.63502003434749\nIteration 1115: Loss = 50.61825067279466\nIteration 1116: Loss = 50.60150106613863\nIteration 1117: Loss = 50.58477118882388\nIteration 1118: Loss = 50.56806101532804\nIteration 1119: Loss = 50.551370520161974\nIteration 1120: Loss = 50.534699677869575\nIteration 1121: Loss = 50.51804846302786\nIteration 1122: Loss = 50.50141685024684\nIteration 1123: Loss = 50.484804814169486\nIteration 1124: Loss = 50.468212329471775\nIteration 1125: Loss = 50.45163937086253\nIteration 1126: Loss = 50.43508591308341\nIteration 1127: Loss = 50.41855193090893\nIteration 1128: Loss = 50.40203739914635\nIteration 1129: Loss = 50.38554229263564\nIteration 1130: Loss = 50.369066586249495\nIteration 1131: Loss = 50.3526102548932\nIteration 1132: Loss = 50.336173273504656\nIteration 1133: Loss = 50.31975561705435\nIteration 1134: Loss = 50.30335726054522\nIteration 1135: Loss = 50.28697817901273\nIteration 1136: Loss = 50.270618347524746\nIteration 1137: Loss = 50.254277741181525\nIteration 1138: Loss = 50.23795633511567\nIteration 1139: Loss = 50.22165410449207\nIteration 1140: Loss = 50.20537102450789\nIteration 1141: Loss = 50.189107070392545\nIteration 1142: Loss = 50.17286221740753\nIteration 1143: Loss = 50.156636440846576\nIteration 1144: Loss = 50.140429716035456\nIteration 1145: Loss = 50.124242018332005\nIteration 1146: Loss = 50.108073323126085\nIteration 1147: Loss = 50.09192360583947\nIteration 1148: Loss = 50.0757928419259\nIteration 1149: Loss = 50.05968100687104\nIteration 1150: Loss = 50.04358807619232\nIteration 1151: Loss = 50.02751402543901\nIteration 1152: Loss = 50.01145883019215\nIteration 1153: Loss = 49.995422466064454\nIteration 1154: Loss = 49.97940490870038\nIteration 1155: Loss = 49.96340613377599\nIteration 1156: Loss = 49.9474261169989\nIteration 1157: Loss = 49.931464834108375\nIteration 1158: Loss = 49.9155222608751\nIteration 1159: Loss = 49.8995983731013\nIteration 1160: Loss = 49.88369314662059\nIteration 1161: Loss = 49.86780655729799\nIteration 1162: Loss = 49.85193858102988\nIteration 1163: Loss = 49.83608919374394\nIteration 1164: Loss = 49.82025837139912\nIteration 1165: Loss = 49.80444608998558\nIteration 1166: Loss = 49.7886523255247\nIteration 1167: Loss = 49.772877054069006\nIteration 1168: Loss = 49.75712025170211\nIteration 1169: Loss = 49.741381894538684\nIteration 1170: Loss = 49.72566195872444\nIteration 1171: Loss = 49.7099604204361\nIteration 1172: Loss = 49.694277255881246\nIteration 1173: Loss = 49.6786124412985\nIteration 1174: Loss = 49.662965952957244\nIteration 1175: Loss = 49.6473377671577\nIteration 1176: Loss = 49.63172786023091\nIteration 1177: Loss = 49.616136208538634\nIteration 1178: Loss = 49.60056278847333\nIteration 1179: Loss = 49.585007576458146\nIteration 1180: Loss = 49.56947054894686\nIteration 1181: Loss = 49.55395168242379\nIteration 1182: Loss = 49.538450953403846\nIteration 1183: Loss = 49.522968338432406\nIteration 1184: Loss = 49.507503814085375\nIteration 1185: Loss = 49.49205735696899\nIteration 1186: Loss = 49.47662894371998\nIteration 1187: Loss = 49.46121855100534\nIteration 1188: Loss = 49.44582615552241\nIteration 1189: Loss = 49.4304517339988\nIteration 1190: Loss = 49.415095263192335\nIteration 1191: Loss = 49.39975671989104\nIteration 1192: Loss = 49.38443608091308\nIteration 1193: Loss = 49.36913332310676\nIteration 1194: Loss = 49.35384842335042\nIteration 1195: Loss = 49.33858135855245\nIteration 1196: Loss = 49.323332105651254\nIteration 1197: Loss = 49.30810064161517\nIteration 1198: Loss = 49.292886943442454\nIteration 1199: Loss = 49.27769098816123\nIteration 1200: Loss = 49.26251275282949\nIteration 1201: Loss = 49.247352214534985\nIteration 1202: Loss = 49.2322093503953\nIteration 1203: Loss = 49.217084137557656\nIteration 1204: Loss = 49.201976553199\nIteration 1205: Loss = 49.186886574525936\nIteration 1206: Loss = 49.17181417877464\nIteration 1207: Loss = 49.1567593432109\nIteration 1208: Loss = 49.14172204513\nIteration 1209: Loss = 49.12670226185675\nIteration 1210: Loss = 49.11169997074535\nIteration 1211: Loss = 49.0967151491795\nIteration 1212: Loss = 49.08174777457221\nIteration 1213: Loss = 49.06679782436586\nIteration 1214: Loss = 49.05186527603212\nIteration 1215: Loss = 49.03695010707191\nIteration 1216: Loss = 49.02205229501543\nIteration 1217: Loss = 49.007171817422005\nIteration 1218: Loss = 48.99230865188014\nIteration 1219: Loss = 48.97746277600742\nIteration 1220: Loss = 48.96263416745057\nIteration 1221: Loss = 48.947822803885245\nIteration 1222: Loss = 48.93302866301621\nIteration 1223: Loss = 48.91825172257711\nIteration 1224: Loss = 48.90349196033058\nIteration 1225: Loss = 48.88874935406805\nIteration 1226: Loss = 48.87402388160989\nIteration 1227: Loss = 48.8593155208052\nIteration 1228: Loss = 48.84462424953191\nIteration 1229: Loss = 48.82995004569667\nIteration 1230: Loss = 48.81529288723482\nIteration 1231: Loss = 48.800652752110366\nIteration 1232: Loss = 48.786029618315915\nIteration 1233: Loss = 48.7714234638727\nIteration 1234: Loss = 48.75683426683048\nIteration 1235: Loss = 48.742262005267506\nIteration 1236: Loss = 48.72770665729055\nIteration 1237: Loss = 48.71316820103479\nIteration 1238: Loss = 48.698646614663794\nIteration 1239: Loss = 48.68414187636952\nIteration 1240: Loss = 48.66965396437226\nIteration 1241: Loss = 48.65518285692057\nIteration 1242: Loss = 48.64072853229125\nIteration 1243: Loss = 48.62629096878935\nIteration 1244: Loss = 48.61187014474809\nIteration 1245: Loss = 48.59746603852882\nIteration 1246: Loss = 48.58307862852102\nIteration 1247: Loss = 48.56870789314222\nIteration 1248: Loss = 48.55435381083797\nIteration 1249: Loss = 48.54001636008187\nIteration 1250: Loss = 48.525695519375425\nIteration 1251: Loss = 48.511391267248115\nIteration 1252: Loss = 48.49710358225724\nIteration 1253: Loss = 48.48283244298801\nIteration 1254: Loss = 48.46857782805345\nIteration 1255: Loss = 48.45433971609433\nIteration 1256: Loss = 48.44011808577919\nIteration 1257: Loss = 48.42591291580427\nIteration 1258: Loss = 48.41172418489348\nIteration 1259: Loss = 48.39755187179837\nIteration 1260: Loss = 48.3833959552981\nIteration 1261: Loss = 48.369256414199356\nIteration 1262: Loss = 48.355133227336374\nIteration 1263: Loss = 48.34102637357094\nIteration 1264: Loss = 48.326935831792184\nIteration 1265: Loss = 48.31286158091673\nIteration 1266: Loss = 48.2988035998886\nIteration 1267: Loss = 48.284761867679094\nIteration 1268: Loss = 48.27073636328691\nIteration 1269: Loss = 48.25672706573796\nIteration 1270: Loss = 48.24273395408544\nIteration 1271: Loss = 48.22875700740974\nIteration 1272: Loss = 48.214796204818406\nIteration 1273: Loss = 48.200851525446154\nIteration 1274: Loss = 48.186922948454765\nIteration 1275: Loss = 48.17301045303313\nIteration 1276: Loss = 48.15911401839713\nIteration 1277: Loss = 48.14523362378967\nIteration 1278: Loss = 48.131369248480596\nIteration 1279: Loss = 48.1175208717667\nIteration 1280: Loss = 48.103688472971655\nIteration 1281: Loss = 48.089872031446006\nIteration 1282: Loss = 48.076071526567084\nIteration 1283: Loss = 48.062286937739024\nIteration 1284: Loss = 48.04851824439274\nIteration 1285: Loss = 48.03476542598583\nIteration 1286: Loss = 48.021028462002576\nIteration 1287: Loss = 48.007307331953925\nIteration 1288: Loss = 47.993602015377434\nIteration 1289: Loss = 47.97991249183722\nIteration 1290: Loss = 47.96623874092397\nIteration 1291: Loss = 47.95258074225486\nIteration 1292: Loss = 47.93893847547355\nIteration 1293: Loss = 47.925311920250145\nIteration 1294: Loss = 47.911701056281146\nIteration 1295: Loss = 47.89810586328943\nIteration 1296: Loss = 47.88452632102421\nIteration 1297: Loss = 47.870962409261004\nIteration 1298: Loss = 47.85741410780162\nIteration 1299: Loss = 47.84388139647406\nIteration 1300: Loss = 47.83036425513256\nIteration 1301: Loss = 47.8168626636575\nIteration 1302: Loss = 47.80337660195542\nIteration 1303: Loss = 47.78990604995894\nIteration 1304: Loss = 47.776450987626745\nIteration 1305: Loss = 47.763011394943554\nIteration 1306: Loss = 47.74958725192008\nIteration 1307: Loss = 47.73617853859304\nIteration 1308: Loss = 47.722785235025015\nIteration 1309: Loss = 47.70940732130451\nIteration 1310: Loss = 47.6960447775459\nIteration 1311: Loss = 47.68269758388941\nIteration 1312: Loss = 47.66936572050102\nIteration 1313: Loss = 47.65604916757246\nIteration 1314: Loss = 47.64274790532127\nIteration 1315: Loss = 47.6294619139906\nIteration 1316: Loss = 47.61619117384929\nIteration 1317: Loss = 47.602935665191815\nIteration 1318: Loss = 47.58969536833826\nIteration 1319: Loss = 47.576470263634235\nIteration 1320: Loss = 47.5632603314509\nIteration 1321: Loss = 47.55006555218491\nIteration 1322: Loss = 47.53688590625837\nIteration 1323: Loss = 47.52372137411885\nIteration 1324: Loss = 47.51057193623927\nIteration 1325: Loss = 47.49743757311795\nIteration 1326: Loss = 47.4843182652785\nIteration 1327: Loss = 47.47121399326988\nIteration 1328: Loss = 47.45812473766626\nIteration 1329: Loss = 47.44505047906709\nIteration 1330: Loss = 47.43199119809697\nIteration 1331: Loss = 47.41894687540572\nIteration 1332: Loss = 47.405917491668255\nIteration 1333: Loss = 47.39290302758459\nIteration 1334: Loss = 47.37990346387984\nIteration 1335: Loss = 47.36691878130413\nIteration 1336: Loss = 47.35394896063258\nIteration 1337: Loss = 47.34099398266532\nIteration 1338: Loss = 47.32805382822737\nIteration 1339: Loss = 47.31512847816868\nIteration 1340: Loss = 47.30221791336408\nIteration 1341: Loss = 47.28932211471323\nIteration 1342: Loss = 47.276441063140595\nIteration 1343: Loss = 47.26357473959544\nIteration 1344: Loss = 47.25072312505173\nIteration 1345: Loss = 47.23788620050821\nIteration 1346: Loss = 47.22506394698822\nIteration 1347: Loss = 47.21225634553981\nIteration 1348: Loss = 47.19946337723564\nIteration 1349: Loss = 47.18668502317293\nIteration 1350: Loss = 47.17392126447347\nIteration 1351: Loss = 47.16117208228356\nIteration 1352: Loss = 47.148437457774\nIteration 1353: Loss = 47.13571737214004\nIteration 1354: Loss = 47.12301180660135\nIteration 1355: Loss = 47.110320742401996\nIteration 1356: Loss = 47.097644160810404\nIteration 1357: Loss = 47.084982043119346\nIteration 1358: Loss = 47.072334370645855\nIteration 1359: Loss = 47.05970112473129\nIteration 1360: Loss = 47.04708228674116\nIteration 1361: Loss = 47.03447783806528\nIteration 1362: Loss = 47.02188776011754\nIteration 1363: Loss = 47.009312034336034\nIteration 1364: Loss = 46.99675064218294\nIteration 1365: Loss = 46.98420356514453\nIteration 1366: Loss = 46.97167078473109\nIteration 1367: Loss = 46.95915228247697\nIteration 1368: Loss = 46.94664803994045\nIteration 1369: Loss = 46.934158038703806\nIteration 1370: Loss = 46.92168226037322\nIteration 1371: Loss = 46.90922068657875\nIteration 1372: Loss = 46.896773298974345\nIteration 1373: Loss = 46.88434007923777\nIteration 1374: Loss = 46.871921009070554\nIteration 1375: Loss = 46.859516070198076\nIteration 1376: Loss = 46.847125244369344\nIteration 1377: Loss = 46.834748513357155\nIteration 1378: Loss = 46.822385858957944\nIteration 1379: Loss = 46.8100372629918\nIteration 1380: Loss = 46.79770270730242\nIteration 1381: Loss = 46.78538217375707\nIteration 1382: Loss = 46.7730756442466\nIteration 1383: Loss = 46.76078310068536\nIteration 1384: Loss = 46.748504525011185\nIteration 1385: Loss = 46.736239899185385\nIteration 1386: Loss = 46.723989205192694\nIteration 1387: Loss = 46.71175242504123\nIteration 1388: Loss = 46.699529540762505\nIteration 1389: Loss = 46.68732053441136\nIteration 1390: Loss = 46.67512538806594\nIteration 1391: Loss = 46.66294408382766\nIteration 1392: Loss = 46.65077660382119\nIteration 1393: Loss = 46.63862293019443\nIteration 1394: Loss = 46.62648304511846\nIteration 1395: Loss = 46.6143569307875\nIteration 1396: Loss = 46.6022445694189\nIteration 1397: Loss = 46.59014594325312\nIteration 1398: Loss = 46.578061034553706\nIteration 1399: Loss = 46.5659898256072\nIteration 1400: Loss = 46.55393229872316\nIteration 1401: Loss = 46.54188843623414\nIteration 1402: Loss = 46.529858220495626\nIteration 1403: Loss = 46.51784163388602\nIteration 1404: Loss = 46.50583865880663\nIteration 1405: Loss = 46.493849277681605\nIteration 1406: Loss = 46.48187347295791\nIteration 1407: Loss = 46.46991122710534\nIteration 1408: Loss = 46.45796252261643\nIteration 1409: Loss = 46.44602734200646\nIteration 1410: Loss = 46.43410566781346\nIteration 1411: Loss = 46.42219748259807\nIteration 1412: Loss = 46.410302768943644\nIteration 1413: Loss = 46.39842150945613\nIteration 1414: Loss = 46.38655368676403\nIteration 1415: Loss = 46.374699283518495\nIteration 1416: Loss = 46.362858282393134\nIteration 1417: Loss = 46.351030666084085\nIteration 1418: Loss = 46.33921641730996\nIteration 1419: Loss = 46.327415518811854\nIteration 1420: Loss = 46.3156279533532\nIteration 1421: Loss = 46.30385370371986\nIteration 1422: Loss = 46.292092752720094\nIteration 1423: Loss = 46.28034508318442\nIteration 1424: Loss = 46.26861067796571\nIteration 1425: Loss = 46.25688951993906\nIteration 1426: Loss = 46.245181592001856\nIteration 1427: Loss = 46.233486877073666\nIteration 1428: Loss = 46.22180535809624\nIteration 1429: Loss = 46.21013701803351\nIteration 1430: Loss = 46.19848183987151\nIteration 1431: Loss = 46.18683980661838\nIteration 1432: Loss = 46.17521090130434\nIteration 1433: Loss = 46.16359510698164\nIteration 1434: Loss = 46.151992406724524\nIteration 1435: Loss = 46.14040278362927\nIteration 1436: Loss = 46.128826220814055\nIteration 1437: Loss = 46.117262701419016\nIteration 1438: Loss = 46.105712208606164\nIteration 1439: Loss = 46.094174725559405\nIteration 1440: Loss = 46.08265023548447\nIteration 1441: Loss = 46.07113872160892\nIteration 1442: Loss = 46.059640167182074\nIteration 1443: Loss = 46.048154555475016\nIteration 1444: Loss = 46.03668186978057\nIteration 1445: Loss = 46.02522209341326\nIteration 1446: Loss = 46.013775209709245\nIteration 1447: Loss = 46.00234120202638\nIteration 1448: Loss = 45.9909200537441\nIteration 1449: Loss = 45.97951174826344\nIteration 1450: Loss = 45.96811626900696\nIteration 1451: Loss = 45.95673359941882\nIteration 1452: Loss = 45.945363722964636\nIteration 1453: Loss = 45.934006623131495\nIteration 1454: Loss = 45.92266228342794\nIteration 1455: Loss = 45.91133068738394\nIteration 1456: Loss = 45.900011818550844\nIteration 1457: Loss = 45.888705660501394\nIteration 1458: Loss = 45.87741219682962\nIteration 1459: Loss = 45.866131411150896\nIteration 1460: Loss = 45.85486328710185\nIteration 1461: Loss = 45.843607808340394\nIteration 1462: Loss = 45.83236495854567\nIteration 1463: Loss = 45.82113472141794\nIteration 1464: Loss = 45.80991708067874\nIteration 1465: Loss = 45.798712020070674\nIteration 1466: Loss = 45.78751952335748\nIteration 1467: Loss = 45.776339574324034\nIteration 1468: Loss = 45.76517215677619\nIteration 1469: Loss = 45.754017254540884\nIteration 1470: Loss = 45.742874851466055\nIteration 1471: Loss = 45.7317449314206\nIteration 1472: Loss = 45.72062747829439\nIteration 1473: Loss = 45.709522475998206\nIteration 1474: Loss = 45.69842990846372\nIteration 1475: Loss = 45.687349759643496\nIteration 1476: Loss = 45.67628201351093\nIteration 1477: Loss = 45.6652266540602\nIteration 1478: Loss = 45.65418366530632\nIteration 1479: Loss = 45.64315303128505\nIteration 1480: Loss = 45.63213473605288\nIteration 1481: Loss = 45.621128763687004\nIteration 1482: Loss = 45.61013509828529\nIteration 1483: Loss = 45.59915372396628\nIteration 1484: Loss = 45.58818462486914\nIteration 1485: Loss = 45.57722778515361\nIteration 1486: Loss = 45.56628318900004\nIteration 1487: Loss = 45.55535082060928\nIteration 1488: Loss = 45.544430664202736\nIteration 1489: Loss = 45.53352270402232\nIteration 1490: Loss = 45.522626924330346\nIteration 1491: Loss = 45.51174330940966\nIteration 1492: Loss = 45.50087184356343\nIteration 1493: Loss = 45.490012511115246\nIteration 1494: Loss = 45.47916529640912\nIteration 1495: Loss = 45.46833018380928\nIteration 1496: Loss = 45.457507157700334\nIteration 1497: Loss = 45.44669620248719\nIteration 1498: Loss = 45.43589730259493\nIteration 1499: Loss = 45.425110442468956\nIteration 1500: Loss = 45.41433560657482\nIteration 1501: Loss = 45.40357277939823\nIteration 1502: Loss = 45.39282194544513\nIteration 1503: Loss = 45.382083089241476\nIteration 1504: Loss = 45.37135619533339\nIteration 1505: Loss = 45.360641248287074\nIteration 1506: Loss = 45.34993823268871\nIteration 1507: Loss = 45.33924713314457\nIteration 1508: Loss = 45.32856793428089\nIteration 1509: Loss = 45.31790062074384\nIteration 1510: Loss = 45.307245177199604\nIteration 1511: Loss = 45.29660158833421\nIteration 1512: Loss = 45.28596983885362\nIteration 1513: Loss = 45.27534991348362\nIteration 1514: Loss = 45.26474179696989\nIteration 1515: Loss = 45.25414547407787\nIteration 1516: Loss = 45.24356092959282\nIteration 1517: Loss = 45.23298814831973\nIteration 1518: Loss = 45.22242711508335\nIteration 1519: Loss = 45.21187781472811\nIteration 1520: Loss = 45.20134023211817\nIteration 1521: Loss = 45.19081435213732\nIteration 1522: Loss = 45.180300159688954\nIteration 1523: Loss = 45.16979763969612\nIteration 1524: Loss = 45.159306777101435\nIteration 1525: Loss = 45.148827556867055\nIteration 1526: Loss = 45.138359963974686\nIteration 1527: Loss = 45.12790398342552\nIteration 1528: Loss = 45.11745960024025\nIteration 1529: Loss = 45.10702679945899\nIteration 1530: Loss = 45.09660556614133\nIteration 1531: Loss = 45.08619588536622\nIteration 1532: Loss = 45.07579774223205\nIteration 1533: Loss = 45.06541112185646\nIteration 1534: Loss = 45.05503600937652\nIteration 1535: Loss = 45.04467238994855\nIteration 1536: Loss = 45.034320248748166\nIteration 1537: Loss = 45.023979570970226\nIteration 1538: Loss = 45.01365034182883\nIteration 1539: Loss = 45.00333254655727\nIteration 1540: Loss = 44.99302617040802\nIteration 1541: Loss = 44.9827311986527\nIteration 1542: Loss = 44.97244761658208\nIteration 1543: Loss = 44.96217540950601\nIteration 1544: Loss = 44.95191456275343\nIteration 1545: Loss = 44.941665061672325\nIteration 1546: Loss = 44.93142689162971\nIteration 1547: Loss = 44.92120003801163\nIteration 1548: Loss = 44.91098448622308\nIteration 1549: Loss = 44.90078022168803\nIteration 1550: Loss = 44.89058722984934\nIteration 1551: Loss = 44.880405496168834\nIteration 1552: Loss = 44.870235006127174\nIteration 1553: Loss = 44.86007574522392\nIteration 1554: Loss = 44.849927698977396\nIteration 1555: Loss = 44.83979085292483\nIteration 1556: Loss = 44.829665192622116\nIteration 1557: Loss = 44.81955070364402\nIteration 1558: Loss = 44.80944737158397\nIteration 1559: Loss = 44.79935518205413\nIteration 1560: Loss = 44.789274120685356\nIteration 1561: Loss = 44.77920417312715\nIteration 1562: Loss = 44.76914532504765\nIteration 1563: Loss = 44.75909756213363\nIteration 1564: Loss = 44.74906087009044\nIteration 1565: Loss = 44.73903523464198\nIteration 1566: Loss = 44.729020641530695\nIteration 1567: Loss = 44.719017076517595\nIteration 1568: Loss = 44.7090245253821\nIteration 1569: Loss = 44.69904297392217\nIteration 1570: Loss = 44.68907240795415\nIteration 1571: Loss = 44.67911281331288\nIteration 1572: Loss = 44.66916417585151\nIteration 1573: Loss = 44.65922648144163\nIteration 1574: Loss = 44.64929971597315\nIteration 1575: Loss = 44.63938386535432\nIteration 1576: Loss = 44.629478915511676\nIteration 1577: Loss = 44.619584852390005\nIteration 1578: Loss = 44.609701661952435\nIteration 1579: Loss = 44.59982933018023\nIteration 1580: Loss = 44.5899678430729\nIteration 1581: Loss = 44.580117186648145\nIteration 1582: Loss = 44.57027734694182\nIteration 1583: Loss = 44.5604483100079\nIteration 1584: Loss = 44.5506300619185\nIteration 1585: Loss = 44.540822588763774\nIteration 1586: Loss = 44.53102587665202\nIteration 1587: Loss = 44.52123991170948\nIteration 1588: Loss = 44.51146468008049\nIteration 1589: Loss = 44.50170016792735\nIteration 1590: Loss = 44.49194636143034\nIteration 1591: Loss = 44.48220324678768\nIteration 1592: Loss = 44.47247081021551\nIteration 1593: Loss = 44.46274903794788\nIteration 1594: Loss = 44.45303791623674\nIteration 1595: Loss = 44.443337431351836\nIteration 1596: Loss = 44.43364756958082\nIteration 1597: Loss = 44.42396831722907\nIteration 1598: Loss = 44.41429966061983\nIteration 1599: Loss = 44.40464158609405\nIteration 1600: Loss = 44.39499408001044\nIteration 1601: Loss = 44.38535712874543\nIteration 1602: Loss = 44.375730718693134\nIteration 1603: Loss = 44.366114836265325\nIteration 1604: Loss = 44.356509467891456\nIteration 1605: Loss = 44.34691460001857\nIteration 1606: Loss = 44.33733021911133\nIteration 1607: Loss = 44.32775631165196\nIteration 1608: Loss = 44.31819286414029\nIteration 1609: Loss = 44.30863986309361\nIteration 1610: Loss = 44.299097295046764\nIteration 1611: Loss = 44.28956514655208\nIteration 1612: Loss = 44.28004340417934\nIteration 1613: Loss = 44.270532054515755\nIteration 1614: Loss = 44.26103108416601\nIteration 1615: Loss = 44.251540479752116\nIteration 1616: Loss = 44.242060227913505\nIteration 1617: Loss = 44.23259031530693\nIteration 1618: Loss = 44.223130728606506\nIteration 1619: Loss = 44.213681454503636\nIteration 1620: Loss = 44.20424247970699\nIteration 1621: Loss = 44.19481379094253\nIteration 1622: Loss = 44.18539537495344\nIteration 1623: Loss = 44.17598721850013\nIteration 1624: Loss = 44.16658930836018\nIteration 1625: Loss = 44.15720163132836\nIteration 1626: Loss = 44.14782417421662\nIteration 1627: Loss = 44.138456923853965\nIteration 1628: Loss = 44.12909986708657\nIteration 1629: Loss = 44.11975299077766\n",
      "Iteration 1630: Loss = 44.11041628180754\nIteration 1631: Loss = 44.10108972707351\nIteration 1632: Loss = 44.09177331348996\nIteration 1633: Loss = 44.0824670279882\nIteration 1634: Loss = 44.073170857516544\nIteration 1635: Loss = 44.06388478904028\nIteration 1636: Loss = 44.05460880954157\nIteration 1637: Loss = 44.04534290601955\nIteration 1638: Loss = 44.036087065490165\nIteration 1639: Loss = 44.02684127498628\nIteration 1640: Loss = 44.01760552155756\nIteration 1641: Loss = 44.00837979227052\nIteration 1642: Loss = 43.99916407420845\nIteration 1643: Loss = 43.98995835447143\nIteration 1644: Loss = 43.980762620176286\nIteration 1645: Loss = 43.97157685845655\nIteration 1646: Loss = 43.96240105646252\nIteration 1647: Loss = 43.953235201361124\nIteration 1648: Loss = 43.944079280336005\nIteration 1649: Loss = 43.9349332805874\nIteration 1650: Loss = 43.92579718933221\nIteration 1651: Loss = 43.91667099380392\nIteration 1652: Loss = 43.907554681252584\nIteration 1653: Loss = 43.89844823894483\nIteration 1654: Loss = 43.88935165416383\nIteration 1655: Loss = 43.88026491420924\nIteration 1656: Loss = 43.871188006397254\nIteration 1657: Loss = 43.86212091806047\nIteration 1658: Loss = 43.85306363654802\nIteration 1659: Loss = 43.844016149225396\nIteration 1660: Loss = 43.83497844347454\nIteration 1661: Loss = 43.82595050669375\nIteration 1662: Loss = 43.816932326297724\nIteration 1663: Loss = 43.807923889717486\nIteration 1664: Loss = 43.79892518440035\nIteration 1665: Loss = 43.78993619781\nIteration 1666: Loss = 43.780956917426344\nIteration 1667: Loss = 43.771987330745574\nIteration 1668: Loss = 43.76302742528011\nIteration 1669: Loss = 43.7540771885586\nIteration 1670: Loss = 43.74513660812587\nIteration 1671: Loss = 43.73620567154294\nIteration 1672: Loss = 43.72728436638699\nIteration 1673: Loss = 43.7183726802513\nIteration 1674: Loss = 43.70947060074527\nIteration 1675: Loss = 43.700578115494444\nIteration 1676: Loss = 43.69169521214035\nIteration 1677: Loss = 43.682821878340626\nIteration 1678: Loss = 43.67395810176893\nIteration 1679: Loss = 43.66510387011491\nIteration 1680: Loss = 43.6562591710842\nIteration 1681: Loss = 43.64742399239842\nIteration 1682: Loss = 43.63859832179513\nIteration 1683: Loss = 43.629782147027804\nIteration 1684: Loss = 43.62097545586581\nIteration 1685: Loss = 43.61217823609444\nIteration 1686: Loss = 43.6033904755148\nIteration 1687: Loss = 43.594612161943864\nIteration 1688: Loss = 43.585843283214444\nIteration 1689: Loss = 43.57708382717511\nIteration 1690: Loss = 43.568333781690235\nIteration 1691: Loss = 43.559593134639954\nIteration 1692: Loss = 43.55086187392014\nIteration 1693: Loss = 43.54213998744238\nIteration 1694: Loss = 43.53342746313397\nIteration 1695: Loss = 43.52472428893787\nIteration 1696: Loss = 43.5160304528127\nIteration 1697: Loss = 43.50734594273273\nIteration 1698: Loss = 43.498670746687814\nIteration 1699: Loss = 43.49000485268348\nIteration 1700: Loss = 43.48134824874071\nIteration 1701: Loss = 43.47270092289616\nIteration 1702: Loss = 43.46406286320195\nIteration 1703: Loss = 43.455434057725746\nIteration 1704: Loss = 43.44681449455071\nIteration 1705: Loss = 43.43820416177543\nIteration 1706: Loss = 43.42960304751406\nIteration 1707: Loss = 43.421011139896066\nIteration 1708: Loss = 43.4124284270664\nIteration 1709: Loss = 43.40385489718539\nIteration 1710: Loss = 43.39529053842875\nIteration 1711: Loss = 43.38673533898753\nIteration 1712: Loss = 43.37818928706812\nIteration 1713: Loss = 43.36965237089225\nIteration 1714: Loss = 43.36112457869692\nIteration 1715: Loss = 43.3526058987344\nIteration 1716: Loss = 43.34409631927223\nIteration 1717: Loss = 43.3355958285932\nIteration 1718: Loss = 43.32710441499529\nIteration 1719: Loss = 43.31862206679168\nIteration 1720: Loss = 43.31014877231073\nIteration 1721: Loss = 43.30168451989597\nIteration 1722: Loss = 43.29322929790607\nIteration 1723: Loss = 43.28478309471477\nIteration 1724: Loss = 43.276345898710964\nIteration 1725: Loss = 43.2679176982986\nIteration 1726: Loss = 43.259498481896664\nIteration 1727: Loss = 43.251088237939236\nIteration 1728: Loss = 43.24268695487537\nIteration 1729: Loss = 43.23429462116911\nIteration 1730: Loss = 43.22591122529955\nIteration 1731: Loss = 43.217536755760655\nIteration 1732: Loss = 43.209171201061395\nIteration 1733: Loss = 43.20081454972564\nIteration 1734: Loss = 43.19246679029218\nIteration 1735: Loss = 43.184127911314654\nIteration 1736: Loss = 43.175797901361584\nIteration 1737: Loss = 43.16747674901635\nIteration 1738: Loss = 43.15916444287712\nIteration 1739: Loss = 43.15086097155694\nIteration 1740: Loss = 43.142566323683546\nIteration 1741: Loss = 43.13428048789952\nIteration 1742: Loss = 43.12600345286217\nIteration 1743: Loss = 43.117735207243506\nIteration 1744: Loss = 43.10947573973027\nIteration 1745: Loss = 43.101225039023916\nIteration 1746: Loss = 43.092983093840544\nIteration 1747: Loss = 43.0847498929109\nIteration 1748: Loss = 43.076525424980396\nIteration 1749: Loss = 43.06830967880902\nIteration 1750: Loss = 43.06010264317139\nIteration 1751: Loss = 43.051904306856684\nIteration 1752: Loss = 43.043714658668634\nIteration 1753: Loss = 43.03553368742553\nIteration 1754: Loss = 43.02736138196014\nIteration 1755: Loss = 43.01919773111982\nIteration 1756: Loss = 43.011042723766295\nIteration 1757: Loss = 43.00289634877584\nIteration 1758: Loss = 42.99475859503914\nIteration 1759: Loss = 42.98662945146132\nIteration 1760: Loss = 42.97850890696189\nIteration 1761: Loss = 42.97039695047478\nIteration 1762: Loss = 42.96229357094826\nIteration 1763: Loss = 42.95419875734497\nIteration 1764: Loss = 42.94611249864188\nIteration 1765: Loss = 42.93803478383026\nIteration 1766: Loss = 42.92996560191571\nIteration 1767: Loss = 42.92190494191807\nIteration 1768: Loss = 42.91385279287145\nIteration 1769: Loss = 42.905809143824214\nIteration 1770: Loss = 42.89777398383892\nIteration 1771: Loss = 42.88974730199235\nIteration 1772: Loss = 42.88172908737547\nIteration 1773: Loss = 42.87371932909341\nIteration 1774: Loss = 42.86571801626545\nIteration 1775: Loss = 42.85772513802497\nIteration 1776: Loss = 42.84974068351951\nIteration 1777: Loss = 42.84176464191065\nIteration 1778: Loss = 42.833797002374105\nIteration 1779: Loss = 42.8258377540996\nIteration 1780: Loss = 42.81788688629089\nIteration 1781: Loss = 42.8099443881658\nIteration 1782: Loss = 42.8020102489561\nIteration 1783: Loss = 42.79408445790759\nIteration 1784: Loss = 42.786167004279996\nIteration 1785: Loss = 42.77825787734704\nIteration 1786: Loss = 42.770357066396286\nIteration 1787: Loss = 42.76246456072933\nIteration 1788: Loss = 42.75458034966153\nIteration 1789: Loss = 42.74670442252223\nIteration 1790: Loss = 42.73883676865456\nIteration 1791: Loss = 42.73097737741551\nIteration 1792: Loss = 42.72312623817591\nIteration 1793: Loss = 42.715283340320354\nIteration 1794: Loss = 42.70744867324725\nIteration 1795: Loss = 42.69962222636877\nIteration 1796: Loss = 42.6918039891108\nIteration 1797: Loss = 42.68399395091301\nIteration 1798: Loss = 42.67619210122876\nIteration 1799: Loss = 42.668398429525084\nIteration 1800: Loss = 42.66061292528272\nIteration 1801: Loss = 42.65283557799606\nIteration 1802: Loss = 42.64506637717313\nIteration 1803: Loss = 42.6373053123356\nIteration 1804: Loss = 42.62955237301871\nIteration 1805: Loss = 42.62180754877132\nIteration 1806: Loss = 42.614070829155864\nIteration 1807: Loss = 42.6063422037483\nIteration 1808: Loss = 42.59862166213816\nIteration 1809: Loss = 42.59090919392844\nIteration 1810: Loss = 42.58320478873569\nIteration 1811: Loss = 42.57550843618993\nIteration 1812: Loss = 42.56782012593462\nIteration 1813: Loss = 42.5601398476267\nIteration 1814: Loss = 42.55246759093654\nIteration 1815: Loss = 42.544803345547884\nIteration 1816: Loss = 42.53714710115791\nIteration 1817: Loss = 42.52949884747716\nIteration 1818: Loss = 42.52185857422956\nIteration 1819: Loss = 42.51422627115233\nIteration 1820: Loss = 42.50660192799606\nIteration 1821: Loss = 42.498985534524635\nIteration 1822: Loss = 42.491377080515235\nIteration 1823: Loss = 42.4837765557583\nIteration 1824: Loss = 42.47618395005754\nIteration 1825: Loss = 42.46859925322992\nIteration 1826: Loss = 42.4610224551056\nIteration 1827: Loss = 42.45345354552795\nIteration 1828: Loss = 42.44589251435354\nIteration 1829: Loss = 42.4383393514521\nIteration 1830: Loss = 42.43079404670654\nIteration 1831: Loss = 42.423256590012876\nIteration 1832: Loss = 42.41572697128025\nIteration 1833: Loss = 42.40820518043092\nIteration 1834: Loss = 42.40069120740022\nIteration 1835: Loss = 42.39318504213655\nIteration 1836: Loss = 42.385686674601374\nIteration 1837: Loss = 42.37819609476918\nIteration 1838: Loss = 42.370713292627485\nIteration 1839: Loss = 42.363238258176786\nIteration 1840: Loss = 42.3557709814306\nIteration 1841: Loss = 42.34831145241538\nIteration 1842: Loss = 42.34085966117053\nIteration 1843: Loss = 42.333415597748434\nIteration 1844: Loss = 42.325979252214296\nIteration 1845: Loss = 42.318550614646334\nIteration 1846: Loss = 42.31112967513556\nIteration 1847: Loss = 42.30371642378593\nIteration 1848: Loss = 42.29631085071417\nIteration 1849: Loss = 42.28891294604988\nIteration 1850: Loss = 42.2815226999355\nIteration 1851: Loss = 42.27414010252621\nIteration 1852: Loss = 42.26676514399004\nIteration 1853: Loss = 42.25939781450775\nIteration 1854: Loss = 42.252038104272835\nIteration 1855: Loss = 42.244686003491545\nIteration 1856: Loss = 42.23734150238286\nIteration 1857: Loss = 42.23000459117843\nIteration 1858: Loss = 42.22267526012261\nIteration 1859: Loss = 42.215353499472435\nIteration 1860: Loss = 42.20803929949754\nIteration 1861: Loss = 42.200732650480255\nIteration 1862: Loss = 42.19343354271548\nIteration 1863: Loss = 42.18614196651073\nIteration 1864: Loss = 42.17885791218614\nIteration 1865: Loss = 42.17158137007438\nIteration 1866: Loss = 42.16431233052066\nIteration 1867: Loss = 42.15705078388276\nIteration 1868: Loss = 42.14979672053095\nIteration 1869: Loss = 42.14255013084806\nIteration 1870: Loss = 42.135311005229305\nIteration 1871: Loss = 42.128079334082486\nIteration 1872: Loss = 42.12085510782777\nIteration 1873: Loss = 42.113638316897834\nIteration 1874: Loss = 42.106428951737726\nIteration 1875: Loss = 42.09922700280491\nIteration 1876: Loss = 42.092032460569264\nIteration 1877: Loss = 42.08484531551301\nIteration 1878: Loss = 42.07766555813075\nIteration 1879: Loss = 42.07049317892942\nIteration 1880: Loss = 42.06332816842827\nIteration 1881: Loss = 42.05617051715892\nIteration 1882: Loss = 42.04902021566519\nIteration 1883: Loss = 42.041877254503255\nIteration 1884: Loss = 42.034741624241505\nIteration 1885: Loss = 42.02761331546061\nIteration 1886: Loss = 42.02049231875344\nIteration 1887: Loss = 42.013378624725114\nIteration 1888: Loss = 42.006272223992916\nIteration 1889: Loss = 41.99917310718633\nIteration 1890: Loss = 41.99208126494703\nIteration 1891: Loss = 41.984996687928785\nIteration 1892: Loss = 41.97791936679756\nIteration 1893: Loss = 41.97084929223137\nIteration 1894: Loss = 41.96378645492042\nIteration 1895: Loss = 41.956730845566916\nIteration 1896: Loss = 41.9496824548852\nIteration 1897: Loss = 41.94264127360166\nIteration 1898: Loss = 41.9356072924547\nIteration 1899: Loss = 41.92858050219477\nIteration 1900: Loss = 41.92156089358431\nIteration 1901: Loss = 41.91454845739779\nIteration 1902: Loss = 41.90754318442163\nIteration 1903: Loss = 41.9005450654542\nIteration 1904: Loss = 41.893554091305866\nIteration 1905: Loss = 41.88657025279888\nIteration 1906: Loss = 41.87959354076744\nIteration 1907: Loss = 41.872623946057615\nIteration 1908: Loss = 41.8656614595274\nIteration 1909: Loss = 41.85870607204664\nIteration 1910: Loss = 41.851757774497\nIteration 1911: Loss = 41.84481655777205\nIteration 1912: Loss = 41.83788241277714\nIteration 1913: Loss = 41.83095533042944\nIteration 1914: Loss = 41.82403530165792\nIteration 1915: Loss = 41.817122317403324\nIteration 1916: Loss = 41.810216368618136\nIteration 1917: Loss = 41.80331744626662\nIteration 1918: Loss = 41.79642554132476\nIteration 1919: Loss = 41.78954064478025\nIteration 1920: Loss = 41.78266274763251\nIteration 1921: Loss = 41.77579184089261\nIteration 1922: Loss = 41.76892791558333\nIteration 1923: Loss = 41.762070962739045\nIteration 1924: Loss = 41.75522097340586\nIteration 1925: Loss = 41.74837793864144\nIteration 1926: Loss = 41.74154184951506\nIteration 1927: Loss = 41.734712697107625\nIteration 1928: Loss = 41.72789047251161\nIteration 1929: Loss = 41.72107516683104\nIteration 1930: Loss = 41.714266771181514\nIteration 1931: Loss = 41.70746527669014\nIteration 1932: Loss = 41.700670674495555\nIteration 1933: Loss = 41.693882955747924\nIteration 1934: Loss = 41.68710211160887\nIteration 1935: Loss = 41.68032813325151\nIteration 1936: Loss = 41.673561011860436\nIteration 1937: Loss = 41.66680073863163\nIteration 1938: Loss = 41.66004730477258\nIteration 1939: Loss = 41.65330070150213\nIteration 1940: Loss = 41.64656092005057\nIteration 1941: Loss = 41.639827951659534\nIteration 1942: Loss = 41.63310178758205\nIteration 1943: Loss = 41.62638241908251\nIteration 1944: Loss = 41.61966983743665\nIteration 1945: Loss = 41.612964033931505\nIteration 1946: Loss = 41.60626499986545\nIteration 1947: Loss = 41.59957272654815\nIteration 1948: Loss = 41.59288720530054\nIteration 1949: Loss = 41.586208427454864\nIteration 1950: Loss = 41.579536384354554\nIteration 1951: Loss = 41.57287106735434\nIteration 1952: Loss = 41.56621246782015\nIteration 1953: Loss = 41.55956057712915\nIteration 1954: Loss = 41.55291538666967\nIteration 1955: Loss = 41.5462768878412\nIteration 1956: Loss = 41.53964507205447\nIteration 1957: Loss = 41.5330199307313\nIteration 1958: Loss = 41.52640145530466\nIteration 1959: Loss = 41.51978963721867\nIteration 1960: Loss = 41.51318446792853\nIteration 1961: Loss = 41.50658593890055\nIteration 1962: Loss = 41.499994041612105\nIteration 1963: Loss = 41.49340876755165\nIteration 1964: Loss = 41.48683010821867\nIteration 1965: Loss = 41.48025805512372\nIteration 1966: Loss = 41.47369259978835\nIteration 1967: Loss = 41.467133733745136\nIteration 1968: Loss = 41.460581448537624\nIteration 1969: Loss = 41.45403573572036\nIteration 1970: Loss = 41.44749658685887\nIteration 1971: Loss = 41.44096399352957\nIteration 1972: Loss = 41.43443794731989\nIteration 1973: Loss = 41.427918439828126\nIteration 1974: Loss = 41.42140546266351\nIteration 1975: Loss = 41.414899007446145\nIteration 1976: Loss = 41.40839906580706\nIteration 1977: Loss = 41.40190562938809\nIteration 1978: Loss = 41.39541868984196\nIteration 1979: Loss = 41.388938238832225\nIteration 1980: Loss = 41.38246426803325\nIteration 1981: Loss = 41.375996769130246\nIteration 1982: Loss = 41.36953573381917\nIteration 1983: Loss = 41.3630811538068\nIteration 1984: Loss = 41.35663302081067\nIteration 1985: Loss = 41.35019132655905\nIteration 1986: Loss = 41.343756062790966\nIteration 1987: Loss = 41.33732722125616\nIteration 1988: Loss = 41.33090479371511\nIteration 1989: Loss = 41.32448877193897\nIteration 1990: Loss = 41.31807914770958\nIteration 1991: Loss = 41.311675912819446\nIteration 1992: Loss = 41.305279059071744\nIteration 1993: Loss = 41.29888857828028\nIteration 1994: Loss = 41.29250446226949\nIteration 1995: Loss = 41.28612670287443\nIteration 1996: Loss = 41.279755291940745\nIteration 1997: Loss = 41.27339022132469\nIteration 1998: Loss = 41.26703148289305\nIteration 1999: Loss = 41.260679068523245\nIteration 2000: Loss = 41.25433297010317\nIteration 2001: Loss = 41.24799317953126\nIteration 2002: Loss = 41.241659688716496\nIteration 2003: Loss = 41.23533248957836\nIteration 2004: Loss = 41.22901157404681\nIteration 2005: Loss = 41.222696934062284\nIteration 2006: Loss = 41.21638856157571\nIteration 2007: Loss = 41.21008644854841\nIteration 2008: Loss = 41.203790586952195\nIteration 2009: Loss = 41.1975009687693\nIteration 2010: Loss = 41.19121758599231\nIteration 2011: Loss = 41.18494043062428\nIteration 2012: Loss = 41.1786694946786\nIteration 2013: Loss = 41.17240477017904\nIteration 2014: Loss = 41.16614624915974\nIteration 2015: Loss = 41.159893923665166\nIteration 2016: Loss = 41.15364778575012\nIteration 2017: Loss = 41.147407827479704\nIteration 2018: Loss = 41.14117404092934\nIteration 2019: Loss = 41.13494641818474\nIteration 2020: Loss = 41.128724951341894\nIteration 2021: Loss = 41.12250963250702\nIteration 2022: Loss = 41.116300453796605\nIteration 2023: Loss = 41.110097407337406\nIteration 2024: Loss = 41.103900485266344\nIteration 2025: Loss = 41.097709679730585\nIteration 2026: Loss = 41.09152498288748\nIteration 2027: Loss = 41.08534638690455\nIteration 2028: Loss = 41.07917388395949\nIteration 2029: Loss = 41.07300746624019\nIteration 2030: Loss = 41.06684712594463\nIteration 2031: Loss = 41.060692855280934\nIteration 2032: Loss = 41.054544646467356\nIteration 2033: Loss = 41.04840249173223\nIteration 2034: Loss = 41.04226638331401\nIteration 2035: Loss = 41.0361363134612\nIteration 2036: Loss = 41.03001227443238\nIteration 2037: Loss = 41.02389425849618\nIteration 2038: Loss = 41.017782257931245\nIteration 2039: Loss = 41.011676265026296\nIteration 2040: Loss = 41.00557627208001\nIteration 2041: Loss = 40.999482271401114\nIteration 2042: Loss = 40.99339425530825\nIteration 2043: Loss = 40.98731221613012\nIteration 2044: Loss = 40.9812361462053\nIteration 2045: Loss = 40.97516603788238\nIteration 2046: Loss = 40.969101883519855\nIteration 2047: Loss = 40.963043675486134\nIteration 2048: Loss = 40.95699140615954\nIteration 2049: Loss = 40.9509450679283\nIteration 2050: Loss = 40.944904653190505\nIteration 2051: Loss = 40.938870154354134\nIteration 2052: Loss = 40.932841563837016\nIteration 2053: Loss = 40.926818874066825\nIteration 2054: Loss = 40.92080207748104\nIteration 2055: Loss = 40.91479116652701\nIteration 2056: Loss = 40.90878613366184\nIteration 2057: Loss = 40.902786971352455\nIteration 2058: Loss = 40.89679367207556\nIteration 2059: Loss = 40.8908062283176\nIteration 2060: Loss = 40.88482463257481\nIteration 2061: Loss = 40.87884887735314\nIteration 2062: Loss = 40.872878955168304\nIteration 2063: Loss = 40.866914858545655\nIteration 2064: Loss = 40.86095658002036\nIteration 2065: Loss = 40.85500411213719\nIteration 2066: Loss = 40.84905744745062\nIteration 2067: Loss = 40.84311657852482\nIteration 2068: Loss = 40.83718149793356\nIteration 2069: Loss = 40.83125219826031\nIteration 2070: Loss = 40.82532867209811\nIteration 2071: Loss = 40.819410912049655\nIteration 2072: Loss = 40.81349891072725\nIteration 2073: Loss = 40.80759266075274\nIteration 2074: Loss = 40.801692154757596\nIteration 2075: Loss = 40.79579738538286\nIteration 2076: Loss = 40.78990834527909\nIteration 2077: Loss = 40.7840250271064\nIteration 2078: Loss = 40.77814742353444\nIteration 2079: Loss = 40.772275527242385\nIteration 2080: Loss = 40.76640933091888\nIteration 2081: Loss = 40.760548827262106\nIteration 2082: Loss = 40.75469400897968\nIteration 2083: Loss = 40.74884486878872\nIteration 2084: Loss = 40.74300139941579\nIteration 2085: Loss = 40.737163593596875\nIteration 2086: Loss = 40.73133144407742\nIteration 2087: Loss = 40.72550494361228\nIteration 2088: Loss = 40.71968408496569\nIteration 2089: Loss = 40.71386886091132\nIteration 2090: Loss = 40.70805926423219\nIteration 2091: Loss = 40.7022552877207\nIteration 2092: Loss = 40.69645692417861\nIteration 2093: Loss = 40.69066416641704\nIteration 2094: Loss = 40.68487700725637\nIteration 2095: Loss = 40.67909543952641\nIteration 2096: Loss = 40.67331945606619\nIteration 2097: Loss = 40.66754904972409\nIteration 2098: Loss = 40.66178421335774\nIteration 2099: Loss = 40.65602493983406\nIteration 2100: Loss = 40.650271222029225\nIteration 2101: Loss = 40.64452305282867\nIteration 2102: Loss = 40.638780425127024\nIteration 2103: Loss = 40.6330433318282\nIteration 2104: Loss = 40.62731176584528\nIteration 2105: Loss = 40.621585720100555\nIteration 2106: Loss = 40.615865187525515\nIteration 2107: Loss = 40.61015016106084\nIteration 2108: Loss = 40.60444063365632\nIteration 2109: Loss = 40.59873659827095\nIteration 2110: Loss = 40.59303804787285\nIteration 2111: Loss = 40.58734497543927\nIteration 2112: Loss = 40.58165737395655\nIteration 2113: Loss = 40.57597523642018\nIteration 2114: Loss = 40.570298555834704\nIteration 2115: Loss = 40.564627325213785\nIteration 2116: Loss = 40.55896153758011\nIteration 2117: Loss = 40.55330118596545\nIteration 2118: Loss = 40.54764626341065\nIteration 2119: Loss = 40.54199676296554\nIteration 2120: Loss = 40.536352677688974\nIteration 2121: Loss = 40.53071400064885\nIteration 2122: Loss = 40.525080724922056\nIteration 2123: Loss = 40.51945284359445\nIteration 2124: Loss = 40.51383034976088\nIteration 2125: Loss = 40.50821323652516\nIteration 2126: Loss = 40.502601497000036\nIteration 2127: Loss = 40.496995124307226\nIteration 2128: Loss = 40.49139411157734\nIteration 2129: Loss = 40.48579845194995\nIteration 2130: Loss = 40.48020813857348\nIteration 2131: Loss = 40.474623164605305\nIteration 2132: Loss = 40.46904352321164\nIteration 2133: Loss = 40.46346920756759\nIteration 2134: Loss = 40.457900210857126\nIteration 2135: Loss = 40.452336526273044\nIteration 2136: Loss = 40.44677814701699\nIteration 2137: Loss = 40.44122506629946\nIteration 2138: Loss = 40.4356772773397\nIteration 2139: Loss = 40.43013477336582\nIteration 2140: Loss = 40.424597547614695\nIteration 2141: Loss = 40.41906559333199\nIteration 2142: Loss = 40.41353890377213\nIteration 2143: Loss = 40.40801747219828\nIteration 2144: Loss = 40.40250129188239\nIteration 2145: Loss = 40.3969903561051\nIteration 2146: Loss = 40.39148465815582\nIteration 2147: Loss = 40.38598419133261\nIteration 2148: Loss = 40.38048894894231\nIteration 2149: Loss = 40.37499892430036\nIteration 2150: Loss = 40.369514110730954\nIteration 2151: Loss = 40.3640345015669\nIteration 2152: Loss = 40.35856009014968\nIteration 2153: Loss = 40.35309086982942\nIteration 2154: Loss = 40.3476268339649\nIteration 2155: Loss = 40.34216797592347\nIteration 2156: Loss = 40.336714289081144\nIteration 2157: Loss = 40.331265766822476\nIteration 2158: Loss = 40.325822402540666\nIteration 2159: Loss = 40.32038418963747\nIteration 2160: Loss = 40.31495112152317\nIteration 2161: Loss = 40.30952319161669\nIteration 2162: Loss = 40.30410039334538\nIteration 2163: Loss = 40.298682720145244\nIteration 2164: Loss = 40.29327016546072\nIteration 2165: Loss = 40.287862722744784\nIteration 2166: Loss = 40.282460385458904\nIteration 2167: Loss = 40.277063147073065\nIteration 2168: Loss = 40.27167100106568\nIteration 2169: Loss = 40.266283940923664\nIteration 2170: Loss = 40.260901960142384\nIteration 2171: Loss = 40.255525052225636\nIteration 2172: Loss = 40.25015321068565\nIteration 2173: Loss = 40.24478642904309\nIteration 2174: Loss = 40.23942470082702\nIteration 2175: Loss = 40.23406801957491\nIteration 2176: Loss = 40.22871637883262\nIteration 2177: Loss = 40.22336977215438\nIteration 2178: Loss = 40.21802819310278\nIteration 2179: Loss = 40.21269163524879\nIteration 2180: Loss = 40.20736009217171\nIteration 2181: Loss = 40.20203355745918\nIteration 2182: Loss = 40.19671202470715\nIteration 2183: Loss = 40.19139548751991\nIteration 2184: Loss = 40.186083939510034\nIteration 2185: Loss = 40.18077737429839\nIteration 2186: Loss = 40.17547578551411\nIteration 2187: Loss = 40.170179166794625\nIteration 2188: Loss = 40.16488751178561\nIteration 2189: Loss = 40.15960081414098\nIteration 2190: Loss = 40.15431906752291\nIteration 2191: Loss = 40.1490422656018\nIteration 2192: Loss = 40.143770402056234\nIteration 2193: Loss = 40.138503470573035\nIteration 2194: Loss = 40.1332414648472\nIteration 2195: Loss = 40.12798437858192\nIteration 2196: Loss = 40.122732205488575\nIteration 2197: Loss = 40.11748493928668\nIteration 2198: Loss = 40.11224257370391\nIteration 2199: Loss = 40.107005102476094\nIteration 2200: Loss = 40.101772519347186\nIteration 2201: Loss = 40.09654481806925\nIteration 2202: Loss = 40.09132199240247\nIteration 2203: Loss = 40.08610403611513\nIteration 2204: Loss = 40.080890942983615\nIteration 2205: Loss = 40.07568270679235\nIteration 2206: Loss = 40.07047932133387\nIteration 2207: Loss = 40.06528078040874\nIteration 2208: Loss = 40.0600870778256\nIteration 2209: Loss = 40.054898207401095\nIteration 2210: Loss = 40.04971416295991\nIteration 2211: Loss = 40.04453493833475\nIteration 2212: Loss = 40.03936052736632\nIteration 2213: Loss = 40.03419092390333\nIteration 2214: Loss = 40.029026121802445\nIteration 2215: Loss = 40.02386611492834\nIteration 2216: Loss = 40.01871089715364\nIteration 2217: Loss = 40.01356046235892\nIteration 2218: Loss = 40.00841480443268\nIteration 2219: Loss = 40.00327391727142\nIteration 2220: Loss = 39.99813779477947\nIteration 2221: Loss = 39.99300643086913\nIteration 2222: Loss = 39.987879819460595\nIteration 2223: Loss = 39.98275795448196\nIteration 2224: Loss = 39.97764082986916\nIteration 2225: Loss = 39.97252843956605\nIteration 2226: Loss = 39.96742077752431\nIteration 2227: Loss = 39.962317837703495\nIteration 2228: Loss = 39.957219614070986\nIteration 2229: Loss = 39.95212610060201\nIteration 2230: Loss = 39.94703729127959\nIteration 2231: Loss = 39.94195318009457\nIteration 2232: Loss = 39.93687376104561\nIteration 2233: Loss = 39.931799028139146\nIteration 2234: Loss = 39.92672897538938\nIteration 2235: Loss = 39.92166359681831\nIteration 2236: Loss = 39.916602886455664\nIteration 2237: Loss = 39.91154683833896\nIteration 2238: Loss = 39.90649544651342\nIteration 2239: Loss = 39.90144870503199\nIteration 2240: Loss = 39.89640660795537\nIteration 2241: Loss = 39.89136914935195\nIteration 2242: Loss = 39.88633632329781\nIteration 2243: Loss = 39.88130812387674\nIteration 2244: Loss = 39.87628454518019\nIteration 2245: Loss = 39.8712655813073\nIteration 2246: Loss = 39.86625122636483\nIteration 2247: Loss = 39.86124147446725\nIteration 2248: Loss = 39.856236319736595\nIteration 2249: Loss = 39.8512357563026\nIteration 2250: Loss = 39.846239778302575\nIteration 2251: Loss = 39.84124837988146\nIteration 2252: Loss = 39.836261555191776\nIteration 2253: Loss = 39.83127929839366\nIteration 2254: Loss = 39.826301603654805\nIteration 2255: Loss = 39.821328465150486\nIteration 2256: Loss = 39.81635987706353\nIteration 2257: Loss = 39.811395833584335\nIteration 2258: Loss = 39.80643632891081\nIteration 2259: Loss = 39.80148135724842\nIteration 2260: Loss = 39.79653091281012\nIteration 2261: Loss = 39.79158498981643\nIteration 2262: Loss = 39.78664358249531\nIteration 2263: Loss = 39.781706685082256\nIteration 2264: Loss = 39.776774291820225\nIteration 2265: Loss = 39.771846396959646\nIteration 2266: Loss = 39.766922994758424\nIteration 2267: Loss = 39.762004079481905\nIteration 2268: Loss = 39.75708964540288\nIteration 2269: Loss = 39.75217968680158\nIteration 2270: Loss = 39.74727419796566\nIteration 2271: Loss = 39.74237317319017\nIteration 2272: Loss = 39.737476606777584\nIteration 2273: Loss = 39.73258449303778\nIteration 2274: Loss = 39.72769682628799\nIteration 2275: Loss = 39.72281360085286\nIteration 2276: Loss = 39.717934811064346\nIteration 2277: Loss = 39.713060451261825\nIteration 2278: Loss = 39.70819051579199\nIteration 2279: Loss = 39.70332499900888\nIteration 2280: Loss = 39.69846389527383\nIteration 2281: Loss = 39.69360719895554\nIteration 2282: Loss = 39.68875490443\nIteration 2283: Loss = 39.68390700608048\nIteration 2284: Loss = 39.67906349829758\nIteration 2285: Loss = 39.67422437547914\nIteration 2286: Loss = 39.669389632030295\nIteration 2287: Loss = 39.66455926236345\nIteration 2288: Loss = 39.659733260898214\nIteration 2289: Loss = 39.654911622061505\nIteration 2290: Loss = 39.65009434028743\nIteration 2291: Loss = 39.64528141001734\nIteration 2292: Loss = 39.640472825699774\nIteration 2293: Loss = 39.63566858179051\nIteration 2294: Loss = 39.63086867275251\nIteration 2295: Loss = 39.626073093055915\nIteration 2296: Loss = 39.621281837178024\nIteration 2297: Loss = 39.61649489960336\nIteration 2298: Loss = 39.61171227482357\nIteration 2299: Loss = 39.60693395733743\nIteration 2300: Loss = 39.60215994165089\nIteration 2301: Loss = 39.597390222277006\nIteration 2302: Loss = 39.592624793736\nIteration 2303: Loss = 39.58786365055514\nIteration 2304: Loss = 39.583106787268854\nIteration 2305: Loss = 39.57835419841861\nIteration 2306: Loss = 39.573605878553025\nIteration 2307: Loss = 39.568861822227746\nIteration 2308: Loss = 39.56412202400548\nIteration 2309: Loss = 39.559386478456034\nIteration 2310: Loss = 39.55465518015621\nIteration 2311: Loss = 39.549928123689895\nIteration 2312: Loss = 39.54520530364798\nIteration 2313: Loss = 39.540486714628386\nIteration 2314: Loss = 39.535772351236034\nIteration 2315: Loss = 39.53106220808285\nIteration 2316: Loss = 39.52635627978776\nIteration 2317: Loss = 39.521654560976685\nIteration 2318: Loss = 39.516957046282485\nIteration 2319: Loss = 39.51226373034503\nIteration 2320: Loss = 39.5075746078111\nIteration 2321: Loss = 39.50288967333446\nIteration 2322: Loss = 39.498208921575795\nIteration 2323: Loss = 39.49353234720274\nIteration 2324: Loss = 39.4888599448898\nIteration 2325: Loss = 39.484191709318466\nIteration 2326: Loss = 39.47952763517707\nIteration 2327: Loss = 39.47486771716086\nIteration 2328: Loss = 39.470211949971976\nIteration 2329: Loss = 39.465560328319434\nIteration 2330: Loss = 39.46091284691909\nIteration 2331: Loss = 39.45626950049369\nIteration 2332: Loss = 39.45163028377281\nIteration 2333: Loss = 39.44699519149289\nIteration 2334: Loss = 39.44236421839715\nIteration 2335: Loss = 39.43773735923568\nIteration 2336: Loss = 39.433114608765386\nIteration 2337: Loss = 39.42849596174994\nIteration 2338: Loss = 39.423881412959844\nIteration 2339: Loss = 39.41927095717237\nIteration 2340: Loss = 39.41466458917158\nIteration 2341: Loss = 39.410062303748276\nIteration 2342: Loss = 39.40546409570007\nIteration 2343: Loss = 39.40086995983128\nIteration 2344: Loss = 39.396279890953\nIteration 2345: Loss = 39.391693883883015\nIteration 2346: Loss = 39.38711193344589\nIteration 2347: Loss = 39.382534034472876\nIteration 2348: Loss = 39.377960181801924\nIteration 2349: Loss = 39.37339037027771\nIteration 2350: Loss = 39.36882459475158\nIteration 2351: Loss = 39.36426285008157\nIteration 2352: Loss = 39.359705131132394\nIteration 2353: Loss = 39.35515143277542\nIteration 2354: Loss = 39.350601749888675\nIteration 2355: Loss = 39.346056077356835\nIteration 2356: Loss = 39.34151441007121\nIteration 2357: Loss = 39.33697674292976\nIteration 2358: Loss = 39.33244307083703\nIteration 2359: Loss = 39.32791338870422\nIteration 2360: Loss = 39.32338769144909\nIteration 2361: Loss = 39.31886597399603\nIteration 2362: Loss = 39.31434823127601\nIteration 2363: Loss = 39.30983445822657\nIteration 2364: Loss = 39.30532464979183\nIteration 2365: Loss = 39.300818800922436\nIteration 2366: Loss = 39.29631690657564\nIteration 2367: Loss = 39.29181896171521\nIteration 2368: Loss = 39.287324961311455\nIteration 2369: Loss = 39.282834900341214\nIteration 2370: Loss = 39.278348773787826\nIteration 2371: Loss = 39.27386657664116\nIteration 2372: Loss = 39.2693883038976\nIteration 2373: Loss = 39.264913950559986\nIteration 2374: Loss = 39.26044351163767\nIteration 2375: Loss = 39.25597698214647\nIteration 2376: Loss = 39.251514357108675\nIteration 2377: Loss = 39.24705563155303\nIteration 2378: Loss = 39.24260080051475\nIteration 2379: Loss = 39.238149859035474\nIteration 2380: Loss = 39.233702802163265\nIteration 2381: Loss = 39.22925962495265\nIteration 2382: Loss = 39.22482032246454\nIteration 2383: Loss = 39.22038488976626\nIteration 2384: Loss = 39.21595332193154\nIteration 2385: Loss = 39.21152561404054\nIteration 2386: Loss = 39.20710176117974\nIteration 2387: Loss = 39.20268175844205\nIteration 2388: Loss = 39.19826560092669\nIteration 2389: Loss = 39.1938532837393\nIteration 2390: Loss = 39.18944480199185\nIteration 2391: Loss = 39.18504015080264\nIteration 2392: Loss = 39.18063932529631\nIteration 2393: Loss = 39.176242320603855\nIteration 2394: Loss = 39.17184913186253\nIteration 2395: Loss = 39.167459754215976\nIteration 2396: Loss = 39.163074182814064\nIteration 2397: Loss = 39.158692412813004\nIteration 2398: Loss = 39.15431443937527\nIteration 2399: Loss = 39.149940257669634\nIteration 2400: Loss = 39.14556986287113\nIteration 2401: Loss = 39.141203250161034\nIteration 2402: Loss = 39.1368404147269\nIteration 2403: Loss = 39.13248135176251\nIteration 2404: Loss = 39.12812605646789\nIteration 2405: Loss = 39.1237745240493\nIteration 2406: Loss = 39.11942674971923\nIteration 2407: Loss = 39.11508272869635\nIteration 2408: Loss = 39.11074245620556\nIteration 2409: Loss = 39.106405927477944\nIteration 2410: Loss = 39.10207313775078\nIteration 2411: Loss = 39.097744082267546\nIteration 2412: Loss = 39.09341875627784\nIteration 2413: Loss = 39.0890971550375\nIteration 2414: Loss = 39.08477927380844\nIteration 2415: Loss = 39.080465107858785\nIteration 2416: Loss = 39.07615465246276\nIteration 2417: Loss = 39.07184790290076\nIteration 2418: Loss = 39.06754485445926\nIteration 2419: Loss = 39.063245502430874\nIteration 2420: Loss = 39.05894984211435\nIteration 2421: Loss = 39.05465786881449\nIteration 2422: Loss = 39.050369577842226\nIteration 2423: Loss = 39.04608496451454\nIteration 2424: Loss = 39.04180402415453\nIteration 2425: Loss = 39.037526752091324\nIteration 2426: Loss = 39.033253143660154\nIteration 2427: Loss = 39.02898319420226\nIteration 2428: Loss = 39.02471689906496\nIteration 2429: Loss = 39.02045425360159\nIteration 2430: Loss = 39.01619525317155\nIteration 2431: Loss = 39.0119398931402\nIteration 2432: Loss = 39.00768816887897\nIteration 2433: Loss = 39.00344007576527\nIteration 2434: Loss = 38.999195609182514\nIteration 2435: Loss = 38.99495476452011\nIteration 2436: Loss = 38.99071753717345\nIteration 2437: Loss = 38.9864839225439\nIteration 2438: Loss = 38.98225391603878\nIteration 2439: Loss = 38.97802751307138\nIteration 2440: Loss = 38.97380470906095\nIteration 2441: Loss = 38.96958549943267\nIteration 2442: Loss = 38.96536987961766\nIteration 2443: Loss = 38.96115784505299\nIteration 2444: Loss = 38.956949391181595\nIteration 2445: Loss = 38.9527445134524\nIteration 2446: Loss = 38.94854320732017\nIteration 2447: Loss = 38.94434546824561\nIteration 2448: Loss = 38.94015129169528\nIteration 2449: Loss = 38.93596067314166\nIteration 2450: Loss = 38.93177360806306\nIteration 2451: Loss = 38.92759009194371\nIteration 2452: Loss = 38.923410120273665\nIteration 2453: Loss = 38.91923368854884\nIteration 2454: Loss = 38.91506079227099\nIteration 2455: Loss = 38.91089142694772\nIteration 2456: Loss = 38.90672558809245\nIteration 2457: Loss = 38.90256327122442\nIteration 2458: Loss = 38.89840447186871\nIteration 2459: Loss = 38.89424918555618\nIteration 2460: Loss = 38.89009740782348\nIteration 2461: Loss = 38.88594913421309\nIteration 2462: Loss = 38.88180436027325\nIteration 2463: Loss = 38.877663081557984\nIteration 2464: Loss = 38.87352529362707\nIteration 2465: Loss = 38.86939099204605\nIteration 2466: Loss = 38.86526017238625\nIteration 2467: Loss = 38.8611328302247\nIteration 2468: Loss = 38.857008961144196\nIteration 2469: Loss = 38.85288856073328\nIteration 2470: Loss = 38.848771624586156\nIteration 2471: Loss = 38.84465814830281\nIteration 2472: Loss = 38.84054812748891\nIteration 2473: Loss = 38.836441557755826\nIteration 2474: Loss = 38.832338434720626\nIteration 2475: Loss = 38.82823875400608\nIteration 2476: Loss = 38.824142511240595\nIteration 2477: Loss = 38.82004970205829\nIteration 2478: Loss = 38.815960322098945\nIteration 2479: Loss = 38.81187436700797\nIteration 2480: Loss = 38.807791832436465\nIteration 2481: Loss = 38.80371271404114\nIteration 2482: Loss = 38.799637007484336\nIteration 2483: Loss = 38.79556470843407\nIteration 2484: Loss = 38.79149581256393\nIteration 2485: Loss = 38.787430315553145\nIteration 2486: Loss = 38.78336821308653\nIteration 2487: Loss = 38.779309500854524\nIteration 2488: Loss = 38.77525417455313\nIteration 2489: Loss = 38.77120222988396\nIteration 2490: Loss = 38.76715366255419\nIteration 2491: Loss = 38.76310846827656\nIteration 2492: Loss = 38.75906664276942\nIteration 2493: Loss = 38.75502818175658\nIteration 2494: Loss = 38.75099308096749\nIteration 2495: Loss = 38.74696133613709\nIteration 2496: Loss = 38.74293294300588\nIteration 2497: Loss = 38.7389078973199\nIteration 2498: Loss = 38.734886194830636\nIteration 2499: Loss = 38.730867831295186\nIteration 2500: Loss = 38.726852802476074\nIteration 2501: Loss = 38.72284110414135\nIteration 2502: Loss = 38.718832732064584\nIteration 2503: Loss = 38.714827682024776\nIteration 2504: Loss = 38.710825949806434\nIteration 2505: Loss = 38.70682753119955\nIteration 2506: Loss = 38.70283242199954\nIteration 2507: Loss = 38.6988406180073\nIteration 2508: Loss = 38.69485211502918\nIteration 2509: Loss = 38.69086690887694\nIteration 2510: Loss = 38.68688499536781\nIteration 2511: Loss = 38.682906370324424\nIteration 2512: Loss = 38.67893102957485\nIteration 2513: Loss = 38.674958968952545\nIteration 2514: Loss = 38.670990184296414\nIteration 2515: Loss = 38.66702467145072\nIteration 2516: Loss = 38.66306242626513\nIteration 2517: Loss = 38.65910344459472\nIteration 2518: Loss = 38.6551477222999\nIteration 2519: Loss = 38.65119525524649\nIteration 2520: Loss = 38.647246039305664\nIteration 2521: Loss = 38.643300070353924\nIteration 2522: Loss = 38.63935734427316\nIteration 2523: Loss = 38.63541785695058\nIteration 2524: Loss = 38.631481604278754\nIteration 2525: Loss = 38.62754858215556\nIteration 2526: Loss = 38.6236187864842\nIteration 2527: Loss = 38.6196922131732\nIteration 2528: Loss = 38.615768858136384\nIteration 2529: Loss = 38.61184871729289\nIteration 2530: Loss = 38.60793178656712\nIteration 2531: Loss = 38.60401806188882\nIteration 2532: Loss = 38.600107539192976\nIteration 2533: Loss = 38.596200214419845\nIteration 2534: Loss = 38.59229608351497\nIteration 2535: Loss = 38.588395142429135\nIteration 2536: Loss = 38.584497387118404\nIteration 2537: Loss = 38.58060281354407\nIteration 2538: Loss = 38.576711417672655\nIteration 2539: Loss = 38.572823195475955\nIteration 2540: Loss = 38.56893814293094\nIteration 2541: Loss = 38.56505625601984\nIteration 2542: Loss = 38.56117753073006\nIteration 2543: Loss = 38.55730196305426\nIteration 2544: Loss = 38.55342954899026\nIteration 2545: Loss = 38.54956028454107\nIteration 2546: Loss = 38.54569416571493\nIteration 2547: Loss = 38.54183118852521\nIteration 2548: Loss = 38.53797134899047\nIteration 2549: Loss = 38.53411464313444\nIteration 2550: Loss = 38.53026106698601\nIteration 2551: Loss = 38.526410616579206\nIteration 2552: Loss = 38.52256328795322\nIteration 2553: Loss = 38.518719077152376\nIteration 2554: Loss = 38.514877980226125\nIteration 2555: Loss = 38.511039993229026\nIteration 2556: Loss = 38.5072051122208\nIteration 2557: Loss = 38.50337333326626\nIteration 2558: Loss = 38.4995446524353\nIteration 2559: Loss = 38.49571906580295\nIteration 2560: Loss = 38.49189656944931\nIteration 2561: Loss = 38.488077159459564\nIteration 2562: Loss = 38.484260831923976\nIteration 2563: Loss = 38.48044758293793\nIteration 2564: Loss = 38.476637408601775\nIteration 2565: Loss = 38.47283030502101\nIteration 2566: Loss = 38.46902626830615\nIteration 2567: Loss = 38.46522529457277\nIteration 2568: Loss = 38.461427379941455\nIteration 2569: Loss = 38.45763252053786\nIteration 2570: Loss = 38.453840712492635\nIteration 2571: Loss = 38.45005195194148\nIteration 2572: Loss = 38.446266235025064\nIteration 2573: Loss = 38.442483557889126\nIteration 2574: Loss = 38.43870391668435\nIteration 2575: Loss = 38.43492730756643\nIteration 2576: Loss = 38.43115372669606\nIteration 2577: Loss = 38.42738317023891\nIteration 2578: Loss = 38.42361563436561\nIteration 2579: Loss = 38.41985111525178\nIteration 2580: Loss = 38.41608960907797\nIteration 2581: Loss = 38.412331112029726\nIteration 2582: Loss = 38.408575620297505\nIteration 2583: Loss = 38.40482313007673\nIteration 2584: Loss = 38.40107363756775\nIteration 2585: Loss = 38.39732713897585\nIteration 2586: Loss = 38.39358363051122\nIteration 2587: Loss = 38.389843108388995\nIteration 2588: Loss = 38.38610556882919\nIteration 2589: Loss = 38.382371008056744\nIteration 2590: Loss = 38.3786394223015\nIteration 2591: Loss = 38.37491080779815\nIteration 2592: Loss = 38.37118516078633\nIteration 2593: Loss = 38.367462477510514\nIteration 2594: Loss = 38.363742754220056\nIteration 2595: Loss = 38.36002598716918\nIteration 2596: Loss = 38.35631217261695\nIteration 2597: Loss = 38.352601306827324\nIteration 2598: Loss = 38.348893386069086\nIteration 2599: Loss = 38.34518840661584\nIteration 2600: Loss = 38.34148636474604\nIteration 2601: Loss = 38.337787256742985\nIteration 2602: Loss = 38.33409107889476\nIteration 2603: Loss = 38.3303978274943\nIteration 2604: Loss = 38.32670749883933\nIteration 2605: Loss = 38.32302008923238\nIteration 2606: Loss = 38.31933559498079\nIteration 2607: Loss = 38.31565401239665\nIteration 2608: Loss = 38.31197533779688\nIteration 2609: Loss = 38.30829956750317\nIteration 2610: Loss = 38.30462669784196\nIteration 2611: Loss = 38.30095672514446\nIteration 2612: Loss = 38.29728964574666\nIteration 2613: Loss = 38.29362545598929\nIteration 2614: Loss = 38.28996415221782\nIteration 2615: Loss = 38.28630573078246\nIteration 2616: Loss = 38.282650188038176\nIteration 2617: Loss = 38.278997520344646\nIteration 2618: Loss = 38.27534772406626\nIteration 2619: Loss = 38.27170079557216\nIteration 2620: Loss = 38.26805673123615\nIteration 2621: Loss = 38.264415527436775\nIteration 2622: Loss = 38.26077718055727\nIteration 2623: Loss = 38.25714168698554\nIteration 2624: Loss = 38.25350904311421\nIteration 2625: Loss = 38.24987924534056\nIteration 2626: Loss = 38.24625229006655\nIteration 2627: Loss = 38.242628173698805\nIteration 2628: Loss = 38.239006892648625\n",
      "Iteration 2629: Loss = 38.23538844333194\nIteration 2630: Loss = 38.231772822169354\nIteration 2631: Loss = 38.2281600255861\nIteration 2632: Loss = 38.22455005001205\nIteration 2633: Loss = 38.22094289188171\nIteration 2634: Loss = 38.21733854763421\nIteration 2635: Loss = 38.21373701371331\nIteration 2636: Loss = 38.21013828656736\nIteration 2637: Loss = 38.206542362649316\nIteration 2638: Loss = 38.20294923841677\nIteration 2639: Loss = 38.19935891033189\nIteration 2640: Loss = 38.19577137486141\nIteration 2641: Loss = 38.19218662847668\nIteration 2642: Loss = 38.18860466765362\nIteration 2643: Loss = 38.18502548887269\nIteration 2644: Loss = 38.181449088618976\nIteration 2645: Loss = 38.17787546338207\nIteration 2646: Loss = 38.17430460965613\nIteration 2647: Loss = 38.17073652393989\nIteration 2648: Loss = 38.167171202736576\nIteration 2649: Loss = 38.163608642554\nIteration 2650: Loss = 38.160048839904476\nIteration 2651: Loss = 38.15649179130484\nIteration 2652: Loss = 38.152937493276454\nIteration 2653: Loss = 38.14938594234519\nIteration 2654: Loss = 38.14583713504143\nIteration 2655: Loss = 38.14229106790007\nIteration 2656: Loss = 38.13874773746046\nIteration 2657: Loss = 38.13520714026647\nIteration 2658: Loss = 38.13166927286646\nIteration 2659: Loss = 38.12813413181324\nIteration 2660: Loss = 38.12460171366411\nIteration 2661: Loss = 38.12107201498082\nIteration 2662: Loss = 38.1175450323296\nIteration 2663: Loss = 38.11402076228114\nIteration 2664: Loss = 38.11049920141054\nIteration 2665: Loss = 38.10698034629737\nIteration 2666: Loss = 38.10346419352564\nIteration 2667: Loss = 38.099950739683756\nIteration 2668: Loss = 38.096439981364625\nIteration 2669: Loss = 38.09293191516547\nIteration 2670: Loss = 38.08942653768801\nIteration 2671: Loss = 38.085923845538346\nIteration 2672: Loss = 38.08242383532696\nIteration 2673: Loss = 38.07892650366877\nIteration 2674: Loss = 38.07543184718306\nIteration 2675: Loss = 38.07193986249348\nIteration 2676: Loss = 38.06845054622809\nIteration 2677: Loss = 38.06496389501934\nIteration 2678: Loss = 38.06147990550401\nIteration 2679: Loss = 38.057998574323236\nIteration 2680: Loss = 38.05451989812255\nIteration 2681: Loss = 38.05104387355181\nIteration 2682: Loss = 38.04757049726523\nIteration 2683: Loss = 38.044099765921345\nIteration 2684: Loss = 38.04063167618306\nIteration 2685: Loss = 38.03716622471756\nIteration 2686: Loss = 38.03370340819639\nIteration 2687: Loss = 38.03024322329539\nIteration 2688: Loss = 38.02678566669475\nIteration 2689: Loss = 38.0233307350789\nIteration 2690: Loss = 38.01987842513664\nIteration 2691: Loss = 38.01642873356102\nIteration 2692: Loss = 38.01298165704938\nIteration 2693: Loss = 38.00953719230337\nIteration 2694: Loss = 38.006095336028906\nIteration 2695: Loss = 38.00265608493617\nIteration 2696: Loss = 37.9992194357396\nIteration 2697: Loss = 37.99578538515793\nIteration 2698: Loss = 37.992353929914124\nIteration 2699: Loss = 37.9889250667354\nIteration 2700: Loss = 37.98549879235322\nIteration 2701: Loss = 37.982075103503306\nIteration 2702: Loss = 37.97865399692558\nIteration 2703: Loss = 37.9752354693642\nIteration 2704: Loss = 37.97181951756758\nIteration 2705: Loss = 37.9684061382883\nIteration 2706: Loss = 37.964995328283194\nIteration 2707: Loss = 37.961587084313294\nIteration 2708: Loss = 37.958181403143804\nIteration 2709: Loss = 37.95477828154417\nIteration 2710: Loss = 37.951377716287986\nIteration 2711: Loss = 37.94797970415305\nIteration 2712: Loss = 37.94458424192133\nIteration 2713: Loss = 37.941191326378984\nIteration 2714: Loss = 37.93780095431633\nIteration 2715: Loss = 37.93441312252784\nIteration 2716: Loss = 37.93102782781216\nIteration 2717: Loss = 37.92764506697206\nIteration 2718: Loss = 37.924264836814494\nIteration 2719: Loss = 37.92088713415053\nIteration 2720: Loss = 37.91751195579538\nIteration 2721: Loss = 37.914139298568394\nIteration 2722: Loss = 37.910769159293025\nIteration 2723: Loss = 37.90740153479687\nIteration 2724: Loss = 37.904036421911634\nIteration 2725: Loss = 37.90067381747312\nIteration 2726: Loss = 37.89731371832125\nIteration 2727: Loss = 37.89395612130003\nIteration 2728: Loss = 37.890601023257574\nIteration 2729: Loss = 37.887248421046074\nIteration 2730: Loss = 37.88389831152179\nIteration 2731: Loss = 37.880550691545096\nIteration 2732: Loss = 37.877205557980425\nIteration 2733: Loss = 37.873862907696235\nIteration 2734: Loss = 37.870522737565096\nIteration 2735: Loss = 37.867185044463625\nIteration 2736: Loss = 37.86384982527248\nIteration 2737: Loss = 37.86051707687636\nIteration 2738: Loss = 37.857186796164015\nIteration 2739: Loss = 37.853858980028214\nIteration 2740: Loss = 37.85053362536578\nIteration 2741: Loss = 37.84721072907753\nIteration 2742: Loss = 37.84389028806834\nIteration 2743: Loss = 37.84057229924706\nIteration 2744: Loss = 37.83725675952658\nIteration 2745: Loss = 37.83394366582377\nIteration 2746: Loss = 37.83063301505951\nIteration 2747: Loss = 37.82732480415868\nIteration 2748: Loss = 37.82401903005013\nIteration 2749: Loss = 37.8207156896667\nIteration 2750: Loss = 37.81741477994522\nIteration 2751: Loss = 37.81411629782647\nIteration 2752: Loss = 37.810820240255225\nIteration 2753: Loss = 37.8075266041802\nIteration 2754: Loss = 37.804235386554076\nIteration 2755: Loss = 37.80094658433347\nIteration 2756: Loss = 37.79766019447898\nIteration 2757: Loss = 37.79437621395511\nIteration 2758: Loss = 37.791094639730325\nIteration 2759: Loss = 37.787815468777005\nIteration 2760: Loss = 37.78453869807147\nIteration 2761: Loss = 37.78126432459395\nIteration 2762: Loss = 37.7779923453286\nIteration 2763: Loss = 37.774722757263476\nIteration 2764: Loss = 37.771455557390546\nIteration 2765: Loss = 37.76819074270569\nIteration 2766: Loss = 37.76492831020866\nIteration 2767: Loss = 37.76166825690311\nIteration 2768: Loss = 37.75841057979658\nIteration 2769: Loss = 37.755155275900506\nIteration 2770: Loss = 37.751902342230174\nIteration 2771: Loss = 37.74865177580475\nIteration 2772: Loss = 37.745403573647266\nIteration 2773: Loss = 37.74215773278462\nIteration 2774: Loss = 37.738914250247554\nIteration 2775: Loss = 37.735673123070676\nIteration 2776: Loss = 37.73243434829242\nIteration 2777: Loss = 37.72919792295508\nIteration 2778: Loss = 37.725963844104776\nIteration 2779: Loss = 37.72273210879145\nIteration 2780: Loss = 37.71950271406889\nIteration 2781: Loss = 37.71627565699468\nIteration 2782: Loss = 37.71305093463025\nIteration 2783: Loss = 37.7098285440408\nIteration 2784: Loss = 37.706608482295394\nIteration 2785: Loss = 37.70339074646683\nIteration 2786: Loss = 37.70017533363175\nIteration 2787: Loss = 37.69696224087055\nIteration 2788: Loss = 37.693751465267454\nIteration 2789: Loss = 37.690543003910435\nIteration 2790: Loss = 37.687336853891246\nIteration 2791: Loss = 37.68413301230544\nIteration 2792: Loss = 37.6809314762523\nIteration 2793: Loss = 37.677732242834885\nIteration 2794: Loss = 37.67453530916001\nIteration 2795: Loss = 37.671340672338225\nIteration 2796: Loss = 37.66814832948386\nIteration 2797: Loss = 37.66495827771499\nIteration 2798: Loss = 37.66177051415337\nIteration 2799: Loss = 37.65858503592454\nIteration 2800: Loss = 37.65540184015775\nIteration 2801: Loss = 37.652220923985986\nIteration 2802: Loss = 37.649042284545914\nIteration 2803: Loss = 37.64586591897796\nIteration 2804: Loss = 37.64269182442624\nIteration 2805: Loss = 37.63951999803855\nIteration 2806: Loss = 37.63635043696642\nIteration 2807: Loss = 37.63318313836504\nIteration 2808: Loss = 37.630018099393325\nIteration 2809: Loss = 37.626855317213845\nIteration 2810: Loss = 37.623694788992864\nIteration 2811: Loss = 37.62053651190032\nIteration 2812: Loss = 37.617380483109805\nIteration 2813: Loss = 37.6142266997986\nIteration 2814: Loss = 37.61107515914763\nIteration 2815: Loss = 37.607925858341474\nIteration 2816: Loss = 37.60477879456837\nIteration 2817: Loss = 37.60163396502019\nIteration 2818: Loss = 37.59849136689247\nIteration 2819: Loss = 37.595350997384365\nIteration 2820: Loss = 37.592212853698655\nIteration 2821: Loss = 37.58907693304177\nIteration 2822: Loss = 37.58594323262373\nIteration 2823: Loss = 37.58281174965821\nIteration 2824: Loss = 37.57968248136247\nIteration 2825: Loss = 37.576555424957384\nIteration 2826: Loss = 37.57343057766744\nIteration 2827: Loss = 37.570307936720724\nIteration 2828: Loss = 37.567187499348904\nIteration 2829: Loss = 37.56406926278723\nIteration 2830: Loss = 37.560953224274556\nIteration 2831: Loss = 37.55783938105332\nIteration 2832: Loss = 37.55472773036952\nIteration 2833: Loss = 37.55161826947273\nIteration 2834: Loss = 37.54851099561609\nIteration 2835: Loss = 37.54540590605631\nIteration 2836: Loss = 37.542302998053636\nIteration 2837: Loss = 37.539202268871875\nIteration 2838: Loss = 37.53610371577842\nIteration 2839: Loss = 37.533007336044115\nIteration 2840: Loss = 37.52991312694345\nIteration 2841: Loss = 37.526821085754385\nIteration 2842: Loss = 37.52373120975842\nIteration 2843: Loss = 37.52064349624058\nIteration 2844: Loss = 37.517557942489425\nIteration 2845: Loss = 37.51447454579701\nIteration 2846: Loss = 37.5113933034589\nIteration 2847: Loss = 37.50831421277421\nIteration 2848: Loss = 37.50523727104549\nIteration 2849: Loss = 37.50216247557883\nIteration 2850: Loss = 37.4990898236838\nIteration 2851: Loss = 37.49601931267348\nIteration 2852: Loss = 37.4929509398644\nIteration 2853: Loss = 37.489884702576575\nIteration 2854: Loss = 37.48682059813352\nIteration 2855: Loss = 37.4837586238622\nIteration 2856: Loss = 37.480698777093046\nIteration 2857: Loss = 37.47764105515996\nIteration 2858: Loss = 37.474585455400295\nIteration 2859: Loss = 37.47153197515485\nIteration 2860: Loss = 37.46848061176788\nIteration 2861: Loss = 37.465431362587076\nIteration 2862: Loss = 37.46238422496357\nIteration 2863: Loss = 37.45933919625194\nIteration 2864: Loss = 37.45629627381018\nIteration 2865: Loss = 37.45325545499973\nIteration 2866: Loss = 37.4502167371854\nIteration 2867: Loss = 37.44718011773548\nIteration 2868: Loss = 37.44414559402163\nIteration 2869: Loss = 37.44111316341893\nIteration 2870: Loss = 37.43808282330586\nIteration 2871: Loss = 37.43505457106433\nIteration 2872: Loss = 37.43202840407957\nIteration 2873: Loss = 37.42900431974029\nIteration 2874: Loss = 37.4259823154385\nIteration 2875: Loss = 37.42296238856969\nIteration 2876: Loss = 37.419944536532604\nIteration 2877: Loss = 37.41692875672947\nIteration 2878: Loss = 37.413915046565826\nIteration 2879: Loss = 37.410903403450575\nIteration 2880: Loss = 37.40789382479599\nIteration 2881: Loss = 37.40488630801772\nIteration 2882: Loss = 37.40188085053473\nIteration 2883: Loss = 37.39887744976934\nIteration 2884: Loss = 37.395876103147216\nIteration 2885: Loss = 37.39287680809736\nIteration 2886: Loss = 37.389879562052116\nIteration 2887: Loss = 37.38688436244714\nIteration 2888: Loss = 37.38389120672144\nIteration 2889: Loss = 37.380900092317304\nIteration 2890: Loss = 37.377911016680365\nIteration 2891: Loss = 37.374923977259556\nIteration 2892: Loss = 37.37193897150713\nIteration 2893: Loss = 37.368955996878626\nIteration 2894: Loss = 37.365975050832894\nIteration 2895: Loss = 37.36299613083206\nIteration 2896: Loss = 37.36001923434158\nIteration 2897: Loss = 37.35704435883015\nIteration 2898: Loss = 37.35407150176977\nIteration 2899: Loss = 37.35110066063572\nIteration 2900: Loss = 37.34813183290654\nIteration 2901: Loss = 37.34516501606406\nIteration 2902: Loss = 37.342200207593336\nIteration 2903: Loss = 37.33923740498272\nIteration 2904: Loss = 37.33627660572382\nIteration 2905: Loss = 37.33331780731147\nIteration 2906: Loss = 37.33036100724377\nIteration 2907: Loss = 37.32740620302207\nIteration 2908: Loss = 37.32445339215093\nIteration 2909: Loss = 37.32150257213818\nIteration 2910: Loss = 37.31855374049484\nIteration 2911: Loss = 37.31560689473521\nIteration 2912: Loss = 37.31266203237677\nIteration 2913: Loss = 37.30971915094022\nIteration 2914: Loss = 37.3067782479495\nIteration 2915: Loss = 37.303839320931736\nIteration 2916: Loss = 37.30090236741727\nIteration 2917: Loss = 37.29796738493966\nIteration 2918: Loss = 37.29503437103561\nIteration 2919: Loss = 37.29210332324509\nIteration 2920: Loss = 37.28917423911119\nIteration 2921: Loss = 37.286247116180235\nIteration 2922: Loss = 37.28332195200171\nIteration 2923: Loss = 37.28039874412826\nIteration 2924: Loss = 37.277477490115736\nIteration 2925: Loss = 37.27455818752315\nIteration 2926: Loss = 37.27164083391265\nIteration 2927: Loss = 37.26872542684957\nIteration 2928: Loss = 37.26581196390241\nIteration 2929: Loss = 37.262900442642774\nIteration 2930: Loss = 37.25999086064548\nIteration 2931: Loss = 37.25708321548843\nIteration 2932: Loss = 37.25417750475272\nIteration 2933: Loss = 37.25127372602252\nIteration 2934: Loss = 37.24837187688518\nIteration 2935: Loss = 37.24547195493115\nIteration 2936: Loss = 37.24257395775403\nIteration 2937: Loss = 37.239677882950524\nIteration 2938: Loss = 37.23678372812044\nIteration 2939: Loss = 37.233891490866725\nIteration 2940: Loss = 37.231001168795395\nIteration 2941: Loss = 37.228112759515604\nIteration 2942: Loss = 37.22522626063958\nIteration 2943: Loss = 37.22234166978268\nIteration 2944: Loss = 37.219458984563296\nIteration 2945: Loss = 37.21657820260298\nIteration 2946: Loss = 37.2136993215263\nIteration 2947: Loss = 37.210822338960945\nIteration 2948: Loss = 37.20794725253765\nIteration 2949: Loss = 37.20507405989025\nIteration 2950: Loss = 37.202202758655616\nIteration 2951: Loss = 37.19933334647372\nIteration 2952: Loss = 37.196465820987555\nIteration 2953: Loss = 37.193600179843195\nIteration 2954: Loss = 37.19073642068976\nIteration 2955: Loss = 37.18787454117939\nIteration 2956: Loss = 37.18501453896733\nIteration 2957: Loss = 37.18215641171179\nIteration 2958: Loss = 37.17930015707406\nIteration 2959: Loss = 37.176445772718466\nIteration 2960: Loss = 37.17359325631232\nIteration 2961: Loss = 37.17074260552601\nIteration 2962: Loss = 37.16789381803292\nIteration 2963: Loss = 37.165046891509434\nIteration 2964: Loss = 37.16220182363496\nIteration 2965: Loss = 37.159358612091914\nIteration 2966: Loss = 37.15651725456574\nIteration 2967: Loss = 37.15367774874484\nIteration 2968: Loss = 37.15084009232062\nIteration 2969: Loss = 37.14800428298752\nIteration 2970: Loss = 37.145170318442915\nIteration 2971: Loss = 37.14233819638721\nIteration 2972: Loss = 37.13950791452374\nIteration 2973: Loss = 37.13667947055886\nIteration 2974: Loss = 37.13385286220188\nIteration 2975: Loss = 37.13102808716508\nIteration 2976: Loss = 37.128205143163704\nIteration 2977: Loss = 37.12538402791596\nIteration 2978: Loss = 37.122564739143016\nIteration 2979: Loss = 37.119747274568994\nIteration 2980: Loss = 37.116931631920956\nIteration 2981: Loss = 37.11411780892892\nIteration 2982: Loss = 37.11130580332584\nIteration 2983: Loss = 37.108495612847605\nIteration 2984: Loss = 37.105687235233056\nIteration 2985: Loss = 37.10288066822393\nIteration 2986: Loss = 37.100075909564936\nIteration 2987: Loss = 37.097272957003675\nIteration 2988: Loss = 37.09447180829067\nIteration 2989: Loss = 37.09167246117938\nIteration 2990: Loss = 37.088874913426146\nIteration 2991: Loss = 37.086079162790234\nIteration 2992: Loss = 37.0832852070338\nIteration 2993: Loss = 37.08049304392193\nIteration 2994: Loss = 37.07770267122258\nIteration 2995: Loss = 37.07491408670662\nIteration 2996: Loss = 37.072127288147755\nIteration 2997: Loss = 37.06934227332266\nIteration 2998: Loss = 37.06655904001083\nIteration 2999: Loss = 37.06377758599466\nIteration 3000: Loss = 37.060997909059395\nIteration 3001: Loss = 37.058220006993196\nIteration 3002: Loss = 37.05544387758703\nIteration 3003: Loss = 37.0526695186348\nIteration 3004: Loss = 37.049896927933204\nIteration 3005: Loss = 37.04712610328183\nIteration 3006: Loss = 37.04435704248311\nIteration 3007: Loss = 37.04158974334229\nIteration 3008: Loss = 37.038824203667545\nIteration 3009: Loss = 37.0360604212698\nIteration 3010: Loss = 37.033298393962866\nIteration 3011: Loss = 37.030538119563374\nIteration 3012: Loss = 37.02777959589078\nIteration 3013: Loss = 37.02502282076738\nIteration 3014: Loss = 37.02226779201828\nIteration 3015: Loss = 37.01951450747142\nIteration 3016: Loss = 37.01676296495753\nIteration 3017: Loss = 37.01401316231016\nIteration 3018: Loss = 37.01126509736569\nIteration 3019: Loss = 37.00851876796326\nIteration 3020: Loss = 37.00577417194487\nIteration 3021: Loss = 37.00303130715526\nIteration 3022: Loss = 37.000290171442\nIteration 3023: Loss = 36.997550762655415\nIteration 3024: Loss = 36.99481307864865\nIteration 3025: Loss = 36.992077117277624\nIteration 3026: Loss = 36.98934287640102\nIteration 3027: Loss = 36.9866103538803\nIteration 3028: Loss = 36.9838795475797\nIteration 3029: Loss = 36.98115045536624\nIteration 3030: Loss = 36.97842307510968\nIteration 3031: Loss = 36.97569740468255\nIteration 3032: Loss = 36.97297344196014\nIteration 3033: Loss = 36.970251184820484\nIteration 3034: Loss = 36.96753063114437\nIteration 3035: Loss = 36.964811778815346\nIteration 3036: Loss = 36.96209462571969\nIteration 3037: Loss = 36.95937916974641\nIteration 3038: Loss = 36.956665408787266\nIteration 3039: Loss = 36.95395334073674\nIteration 3040: Loss = 36.95124296349206\nIteration 3041: Loss = 36.94853427495313\nIteration 3042: Loss = 36.94582727302264\nIteration 3043: Loss = 36.943121955605946\nIteration 3044: Loss = 36.94041832061116\nIteration 3045: Loss = 36.93771636594907\nIteration 3046: Loss = 36.93501608953319\nIteration 3047: Loss = 36.93231748927974\nIteration 3048: Loss = 36.929620563107626\nIteration 3049: Loss = 36.92692530893845\nIteration 3050: Loss = 36.92423172469653\nIteration 3051: Loss = 36.92153980830886\nIteration 3052: Loss = 36.9188495577051\nIteration 3053: Loss = 36.916160970817636\nIteration 3054: Loss = 36.913474045581495\nIteration 3055: Loss = 36.910788779934386\nIteration 3056: Loss = 36.90810517181671\nIteration 3057: Loss = 36.90542321917152\nIteration 3058: Loss = 36.90274291994454\nIteration 3059: Loss = 36.90006427208416\nIteration 3060: Loss = 36.8973872735414\nIteration 3061: Loss = 36.894711922269984\nIteration 3062: Loss = 36.89203821622626\nIteration 3063: Loss = 36.88936615336921\nIteration 3064: Loss = 36.88669573166048\nIteration 3065: Loss = 36.884026949064356\nIteration 3066: Loss = 36.88135980354776\nIteration 3067: Loss = 36.87869429308025\nIteration 3068: Loss = 36.876030415634\nIteration 3069: Loss = 36.87336816918384\nIteration 3070: Loss = 36.870707551707184\nIteration 3071: Loss = 36.86804856118411\nIteration 3072: Loss = 36.86539119559729\nIteration 3073: Loss = 36.86273545293201\nIteration 3074: Loss = 36.86008133117616\nIteration 3075: Loss = 36.85742882832027\nIteration 3076: Loss = 36.85477794235742\nIteration 3077: Loss = 36.85212867128335\nIteration 3078: Loss = 36.84948101309635\nIteration 3079: Loss = 36.84683496579732\nIteration 3080: Loss = 36.844190527389756\nIteration 3081: Loss = 36.841547695879726\nIteration 3082: Loss = 36.83890646927591\nIteration 3083: Loss = 36.83626684558954\nIteration 3084: Loss = 36.83362882283443\nIteration 3085: Loss = 36.83099239902697\nIteration 3086: Loss = 36.82835757218613\nIteration 3087: Loss = 36.82572434033343\nIteration 3088: Loss = 36.82309270149295\nIteration 3089: Loss = 36.82046265369137\nIteration 3090: Loss = 36.81783419495789\nIteration 3091: Loss = 36.81520732332425\nIteration 3092: Loss = 36.81258203682478\nIteration 3093: Loss = 36.80995833349633\nIteration 3094: Loss = 36.807336211378306\nIteration 3095: Loss = 36.80471566851266\nIteration 3096: Loss = 36.80209670294385\nIteration 3097: Loss = 36.799479312718894\nIteration 3098: Loss = 36.79686349588734\nIteration 3099: Loss = 36.79424925050125\nIteration 3100: Loss = 36.79163657461524\nIteration 3101: Loss = 36.78902546628639\nIteration 3102: Loss = 36.78641592357434\nIteration 3103: Loss = 36.78380794454125\nIteration 3104: Loss = 36.78120152725176\nIteration 3105: Loss = 36.778596669773044\nIteration 3106: Loss = 36.77599337017475\nIteration 3107: Loss = 36.773391626529055\nIteration 3108: Loss = 36.770791436910635\nIteration 3109: Loss = 36.76819279939663\nIteration 3110: Loss = 36.7655957120667\nIteration 3111: Loss = 36.76300017300298\nIteration 3112: Loss = 36.76040618029009\nIteration 3113: Loss = 36.757813732015144\nIteration 3114: Loss = 36.75522282626771\nIteration 3115: Loss = 36.75263346113986\nIteration 3116: Loss = 36.75004563472611\nIteration 3117: Loss = 36.74745934512346\nIteration 3118: Loss = 36.74487459043138\nIteration 3119: Loss = 36.74229136875178\nIteration 3120: Loss = 36.739709678189065\nIteration 3121: Loss = 36.73712951685005\nIteration 3122: Loss = 36.73455088284404\nIteration 3123: Loss = 36.73197377428277\nIteration 3124: Loss = 36.72939818928044\nIteration 3125: Loss = 36.72682412595368\nIteration 3126: Loss = 36.72425158242154\nIteration 3127: Loss = 36.72168055680553\nIteration 3128: Loss = 36.7191110472296\nIteration 3129: Loss = 36.716543051820125\nIteration 3130: Loss = 36.7139765687059\nIteration 3131: Loss = 36.711411596018145\nIteration 3132: Loss = 36.7088481318905\nIteration 3133: Loss = 36.70628617445902\nIteration 3134: Loss = 36.70372572186217\nIteration 3135: Loss = 36.70116677224087\nIteration 3136: Loss = 36.698609323738395\nIteration 3137: Loss = 36.696053374500444\nIteration 3138: Loss = 36.69349892267511\nIteration 3139: Loss = 36.69094596641292\nIteration 3140: Loss = 36.68839450386674\nIteration 3141: Loss = 36.68584453319188\nIteration 3142: Loss = 36.68329605254601\nIteration 3143: Loss = 36.6807490600892\nIteration 3144: Loss = 36.67820355398389\nIteration 3145: Loss = 36.67565953239492\nIteration 3146: Loss = 36.67311699348947\nIteration 3147: Loss = 36.670575935437164\nIteration 3148: Loss = 36.668036356409914\nIteration 3149: Loss = 36.66549825458207\nIteration 3150: Loss = 36.66296162813028\nIteration 3151: Loss = 36.66042647523361\nIteration 3152: Loss = 36.65789279407348\nIteration 3153: Loss = 36.65536058283363\nIteration 3154: Loss = 36.65282983970017\nIteration 3155: Loss = 36.65030056286157\nIteration 3156: Loss = 36.64777275050864\nIteration 3157: Loss = 36.64524640083452\nIteration 3158: Loss = 36.64272151203473\nIteration 3159: Loss = 36.640198082307066\nIteration 3160: Loss = 36.63767610985172\nIteration 3161: Loss = 36.63515559287118\nIteration 3162: Loss = 36.63263652957026\nIteration 3163: Loss = 36.63011891815611\nIteration 3164: Loss = 36.62760275683821\nIteration 3165: Loss = 36.62508804382835\nIteration 3166: Loss = 36.622574777340624\nIteration 3167: Loss = 36.62006295559147\nIteration 3168: Loss = 36.61755257679961\nIteration 3169: Loss = 36.61504363918607\nIteration 3170: Loss = 36.6125361409742\nIteration 3171: Loss = 36.61003008038966\nIteration 3172: Loss = 36.607525455660365\nIteration 3173: Loss = 36.60502226501657\nIteration 3174: Loss = 36.602520506690794\nIteration 3175: Loss = 36.60002017891785\nIteration 3176: Loss = 36.597521279934845\nIteration 3177: Loss = 36.59502380798118\nIteration 3178: Loss = 36.59252776129849\nIteration 3179: Loss = 36.59003313813074\nIteration 3180: Loss = 36.58753993672416\nIteration 3181: Loss = 36.58504815532723\nIteration 3182: Loss = 36.58255779219071\nIteration 3183: Loss = 36.580068845567624\nIteration 3184: Loss = 36.57758131371325\nIteration 3185: Loss = 36.57509519488515\nIteration 3186: Loss = 36.57261048734312\nIteration 3187: Loss = 36.57012718934922\nIteration 3188: Loss = 36.567645299167765\nIteration 3189: Loss = 36.56516481506529\nIteration 3190: Loss = 36.562685735310616\nIteration 3191: Loss = 36.56020805817478\nIteration 3192: Loss = 36.557731781931075\nIteration 3193: Loss = 36.55525690485501\nIteration 3194: Loss = 36.55278342522433\nIteration 3195: Loss = 36.55031134131904\nIteration 3196: Loss = 36.54784065142132\nIteration 3197: Loss = 36.54537135381564\nIteration 3198: Loss = 36.542903446788635\nIteration 3199: Loss = 36.54043692862918\nIteration 3200: Loss = 36.53797179762838\nIteration 3201: Loss = 36.535508052079535\nIteration 3202: Loss = 36.533045690278165\nIteration 3203: Loss = 36.53058471052197\nIteration 3204: Loss = 36.52812511111091\nIteration 3205: Loss = 36.525666890347104\nIteration 3206: Loss = 36.523210046534864\nIteration 3207: Loss = 36.52075457798074\nIteration 3208: Loss = 36.51830048299343\nIteration 3209: Loss = 36.515847759883854\nIteration 3210: Loss = 36.51339640696509\nIteration 3211: Loss = 36.510946422552436\nIteration 3212: Loss = 36.50849780496335\nIteration 3213: Loss = 36.50605055251746\nIteration 3214: Loss = 36.50360466353659\nIteration 3215: Loss = 36.50116013634474\nIteration 3216: Loss = 36.49871696926805\nIteration 3217: Loss = 36.49627516063486\nIteration 3218: Loss = 36.493834708775665\nIteration 3219: Loss = 36.49139561202311\nIteration 3220: Loss = 36.48895786871202\nIteration 3221: Loss = 36.486521477179366\nIteration 3222: Loss = 36.48408643576426\nIteration 3223: Loss = 36.48165274280798\nIteration 3224: Loss = 36.47922039665394\nIteration 3225: Loss = 36.476789395647735\nIteration 3226: Loss = 36.47435973813705\nIteration 3227: Loss = 36.471931422471755\nIteration 3228: Loss = 36.469504447003814\nIteration 3229: Loss = 36.46707881008735\nIteration 3230: Loss = 36.46465451007864\nIteration 3231: Loss = 36.462231545336024\nIteration 3232: Loss = 36.45980991422007\nIteration 3233: Loss = 36.45738961509333\nIteration 3234: Loss = 36.45497064632062\nIteration 3235: Loss = 36.45255300626876\nIteration 3236: Loss = 36.45013669330677\nIteration 3237: Loss = 36.44772170580571\nIteration 3238: Loss = 36.4453080421388\nIteration 3239: Loss = 36.44289570068136\nIteration 3240: Loss = 36.44048467981079\nIteration 3241: Loss = 36.438074977906595\nIteration 3242: Loss = 36.43566659335041\nIteration 3243: Loss = 36.43325952452592\nIteration 3244: Loss = 36.430853769818945\nIteration 3245: Loss = 36.428449327617365\nIteration 3246: Loss = 36.42604619631117\nIteration 3247: Loss = 36.423644374292415\nIteration 3248: Loss = 36.42124385995524\nIteration 3249: Loss = 36.418844651695885\nIteration 3250: Loss = 36.41644674791264\nIteration 3251: Loss = 36.414050147005874\nIteration 3252: Loss = 36.411654847378045\nIteration 3253: Loss = 36.40926084743368\nIteration 3254: Loss = 36.40686814557934\nIteration 3255: Loss = 36.40447674022367\nIteration 3256: Loss = 36.40208662977738\nIteration 3257: Loss = 36.39969781265324\nIteration 3258: Loss = 36.39731028726605\nIteration 3259: Loss = 36.39492405203269\nIteration 3260: Loss = 36.39253910537209\nIteration 3261: Loss = 36.3901554457052\nIteration 3262: Loss = 36.38777307145503\nIteration 3263: Loss = 36.385391981046645\nIteration 3264: Loss = 36.38301217290714\nIteration 3265: Loss = 36.38063364546563\nIteration 3266: Loss = 36.37825639715328\nIteration 3267: Loss = 36.3758804264033\nIteration 3268: Loss = 36.37350573165089\nIteration 3269: Loss = 36.37113231133331\nIteration 3270: Loss = 36.36876016388984\nIteration 3271: Loss = 36.36638928776175\nIteration 3272: Loss = 36.36401968139237\nIteration 3273: Loss = 36.36165134322702\nIteration 3274: Loss = 36.35928427171303\nIteration 3275: Loss = 36.35691846529977\nIteration 3276: Loss = 36.35455392243859\nIteration 3277: Loss = 36.35219064158283\nIteration 3278: Loss = 36.349828621187875\nIteration 3279: Loss = 36.34746785971108\nIteration 3280: Loss = 36.34510835561182\nIteration 3281: Loss = 36.342750107351435\nIteration 3282: Loss = 36.34039311339328\nIteration 3283: Loss = 36.338037372202685\nIteration 3284: Loss = 36.335682882246985\nIteration 3285: Loss = 36.333329641995476\nIteration 3286: Loss = 36.33097764991946\nIteration 3287: Loss = 36.3286269044922\nIteration 3288: Loss = 36.326277404188936\nIteration 3289: Loss = 36.3239291474869\nIteration 3290: Loss = 36.32158213286527\nIteration 3291: Loss = 36.319236358805206\nIteration 3292: Loss = 36.31689182378984\nIteration 3293: Loss = 36.314548526304264\nIteration 3294: Loss = 36.31220646483553\nIteration 3295: Loss = 36.30986563787265\nIteration 3296: Loss = 36.30752604390657\nIteration 3297: Loss = 36.305187681430226\nIteration 3298: Loss = 36.30285054893848\nIteration 3299: Loss = 36.30051464492816\nIteration 3300: Loss = 36.298179967898015\nIteration 3301: Loss = 36.29584651634877\nIteration 3302: Loss = 36.29351428878307\nIteration 3303: Loss = 36.29118328370549\nIteration 3304: Loss = 36.28885349962257\nIteration 3305: Loss = 36.286524935042756\nIteration 3306: Loss = 36.28419758847643\nIteration 3307: Loss = 36.28187145843593\nIteration 3308: Loss = 36.279546543435465\nIteration 3309: Loss = 36.27722284199123\nIteration 3310: Loss = 36.27490035262129\nIteration 3311: Loss = 36.272579073845655\nIteration 3312: Loss = 36.27025900418626\nIteration 3313: Loss = 36.2679401421669\nIteration 3314: Loss = 36.26562248631336\nIteration 3315: Loss = 36.26330603515328\nIteration 3316: Loss = 36.260990787216194\nIteration 3317: Loss = 36.2586767410336\nIteration 3318: Loss = 36.25636389513883\nIteration 3319: Loss = 36.25405224806716\nIteration 3320: Loss = 36.25174179835575\nIteration 3321: Loss = 36.24943254454363\nIteration 3322: Loss = 36.247124485171774\nIteration 3323: Loss = 36.24481761878299\nIteration 3324: Loss = 36.242511943922004\nIteration 3325: Loss = 36.2402074591354\nIteration 3326: Loss = 36.23790416297167\nIteration 3327: Loss = 36.23560205398118\nIteration 3328: Loss = 36.23330113071616\nIteration 3329: Loss = 36.23100139173072\nIteration 3330: Loss = 36.22870283558085\nIteration 3331: Loss = 36.22640546082439\nIteration 3332: Loss = 36.22410926602106\nIteration 3333: Loss = 36.22181424973245\nIteration 3334: Loss = 36.21952041052199\nIteration 3335: Loss = 36.217227746955\nIteration 3336: Loss = 36.21493625759863\nIteration 3337: Loss = 36.2126459410219\nIteration 3338: Loss = 36.21035679579566\nIteration 3339: Loss = 36.20806882049265\nIteration 3340: Loss = 36.205782013687426\nIteration 3341: Loss = 36.2034963739564\nIteration 3342: Loss = 36.201211899877805\nIteration 3343: Loss = 36.198928590031755\nIteration 3344: Loss = 36.19664644300017\nIteration 3345: Loss = 36.19436545736682\nIteration 3346: Loss = 36.1920856317173\nIteration 3347: Loss = 36.18980696463903\nIteration 3348: Loss = 36.187529454721286\nIteration 3349: Loss = 36.18525310055512\nIteration 3350: Loss = 36.182977900733476\nIteration 3351: Loss = 36.18070385385106\nIteration 3352: Loss = 36.17843095850442\nIteration 3353: Loss = 36.17615921329192\nIteration 3354: Loss = 36.173888616813734\nIteration 3355: Loss = 36.17161916767186\nIteration 3356: Loss = 36.16935086447008\nIteration 3357: Loss = 36.167083705813994\nIteration 3358: Loss = 36.16481769031104\nIteration 3359: Loss = 36.1625528165704\nIteration 3360: Loss = 36.16028908320311\nIteration 3361: Loss = 36.15802648882197\nIteration 3362: Loss = 36.155765032041565\nIteration 3363: Loss = 36.15350471147832\nIteration 3364: Loss = 36.15124552575041\nIteration 3365: Loss = 36.14898747347781\nIteration 3366: Loss = 36.14673055328228\nIteration 3367: Loss = 36.14447476378738\nIteration 3368: Loss = 36.142220103618435\nIteration 3369: Loss = 36.139966571402525\nIteration 3370: Loss = 36.13771416576856\nIteration 3371: Loss = 36.13546288534719\nIteration 3372: Loss = 36.13321272877083\nIteration 3373: Loss = 36.13096369467372\nIteration 3374: Loss = 36.12871578169177\nIteration 3375: Loss = 36.12646898846275\nIteration 3376: Loss = 36.12422331362616\nIteration 3377: Loss = 36.121978755823214\nIteration 3378: Loss = 36.119735313696964\nIteration 3379: Loss = 36.11749298589218\nIteration 3380: Loss = 36.11525177105536\nIteration 3381: Loss = 36.11301166783478\nIteration 3382: Loss = 36.110772674880494\nIteration 3383: Loss = 36.108534790844246\nIteration 3384: Loss = 36.106298014379576\nIteration 3385: Loss = 36.10406234414172\nIteration 3386: Loss = 36.10182777878767\nIteration 3387: Loss = 36.099594316976194\nIteration 3388: Loss = 36.097361957367724\nIteration 3389: Loss = 36.09513069862451\nIteration 3390: Loss = 36.09290053941046\nIteration 3391: Loss = 36.09067147839122\nIteration 3392: Loss = 36.08844351423423\nIteration 3393: Loss = 36.08621664560857\nIteration 3394: Loss = 36.083990871185094\nIteration 3395: Loss = 36.08176618963636\nIteration 3396: Loss = 36.07954259963663\nIteration 3397: Loss = 36.077320099861915\nIteration 3398: Loss = 36.075098688989904\nIteration 3399: Loss = 36.07287836570002\nIteration 3400: Loss = 36.07065912867339\nIteration 3401: Loss = 36.068440976592825\nIteration 3402: Loss = 36.06622390814289\nIteration 3403: Loss = 36.06400792200981\nIteration 3404: Loss = 36.06179301688151\nIteration 3405: Loss = 36.059579191447625\nIteration 3406: Loss = 36.05736644439951\nIteration 3407: Loss = 36.055154774430164\nIteration 3408: Loss = 36.05294418023433\nIteration 3409: Loss = 36.05073466050837\nIteration 3410: Loss = 36.0485262139504\nIteration 3411: Loss = 36.0463188392602\nIteration 3412: Loss = 36.044112535139206\nIteration 3413: Loss = 36.04190730029058\nIteration 3414: Loss = 36.039703133419124\nIteration 3415: Loss = 36.03750003323132\nIteration 3416: Loss = 36.035297998435354\nIteration 3417: Loss = 36.033097027741036\nIteration 3418: Loss = 36.03089711985989\nIteration 3419: Loss = 36.02869827350509\nIteration 3420: Loss = 36.02650048739146\nIteration 3421: Loss = 36.02430376023552\nIteration 3422: Loss = 36.022108090755395\nIteration 3423: Loss = 36.01991347767093\nIteration 3424: Loss = 36.01771991970361\nIteration 3425: Loss = 36.01552741557654\nIteration 3426: Loss = 36.01333596401452\nIteration 3427: Loss = 36.01114556374396\nIteration 3428: Loss = 36.008956213492965\nIteration 3429: Loss = 36.00676791199124\nIteration 3430: Loss = 36.00458065797015\nIteration 3431: Loss = 36.002394450162726\nIteration 3432: Loss = 36.00020928730359\nIteration 3433: Loss = 35.99802516812905\nIteration 3434: Loss = 35.99584209137702\nIteration 3435: Loss = 35.993660055787046\nIteration 3436: Loss = 35.99147906010031\nIteration 3437: Loss = 35.989299103059636\nIteration 3438: Loss = 35.98712018340945\nIteration 3439: Loss = 35.984942299895835\nIteration 3440: Loss = 35.98276545126644\nIteration 3441: Loss = 35.98058963627061\nIteration 3442: Loss = 35.978414853659224\nIteration 3443: Loss = 35.97624110218488\nIteration 3444: Loss = 35.97406838060168\nIteration 3445: Loss = 35.9718966876654\nIteration 3446: Loss = 35.969726022133436\nIteration 3447: Loss = 35.96755638276474\nIteration 3448: Loss = 35.965387768319914\nIteration 3449: Loss = 35.963220177561155\nIteration 3450: Loss = 35.96105360925224\nIteration 3451: Loss = 35.95888806215857\nIteration 3452: Loss = 35.95672353504711\nIteration 3453: Loss = 35.95456002668649\nIteration 3454: Loss = 35.95239753584684\nIteration 3455: Loss = 35.95023606129995\nIteration 3456: Loss = 35.94807560181916\nIteration 3457: Loss = 35.94591615617943\nIteration 3458: Loss = 35.943757723157276\nIteration 3459: Loss = 35.941600301530805\nIteration 3460: Loss = 35.93944389007971\nIteration 3461: Loss = 35.93728848758526\nIteration 3462: Loss = 35.93513409283031\nIteration 3463: Loss = 35.93298070459927\nIteration 3464: Loss = 35.93082832167813\nIteration 3465: Loss = 35.928676942854445\nIteration 3466: Loss = 35.92652656691736\nIteration 3467: Loss = 35.92437719265757\nIteration 3468: Loss = 35.92222881886732\nIteration 3469: Loss = 35.92008144434046\nIteration 3470: Loss = 35.91793506787235\nIteration 3471: Loss = 35.91578968825995\nIteration 3472: Loss = 35.91364530430174\nIteration 3473: Loss = 35.911501914797775\nIteration 3474: Loss = 35.909359518549664\nIteration 3475: Loss = 35.90721811436056\nIteration 3476: Loss = 35.90507770103516\nIteration 3477: Loss = 35.90293827737972\nIteration 3478: Loss = 35.90079984220203\nIteration 3479: Loss = 35.89866239431141\nIteration 3480: Loss = 35.89652593251875\nIteration 3481: Loss = 35.894390455636454\nIteration 3482: Loss = 35.89225596247847\nIteration 3483: Loss = 35.89012245186029\nIteration 3484: Loss = 35.88798992259891\nIteration 3485: Loss = 35.885858373512896\nIteration 3486: Loss = 35.8837278034223\nIteration 3487: Loss = 35.881598211148734\nIteration 3488: Loss = 35.8794695955153\nIteration 3489: Loss = 35.87734195534667\nIteration 3490: Loss = 35.87521528946898\nIteration 3491: Loss = 35.87308959670993\nIteration 3492: Loss = 35.87096487589872\nIteration 3493: Loss = 35.868841125866055\nIteration 3494: Loss = 35.86671834544416\nIteration 3495: Loss = 35.86459653346678\nIteration 3496: Loss = 35.862475688769145\nIteration 3497: Loss = 35.860355810188025\nIteration 3498: Loss = 35.858236896561635\nIteration 3499: Loss = 35.85611894672978\nIteration 3500: Loss = 35.8540019595337\nIteration 3501: Loss = 35.85188593381613\nIteration 3502: Loss = 35.84977086842134\nIteration 3503: Loss = 35.84765676219508\nIteration 3504: Loss = 35.84554361398459\nIteration 3505: Loss = 35.843431422638595\nIteration 3506: Loss = 35.84132018700731\nIteration 3507: Loss = 35.83920990594246\nIteration 3508: Loss = 35.8371005782972\nIteration 3509: Loss = 35.83499220292625\nIteration 3510: Loss = 35.83288477868573\nIteration 3511: Loss = 35.8307783044333\nIteration 3512: Loss = 35.82867277902806\nIteration 3513: Loss = 35.826568201330595\nIteration 3514: Loss = 35.82446457020296\nIteration 3515: Loss = 35.822361884508716\nIteration 3516: Loss = 35.820260143112826\nIteration 3517: Loss = 35.818159344881785\nIteration 3518: Loss = 35.81605948868353\nIteration 3519: Loss = 35.81396057338744\nIteration 3520: Loss = 35.81186259786439\nIteration 3521: Loss = 35.80976556098671\nIteration 3522: Loss = 35.807669461628166\nIteration 3523: Loss = 35.805574298664\nIteration 3524: Loss = 35.80348007097089\nIteration 3525: Loss = 35.801386777427\nIteration 3526: Loss = 35.799294416911906\nIteration 3527: Loss = 35.79720298830666\nIteration 3528: Loss = 35.79511249049375\nIteration 3529: Loss = 35.7930229223571\nIteration 3530: Loss = 35.79093428278211\nIteration 3531: Loss = 35.78884657065557\nIteration 3532: Loss = 35.78675978486576\nIteration 3533: Loss = 35.78467392430235\nIteration 3534: Loss = 35.78258898785651\nIteration 3535: Loss = 35.78050497442078\nIteration 3536: Loss = 35.778421882889155\nIteration 3537: Loss = 35.77633971215706\nIteration 3538: Loss = 35.77425846112137\nIteration 3539: Loss = 35.77217812868034\nIteration 3540: Loss = 35.770098713733695\nIteration 3541: Loss = 35.768020215182545\nIteration 3542: Loss = 35.76594263192947\nIteration 3543: Loss = 35.76386596287842\nIteration 3544: Loss = 35.761790206934755\nIteration 3545: Loss = 35.75971536300531\nIteration 3546: Loss = 35.757641429998294\nIteration 3547: Loss = 35.75556840682333\nIteration 3548: Loss = 35.75349629239144\nIteration 3549: Loss = 35.75142508561507\nIteration 3550: Loss = 35.749354785408094\nIteration 3551: Loss = 35.74728539068574\nIteration 3552: Loss = 35.745216900364674\nIteration 3553: Loss = 35.74314931336295\nIteration 3554: Loss = 35.74108262860002\nIteration 3555: Loss = 35.73901684499675\nIteration 3556: Loss = 35.736951961475356\nIteration 3557: Loss = 35.734887976959506\nIteration 3558: Loss = 35.73282489037424\nIteration 3559: Loss = 35.730762700645954\nIteration 3560: Loss = 35.72870140670247\nIteration 3561: Loss = 35.726641007473\nIteration 3562: Loss = 35.72458150188808\nIteration 3563: Loss = 35.72252288887973\nIteration 3564: Loss = 35.72046516738125\nIteration 3565: Loss = 35.71840833632738\nIteration 3566: Loss = 35.71635239465422\nIteration 3567: Loss = 35.71429734129924\nIteration 3568: Loss = 35.712243175201294\nIteration 3569: Loss = 35.71018989530061\nIteration 3570: Loss = 35.70813750053875\nIteration 3571: Loss = 35.70608598985871\nIteration 3572: Loss = 35.704035362204785\nIteration 3573: Loss = 35.701985616522684\nIteration 3574: Loss = 35.699936751759445\nIteration 3575: Loss = 35.69788876686349\nIteration 3576: Loss = 35.6958416607846\nIteration 3577: Loss = 35.69379543247389\nIteration 3578: Loss = 35.69175008088386\nIteration 3579: Loss = 35.689705604968346\nIteration 3580: Loss = 35.68766200368253\nIteration 3581: Loss = 35.68561927598298\nIteration 3582: Loss = 35.68357742082757\nIteration 3583: Loss = 35.68153643717554\nIteration 3584: Loss = 35.679496323987486\nIteration 3585: Loss = 35.67745708022532\nIteration 3586: Loss = 35.67541870485233\nIteration 3587: Loss = 35.67338119683312\nIteration 3588: Loss = 35.671344555133636\nIteration 3589: Loss = 35.66930877872116\nIteration 3590: Loss = 35.66727386656433\nIteration 3591: Loss = 35.665239817633065\nIteration 3592: Loss = 35.66320663089868\nIteration 3593: Loss = 35.66117430533377\nIteration 3594: Loss = 35.659142839912306\nIteration 3595: Loss = 35.657112233609524\nIteration 3596: Loss = 35.65508248540204\nIteration 3597: Loss = 35.65305359426774\nIteration 3598: Loss = 35.6510255591859\nIteration 3599: Loss = 35.64899837913706\nIteration 3600: Loss = 35.64697205310308\nIteration 3601: Loss = 35.64494658006716\nIteration 3602: Loss = 35.6429219590138\nIteration 3603: Loss = 35.64089818892883\nIteration 3604: Loss = 35.63887526879936\nIteration 3605: Loss = 35.63685319761384\nIteration 3606: Loss = 35.634831974361994\nIteration 3607: Loss = 35.632811598034905\nIteration 3608: Loss = 35.63079206762489\nIteration 3609: Loss = 35.62877338212563\nIteration 3610: Loss = 35.62675554053206\nIteration 3611: Loss = 35.62473854184046\nIteration 3612: Loss = 35.622722385048355\nIteration 3613: Loss = 35.62070706915462\nIteration 3614: Loss = 35.61869259315937\nIteration 3615: Loss = 35.616678956064064\nIteration 3616: Loss = 35.61466615687141\nIteration 3617: Loss = 35.61265419458543\nIteration 3618: Loss = 35.61064306821141\nIteration 3619: Loss = 35.60863277675596\nIteration 3620: Loss = 35.60662331922692\nIteration 3621: Loss = 35.604614694633476\nIteration 3622: Loss = 35.602606901986036\nIteration 3623: Loss = 35.600599940296306\nIteration 3624: Loss = 35.5985938085773\nIteration 3625: Loss = 35.59658850584325\nIteration 3626: Loss = 35.594584031109726\nIteration 3627: Loss = 35.59258038339351\nIteration 3628: Loss = 35.59057756171271\n",
      "Iteration 3629: Loss = 35.58857556508665\nIteration 3630: Loss = 35.586574392535965\nIteration 3631: Loss = 35.58457404308252\nIteration 3632: Loss = 35.58257451574948\nIteration 3633: Loss = 35.58057580956124\nIteration 3634: Loss = 35.57857792354348\nIteration 3635: Loss = 35.576580856723105\nIteration 3636: Loss = 35.57458460812832\nIteration 3637: Loss = 35.57258917678856\nIteration 3638: Loss = 35.57059456173454\nIteration 3639: Loss = 35.56860076199816\nIteration 3640: Loss = 35.56660777661268\nIteration 3641: Loss = 35.5646156046125\nIteration 3642: Loss = 35.562624245033334\nIteration 3643: Loss = 35.56063369691212\nIteration 3644: Loss = 35.558643959287046\nIteration 3645: Loss = 35.55665503119754\nIteration 3646: Loss = 35.55466691168428\nIteration 3647: Loss = 35.552679599789165\nIteration 3648: Loss = 35.55069309455534\nIteration 3649: Loss = 35.5487073950272\nIteration 3650: Loss = 35.546722500250354\nIteration 3651: Loss = 35.544738409271666\nIteration 3652: Loss = 35.5427551211392\nIteration 3653: Loss = 35.54077263490229\nIteration 3654: Loss = 35.538790949611446\nIteration 3655: Loss = 35.53681006431847\nIteration 3656: Loss = 35.534829978076345\nIteration 3657: Loss = 35.53285068993928\nIteration 3658: Loss = 35.53087219896271\nIteration 3659: Loss = 35.52889450420331\nIteration 3660: Loss = 35.526917604718946\nIteration 3661: Loss = 35.524941499568726\nIteration 3662: Loss = 35.52296618781294\nIteration 3663: Loss = 35.520991668513126\nIteration 3664: Loss = 35.51901794073203\nIteration 3665: Loss = 35.517045003533575\nIteration 3666: Loss = 35.515072855982936\nIteration 3667: Loss = 35.51310149714648\nIteration 3668: Loss = 35.51113092609177\nIteration 3669: Loss = 35.50916114188758\nIteration 3670: Loss = 35.507192143603895\nIteration 3671: Loss = 35.50522393031189\nIteration 3672: Loss = 35.50325650108396\nIteration 3673: Loss = 35.501289854993665\nIteration 3674: Loss = 35.49932399111579\nIteration 3675: Loss = 35.4973589085263\nIteration 3676: Loss = 35.495394606302376\nIteration 3677: Loss = 35.493431083522346\nIteration 3678: Loss = 35.49146833926578\nIteration 3679: Loss = 35.489506372613405\nIteration 3680: Loss = 35.487545182647146\nIteration 3681: Loss = 35.48558476845012\nIteration 3682: Loss = 35.48362512910661\nIteration 3683: Loss = 35.481666263702095\nIteration 3684: Loss = 35.47970817132324\nIteration 3685: Loss = 35.47775085105788\nIteration 3686: Loss = 35.475794301995016\nIteration 3687: Loss = 35.47383852322488\nIteration 3688: Loss = 35.4718835138388\nIteration 3689: Loss = 35.469929272929335\nIteration 3690: Loss = 35.46797579959021\nIteration 3691: Loss = 35.46602309291628\nIteration 3692: Loss = 35.464071152003626\nIteration 3693: Loss = 35.46211997594946\nIteration 3694: Loss = 35.46016956385218\nIteration 3695: Loss = 35.45821991481132\nIteration 3696: Loss = 35.45627102792761\nIteration 3697: Loss = 35.45432290230292\nIteration 3698: Loss = 35.4523755370403\nIteration 3699: Loss = 35.45042893124391\nIteration 3700: Loss = 35.44848308401915\nIteration 3701: Loss = 35.44653799447249\nIteration 3702: Loss = 35.444593661711615\nIteration 3703: Loss = 35.44265008484534\nIteration 3704: Loss = 35.440707262983615\nIteration 3705: Loss = 35.438765195237565\nIteration 3706: Loss = 35.43682388071945\nIteration 3707: Loss = 35.43488331854269\nIteration 3708: Loss = 35.43294350782183\nIteration 3709: Loss = 35.43100444767257\nIteration 3710: Loss = 35.42906613721174\nIteration 3711: Loss = 35.427128575557354\nIteration 3712: Loss = 35.42519176182849\nIteration 3713: Loss = 35.42325569514545\nIteration 3714: Loss = 35.4213203746296\nIteration 3715: Loss = 35.419385799403464\nIteration 3716: Loss = 35.417451968590726\nIteration 3717: Loss = 35.41551888131618\nIteration 3718: Loss = 35.41358653670572\nIteration 3719: Loss = 35.411654933886425\nIteration 3720: Loss = 35.409724071986474\nIteration 3721: Loss = 35.407793950135165\nIteration 3722: Loss = 35.40586456746295\nIteration 3723: Loss = 35.40393592310134\nIteration 3724: Loss = 35.40200801618305\nIteration 3725: Loss = 35.40008084584186\nIteration 3726: Loss = 35.39815441121269\nIteration 3727: Loss = 35.39622871143156\nIteration 3728: Loss = 35.39430374563563\nIteration 3729: Loss = 35.392379512963146\nIteration 3730: Loss = 35.39045601255351\nIteration 3731: Loss = 35.388533243547165\nIteration 3732: Loss = 35.38661120508574\nIteration 3733: Loss = 35.38468989631192\nIteration 3734: Loss = 35.38276931636953\nIteration 3735: Loss = 35.38084946440348\nIteration 3736: Loss = 35.378930339559794\nIteration 3737: Loss = 35.37701194098559\nIteration 3738: Loss = 35.37509426782909\nIteration 3739: Loss = 35.373177319239616\nIteration 3740: Loss = 35.3712610943676\nIteration 3741: Loss = 35.36934559236456\nIteration 3742: Loss = 35.36743081238311\nIteration 3743: Loss = 35.36551675357696\nIteration 3744: Loss = 35.36360341510091\nIteration 3745: Loss = 35.36169079611086\nIteration 3746: Loss = 35.35977889576378\nIteration 3747: Loss = 35.35786771321776\nIteration 3748: Loss = 35.35595724763194\nIteration 3749: Loss = 35.35404749816657\nIteration 3750: Loss = 35.35213846398299\nIteration 3751: Loss = 35.350230144243596\nIteration 3752: Loss = 35.3483225381119\nIteration 3753: Loss = 35.34641564475247\nIteration 3754: Loss = 35.34450946333094\nIteration 3755: Loss = 35.34260399301405\nIteration 3756: Loss = 35.340699232969605\nIteration 3757: Loss = 35.338795182366496\nIteration 3758: Loss = 35.33689184037466\nIteration 3759: Loss = 35.334989206165126\nIteration 3760: Loss = 35.33308727890999\nIteration 3761: Loss = 35.33118605778241\nIteration 3762: Loss = 35.32928554195662\nIteration 3763: Loss = 35.32738573060792\nIteration 3764: Loss = 35.32548662291268\nIteration 3765: Loss = 35.323588218048286\nIteration 3766: Loss = 35.32169051519326\nIteration 3767: Loss = 35.31979351352715\nIteration 3768: Loss = 35.31789721223054\nIteration 3769: Loss = 35.31600161048512\nIteration 3770: Loss = 35.31410670747359\nIteration 3771: Loss = 35.312212502379744\nIteration 3772: Loss = 35.31031899438839\nIteration 3773: Loss = 35.30842618268542\nIteration 3774: Loss = 35.30653406645777\nIteration 3775: Loss = 35.30464264489341\nIteration 3776: Loss = 35.302751917181396\nIteration 3777: Loss = 35.30086188251179\nIteration 3778: Loss = 35.298972540075695\nIteration 3779: Loss = 35.297083889065306\nIteration 3780: Loss = 35.29519592867382\nIteration 3781: Loss = 35.2933086580955\nIteration 3782: Loss = 35.29142207652562\nIteration 3783: Loss = 35.28953618316053\nIteration 3784: Loss = 35.28765097719757\nIteration 3785: Loss = 35.285766457835166\nIteration 3786: Loss = 35.283882624272756\nIteration 3787: Loss = 35.28199947571081\nIteration 3788: Loss = 35.280117011350804\nIteration 3789: Loss = 35.27823523039532\nIteration 3790: Loss = 35.27635413204788\nIteration 3791: Loss = 35.2744737155131\nIteration 3792: Loss = 35.272593979996586\nIteration 3793: Loss = 35.270714924705\nIteration 3794: Loss = 35.26883654884598\nIteration 3795: Loss = 35.26695885162823\nIteration 3796: Loss = 35.265081832261465\nIteration 3797: Loss = 35.26320548995643\nIteration 3798: Loss = 35.26132982392485\nIteration 3799: Loss = 35.259454833379486\nIteration 3800: Loss = 35.25758051753416\nIteration 3801: Loss = 35.25570687560363\nIteration 3802: Loss = 35.253833906803735\nIteration 3803: Loss = 35.251961610351294\nIteration 3804: Loss = 35.25008998546413\nIteration 3805: Loss = 35.2482190313611\nIteration 3806: Loss = 35.24634874726205\nIteration 3807: Loss = 35.24447913238782\nIteration 3808: Loss = 35.24261018596031\nIteration 3809: Loss = 35.24074190720237\nIteration 3810: Loss = 35.23887429533788\nIteration 3811: Loss = 35.23700734959169\nIteration 3812: Loss = 35.2351410691897\nIteration 3813: Loss = 35.233275453358786\nIteration 3814: Loss = 35.231410501326785\nIteration 3815: Loss = 35.2295462123226\nIteration 3816: Loss = 35.22768258557607\nIteration 3817: Loss = 35.22581962031806\nIteration 3818: Loss = 35.22395731578042\nIteration 3819: Loss = 35.22209567119598\nIteration 3820: Loss = 35.22023468579858\nIteration 3821: Loss = 35.218374358823034\nIteration 3822: Loss = 35.216514689505146\nIteration 3823: Loss = 35.21465567708171\nIteration 3824: Loss = 35.2127973207905\nIteration 3825: Loss = 35.21093961987028\nIteration 3826: Loss = 35.2090825735608\nIteration 3827: Loss = 35.207226181102754\nIteration 3828: Loss = 35.20537044173788\nIteration 3829: Loss = 35.203515354708834\nIteration 3830: Loss = 35.20166091925928\nIteration 3831: Loss = 35.19980713463388\nIteration 3832: Loss = 35.19795400007821\nIteration 3833: Loss = 35.196101514838865\nIteration 3834: Loss = 35.194249678163395\nIteration 3835: Loss = 35.19239848930034\nIteration 3836: Loss = 35.19054794749918\nIteration 3837: Loss = 35.18869805201037\nIteration 3838: Loss = 35.18684880208538\nIteration 3839: Loss = 35.18500019697656\nIteration 3840: Loss = 35.18315223593732\nIteration 3841: Loss = 35.181304918221926\nIteration 3842: Loss = 35.17945824308571\nIteration 3843: Loss = 35.17761220978491\nIteration 3844: Loss = 35.17576681757672\nIteration 3845: Loss = 35.173922065719324\nIteration 3846: Loss = 35.172077953471835",
      "\nIteration 3847: Loss = 35.17023448009433\nIteration 3848: Loss = 35.16839164484784\nIteration 3849: Loss = 35.166549446994345\nIteration 3850: Loss = 35.1647078857968\nIteration 3851: Loss = 35.16286696051908\nIteration 3852: Loss = 35.16102667042604\nIteration 3853: Loss = 35.159187014783456\nIteration 3854: Loss = 35.157347992858064\nIteration 3855: Loss = 35.15550960391754\nIteration 3856: Loss = 35.15367184723051\nIteration 3857: Loss = 35.15183472206655\nIteration 3858: Loss = 35.14999822769617\nIteration 3859: Loss = 35.14816236339082\nIteration 3860: Loss = 35.14632712842289\nIteration 3861: Loss = 35.14449252206571\nIteration 3862: Loss = 35.14265854359354\nIteration 3863: Loss = 35.14082519228161\nIteration 3864: Loss = 35.13899246740604\nIteration 3865: Loss = 35.1371603682439\nIteration 3866: Loss = 35.13532889407321\nIteration 3867: Loss = 35.13349804417288\nIteration 3868: Loss = 35.1316678178228\nIteration 3869: Loss = 35.12983821430376\nIteration 3870: Loss = 35.1280092328975\nIteration 3871: Loss = 35.12618087288663\nIteration 3872: Loss = 35.12435313355475\nIteration 3873: Loss = 35.12252601418636\nIteration 3874: Loss = 35.12069951406688\nIteration 3875: Loss = 35.11887363248266\nIteration 3876: Loss = 35.11704836872094\nIteration 3877: Loss = 35.11522372206993\nIteration 3878: Loss = 35.113399691818714\nIteration 3879: Loss = 35.11157627725733\nIteration 3880: Loss = 35.10975347767669\nIteration 3881: Loss = 35.10793129236866\nIteration 3882: Loss = 35.10610972062599\nIteration 3883: Loss = 35.10428876174236\nIteration 3884: Loss = 35.10246841501235\nIteration 3885: Loss = 35.10064867973147\nIteration 3886: Loss = 35.098829555196104\nIteration 3887: Loss = 35.09701104070357\nIteration 3888: Loss = 35.09519313555211\nIteration 3889: Loss = 35.0933758390408\nIteration 3890: Loss = 35.0915591504697\nIteration 3891: Loss = 35.08974306913972\nIteration 3892: Loss = 35.08792759435272\nIteration 3893: Loss = 35.0861127254114\nIteration 3894: Loss = 35.0842984616194\nIteration 3895: Loss = 35.08248480228124\nIteration 3896: Loss = 35.08067174670237\nIteration 3897: Loss = 35.07885929418909\nIteration 3898: Loss = 35.07704744404862\nIteration 3899: Loss = 35.07523619558906\nIteration 3900: Loss = 35.07342554811943\nIteration 3901: Loss = 35.07161550094962\nIteration 3902: Loss = 35.06980605339039\nIteration 3903: Loss = 35.067997204753425\nIteration 3904: Loss = 35.06618895435129\nIteration 3905: Loss = 35.06438130149742\nIteration 3906: Loss = 35.062574245506134\nIteration 3907: Loss = 35.06076778569268\nIteration 3908: Loss = 35.05896192137313\nIteration 3909: Loss = 35.05715665186447\nIteration 3910: Loss = 35.05535197648455\nIteration 3911: Loss = 35.053547894552125\nIteration 3912: Loss = 35.05174440538679\nIteration 3913: Loss = 35.04994150830908\nIteration 3914: Loss = 35.04813920264032\nIteration 3915: Loss = 35.04633748770279\nIteration 3916: Loss = 35.044536362819585\nIteration 3917: Loss = 35.04273582731472\nIteration 3918: Loss = 35.04093588051304\nIteration 3919: Loss = 35.03913652174031\nIteration 3920: Loss = 35.0373377503231\nIteration 3921: Loss = 35.0355395655889\nIteration 3922: Loss = 35.03374196686603\nIteration 3923: Loss = 35.03194495348373\nIteration 3924: Loss = 35.030148524772045\nIteration 3925: Loss = 35.028352680061914\nIteration 3926: Loss = 35.02655741868514\nIteration 3927: Loss = 35.02476273997437\nIteration 3928: Loss = 35.02296864326313\nIteration 3929: Loss = 35.02117512788579\nIteration 3930: Loss = 35.01938219317759\nIteration 3931: Loss = 35.017589838474635\nIteration 3932: Loss = 35.015798063113856\nIteration 3933: Loss = 35.014006866433064\nIteration 3934: Loss = 35.012216247770915\nIteration 3935: Loss = 35.010426206466924\nIteration 3936: Loss = 35.00863674186144\nIteration 3937: Loss = 35.00684785329569\nIteration 3938: Loss = 35.005059540111716\nIteration 3939: Loss = 35.00327180165244\nIteration 3940: Loss = 35.00148463726161\nIteration 3941: Loss = 34.99969804628384\nIteration 3942: Loss = 34.997912028064576\nIteration 3943: Loss = 34.99612658195008\nIteration 3944: Loss = 34.994341707287525\nIteration 3945: Loss = 34.99255740342486\nIteration 3946: Loss = 34.99077366971091\nIteration 3947: Loss = 34.98899050549532\nIteration 3948: Loss = 34.98720791012859\nIteration 3949: Loss = 34.98542588296206\nIteration 3950: Loss = 34.98364442334787\nIteration 3951: Loss = 34.981863530639046\nIteration 3952: Loss = 34.9800832041894\nIteration 3953: Loss = 34.97830344335363\nIteration 3954: Loss = 34.97652424748721\nIteration 3955: Loss = 34.974745615946475\nIteration 3956: Loss = 34.97296754808859\nIteration 3957: Loss = 34.97119004327154\nIteration 3958: Loss = 34.96941310085414\nIteration 3959: Loss = 34.96763672019605\nIteration 3960: Loss = 34.9658609006577\nIteration 3961: Loss = 34.96408564160042\nIteration 3962: Loss = 34.9623109423863\nIteration 3963: Loss = 34.96053680237829\nIteration 3964: Loss = 34.95876322094014\nIteration 3965: Loss = 34.95699019743644\nIteration 3966: Loss = 34.955217731232565\nIteration 3967: Loss = 34.95344582169475\nIteration 3968: Loss = 34.95167446819003\nIteration 3969: Loss = 34.94990367008623\nIteration 3970: Loss = 34.94813342675202\nIteration 3971: Loss = 34.94636373755688\nIteration 3972: Loss = 34.9445946018711\nIteration 3973: Loss = 34.942826019065784\nIteration 3974: Loss = 34.94105798851283\nIteration 3975: Loss = 34.939290509584964\nIteration 3976: Loss = 34.93752358165571\nIteration 3977: Loss = 34.9357572040994\nIteration 3978: Loss = 34.93399137629119\nIteration 3979: Loss = 34.932226097607014\nIteration 3980: Loss = 34.930461367423625\nIteration 3981: Loss = 34.928697185118565\nIteration 3982: Loss = 34.92693355007022\nIteration 3983: Loss = 34.9251704616577\nIteration 3984: Loss = 34.923407919261\nIteration 3985: Loss = 34.921645922260886\nIteration 3986: Loss = 34.91988447003887\nIteration 3987: Loss = 34.918123561977325\nIteration 3988: Loss = 34.9163631974594\nIteration 3989: Loss = 34.914603375869014\nIteration 3990: Loss = 34.91284409659093\nIteration 3991: Loss = 34.911085359010656\nIteration 3992: Loss = 34.909327162514536\nIteration 3993: Loss = 34.907569506489644\nIteration 3994: Loss = 34.9058123903239\nIteration 3995: Loss = 34.90405581340599\nIteration 3996: Loss = 34.9022997751254\nIteration 3997: Loss = 34.900544274872374\nIteration 3998: Loss = 34.89878931203796\nIteration 3999: Loss = 34.89703488601401\nIteration 4000: Loss = 34.895280996193115\nIteration 4001: Loss = 34.89352764196869\nIteration 4002: Loss = 34.89177482273491\nIteration 4003: Loss = 34.890022537886736\nIteration 4004: Loss = 34.8882707868199\nIteration 4005: Loss = 34.886519568930936\nIteration 4006: Loss = 34.884768883617134\nIteration 4007: Loss = 34.88301873027655\nIteration 4008: Loss = 34.88126910830805\nIteration 4009: Loss = 34.87952001711125\nIteration 4010: Loss = 34.87777145608654\nIteration 4011: Loss = 34.87602342463509\nIteration 4012: Loss = 34.874275922158844\nIteration 4013: Loss = 34.872528948060506\nIteration 4014: Loss = 34.870782501743555\nIteration 4015: Loss = 34.86903658261225\nIteration 4016: Loss = 34.86729119007159\nIteration 4017: Loss = 34.865546323527354\nIteration 4018: Loss = 34.8638019823861\nIteration 4019: Loss = 34.86205816605514\nIteration 4020: Loss = 34.86031487394254\nIteration 4021: Loss = 34.85857210545713\nIteration 4022: Loss = 34.85682986000852\nIteration 4023: Loss = 34.85508813700706\nIteration 4024: Loss = 34.853346935863875\nIteration 4025: Loss = 34.85160625599084\nIteration 4026: Loss = 34.849866096800596\nIteration 4027: Loss = 34.84812645770652\nIteration 4028: Loss = 34.84638733812275\nIteration 4029: Loss = 34.84464873746421\nIteration 4030: Loss = 34.84291065514654\nIteration 4031: Loss = 34.841173090586146\nIteration 4032: Loss = 34.839436043200195\nIteration 4033: Loss = 34.83769951240659\nIteration 4034: Loss = 34.83596349762398\nIteration 4035: Loss = 34.83422799827179\nIteration 4036: Loss = 34.83249301377016\nIteration 4037: Loss = 34.83075854354\nIteration 4038: Loss = 34.829024587002955\nIteration 4039: Loss = 34.82729114358142\nIteration 4040: Loss = 34.825558212698525\nIteration 4041: Loss = 34.82382579377814\nIteration 4042: Loss = 34.82209388624491\nIteration 4043: Loss = 34.82036248952418\nIteration 4044: Loss = 34.81863160304205\nIteration 4045: Loss = 34.81690122622537\nIteration 4046: Loss = 34.81517135850171\nIteration 4047: Loss = 34.81344199929939\nIteration 4048: Loss = 34.81171314804745\nIteration 4049: Loss = 34.809984804175706\nIteration 4050: Loss = 34.80825696711466\nIteration 4051: Loss = 34.80652963629556\nIteration 4052: Loss = 34.8048028111504\nIteration 4053: Loss = 34.8030764911119\nIteration 4054: Loss = 34.801350675613506\nIteration 4055: Loss = 34.7996253640894\nIteration 4056: Loss = 34.797900555974486\nIteration 4057: Loss = 34.7961762507044\nIteration 4058: Loss = 34.79445244771549\nIteration 4059: Loss = 34.79272914644486\nIteration 4060: Loss = 34.79100634633033\nIteration 4061: Loss = 34.7892840468104\nIteration 4062: Loss = 34.78756224732436\nIteration 4063: Loss = 34.78584094731217\nIteration 4064: Loss = 34.78412014621453\nIteration 4065: Loss = 34.78239984347288\nIteration 4066: Loss = 34.780680038529354\nIteration 4067: Loss = 34.778960730826796\nIteration 4068: Loss = 34.777241919808795\nIteration 4069: Loss = 34.775523604919634\nIteration 4070: Loss = 34.77380578560433\nIteration 4071: Loss = 34.7720884613086\nIteration 4072: Loss = 34.77037163147888\nIteration 4073: Loss = 34.76865529556232\nIteration 4074: Loss = 34.766939453006756\nIteration 4075: Loss = 34.7652241032608\nIteration 4076: Loss = 34.763509245773704\nIteration 4077: Loss = 34.761794879995456\nIteration 4078: Loss = 34.76008100537677\nIteration 4079: Loss = 34.75836762136903\nIteration 4080: Loss = 34.756654727424376\nIteration 4081: Loss = 34.75494232299559\nIteration 4082: Loss = 34.75323040753621\nIteration 4083: Loss = 34.75151898050045\nIteration 4084: Loss = 34.74980804134326\nIteration 4085: Loss = 34.74809758952023\nIteration 4086: Loss = 34.74638762448771\nIteration 4087: Loss = 34.74467814570272\nIteration 4088: Loss = 34.74296915262299\nIteration 4089: Loss = 34.741260644706955\nIteration 4090: Loss = 34.73955262141372\nIteration 4091: Loss = 34.737845082203116\nIteration 4092: Loss = 34.73613802653565\nIteration 4093: Loss = 34.73443145387252\nIteration 4094: Loss = 34.732725363675634\nIteration 4095: Loss = 34.73101975540761\nIteration 4096: Loss = 34.72931462853168\nIteration 4097: Loss = 34.727609982511865\nIteration 4098: Loss = 34.72590581681281\nIteration 4099: Loss = 34.72420213089988\nIteration 4100: Loss = 34.722498924239105\nIteration 4101: Loss = 34.720796196297236\nIteration 4102: Loss = 34.71909394654166\nIteration 4103: Loss = 34.7173921744405\nIteration 4104: Loss = 34.715690879462535\nIteration 4105: Loss = 34.71399006107723\nIteration 4106: Loss = 34.71228971875473\nIteration 4107: Loss = 34.7105898519659\nIteration 4108: Loss = 34.70889046018222\nIteration 4109: Loss = 34.70719154287591\nIteration 4110: Loss = 34.70549309951983\nIteration 4111: Loss = 34.70379512958753\nIteration 4112: Loss = 34.702097632553254\nIteration 4113: Loss = 34.700400607891886\nIteration 4114: Loss = 34.698704055079034\nIteration 4115: Loss = 34.69700797359093\nIteration 4116: Loss = 34.695312362904524\nIteration 4117: Loss = 34.69361722249742\nIteration 4118: Loss = 34.691922551847874\nIteration 4119: Loss = 34.69022835043484\nIteration 4120: Loss = 34.68853461773796\nIteration 4121: Loss = 34.68684135323748\nIteration 4122: Loss = 34.68514855641438\nIteration 4123: Loss = 34.683456226750295\nIteration 4124: Loss = 34.68176436372749\nIteration 4125: Loss = 34.680072966828924\nIteration 4126: Loss = 34.67838203553824\nIteration 4127: Loss = 34.676691569339695\nIteration 4128: Loss = 34.67500156771826\nIteration 4129: Loss = 34.673312030159536\nIteration 4130: Loss = 34.671622956149804\nIteration 4131: Loss = 34.66993434517599\nIteration 4132: Loss = 34.6682461967257\nIteration 4133: Loss = 34.66655851028717\nIteration 4134: Loss = 34.66487128534933\nIteration 4135: Loss = 34.66318452140174\nIteration 4136: Loss = 34.661498217934636\nIteration 4137: Loss = 34.659812374438886\nIteration 4138: Loss = 34.65812699040603\nIteration 4139: Loss = 34.65644206532828\nIteration 4140: Loss = 34.65475759869844\nIteration 4141: Loss = 34.65307359001003\nIteration 4142: Loss = 34.651390038757214\nIteration 4143: Loss = 34.649706944434755\nIteration 4144: Loss = 34.648024306538126\nIteration 4145: Loss = 34.64634212456341\nIteration 4146: Loss = 34.644660398007375\nIteration 4147: Loss = 34.64297912636739\nIteration 4148: Loss = 34.641298309141504\nIteration 4149: Loss = 34.6396179458284\nIteration 4150: Loss = 34.63793803592741\nIteration 4151: Loss = 34.63625857893852\nIteration 4152: Loss = 34.634579574362306\nIteration 4153: Loss = 34.63290102170007\nIteration 4154: Loss = 34.63122292045368\nIteration 4155: Loss = 34.62954527012571\nIteration 4156: Loss = 34.62786807021931\nIteration 4157: Loss = 34.62619132023833\nIteration 4158: Loss = 34.624515019687195\nIteration 4159: Loss = 34.62283916807102\nIteration 4160: Loss = 34.621163764895535\nIteration 4161: Loss = 34.6194888096671\nIteration 4162: Loss = 34.61781430189272\nIteration 4163: Loss = 34.616140241080025\nIteration 4164: Loss = 34.6144666267373\nIteration 4165: Loss = 34.61279345837344\nIteration 4166: Loss = 34.61112073549798\nIteration 4167: Loss = 34.60944845762106\nIteration 4168: Loss = 34.6077766242535\nIteration 4169: Loss = 34.60610523490672\nIteration 4170: Loss = 34.604434289092765\nIteration 4171: Loss = 34.60276378632432\nIteration 4172: Loss = 34.601093726114655\nIteration 4173: Loss = 34.59942410797775\nIteration 4174: Loss = 34.59775493142813\nIteration 4175: Loss = 34.59608619598099\nIteration 4176: Loss = 34.59441790115212\nIteration 4177: Loss = 34.59275004645796\nIteration 4178: Loss = 34.591082631415546\nIteration 4179: Loss = 34.589415655542545\nIteration 4180: Loss = 34.58774911835725\nIteration 4181: Loss = 34.58608301937857\nIteration 4182: Loss = 34.58441735812603\nIteration 4183: Loss = 34.58275213411977\nIteration 4184: Loss = 34.58108734688055\nIteration 4185: Loss = 34.57942299592977\nIteration 4186: Loss = 34.5777590807894\nIteration 4187: Loss = 34.57609560098203\nIteration 4188: Loss = 34.57443255603091\nIteration 4189: Loss = 34.572769945459875\nIteration 4190: Loss = 34.57110776879335\nIteration 4191: Loss = 34.569446025556395\nIteration 4192: Loss = 34.5677847152747\nIteration 4193: Loss = 34.566123837474514\nIteration 4194: Loss = 34.564463391682736\nIteration 4195: Loss = 34.56280337742686\nIteration 4196: Loss = 34.56114379423498\nIteration 4197: Loss = 34.55948464163582\nIteration 4198: Loss = 34.55782591915869\nIteration 4199: Loss = 34.55616762633348\nIteration 4200: Loss = 34.55450976269076\nIteration 4201: Loss = 34.55285232776162\nIteration 4202: Loss = 34.55119532107781\nIteration 4203: Loss = 34.54953874217165\nIteration 4204: Loss = 34.54788259057608\nIteration 4205: Loss = 34.546226865824615\nIteration 4206: Loss = 34.54457156745141\nIteration 4207: Loss = 34.54291669499119\nIteration 4208: Loss = 34.54126224797928\nIteration 4209: Loss = 34.53960822595161\nIteration 4210: Loss = 34.53795462844468\nIteration 4211: Loss = 34.536301454995645\nIteration 4212: Loss = 34.53464870514219\nIteration 4213: Loss = 34.532996378422624\nIteration 4214: Loss = 34.531344474375864\nIteration 4215: Loss = 34.529692992541406\nIteration 4216: Loss = 34.5280419324593\nIteration 4217: Loss = 34.52639129367026\nIteration 4218: Loss = 34.52474107571553\nIteration 4219: Loss = 34.52309127813697\nIteration 4220: Loss = 34.52144190047702\nIteration 4221: Loss = 34.51979294227873\nIteration 4222: Loss = 34.5181444030857\nIteration 4223: Loss = 34.516496282442134\nIteration 4224: Loss = 34.514848579892856\nIteration 4225: Loss = 34.513201294983205\nIteration 4226: Loss = 34.51155442725916\nIteration 4227: Loss = 34.50990797626726\nIteration 4228: Loss = 34.50826194155463\nIteration 4229: Loss = 34.506616322668975\nIteration 4230: Loss = 34.504971119158604\nIteration 4231: Loss = 34.503326330572364\nIteration 4232: Loss = 34.50168195645971\nIteration 4233: Loss = 34.50003799637068\nIteration 4234: Loss = 34.49839444985588\nIteration 4235: Loss = 34.49675131646648\nIteration 4236: Loss = 34.49510859575425\nIteration 4237: Loss = 34.49346628727153\nIteration 4238: Loss = 34.49182439057122\nIteration 4239: Loss = 34.490182905206815\nIteration 4240: Loss = 34.48854183073237\nIteration 4241: Loss = 34.48690116670252\nIteration 4242: Loss = 34.48526091267248\nIteration 4243: Loss = 34.483621068198005\nIteration 4244: Loss = 34.48198163283546\nIteration 4245: Loss = 34.480342606141754\nIteration 4246: Loss = 34.478703987674365\nIteration 4247: Loss = 34.47706577699135\nIteration 4248: Loss = 34.47542797365135\nIteration 4249: Loss = 34.47379057721353\nIteration 4250: Loss = 34.47215358723765\nIteration 4251: Loss = 34.47051700328404\nIteration 4252: Loss = 34.46888082491357\nIteration 4253: Loss = 34.467245051687705\nIteration 4254: Loss = 34.46560968316845\nIteration 4255: Loss = 34.46397471891838\nIteration 4256: Loss = 34.462340158500645\nIteration 4257: Loss = 34.46070600147892\nIteration 4258: Loss = 34.45907224741747\nIteration 4259: Loss = 34.457438895881126\nIteration 4260: Loss = 34.45580594643525\nIteration 4261: Loss = 34.45417339864579\nIteration 4262: Loss = 34.45254125207922\nIteration 4263: Loss = 34.450909506302615\nIteration 4264: Loss = 34.44927816088356\nIteration 4265: Loss = 34.44764721539023\nIteration 4266: Loss = 34.44601666939133\nIteration 4267: Loss = 34.44438652245612\nIteration 4268: Loss = 34.44275677415444\nIteration 4269: Loss = 34.44112742405666\nIteration 4270: Loss = 34.439498471733714\nIteration 4271: Loss = 34.43786991675706\nIteration 4272: Loss = 34.436241758698735\nIteration 4273: Loss = 34.434613997131315\nIteration 4274: Loss = 34.432986631627934\nIteration 4275: Loss = 34.431359661762265\nIteration 4276: Loss = 34.42973308710852\nIteration 4277: Loss = 34.42810690724146\nIteration 4278: Loss = 34.42648112173642\nIteration 4279: Loss = 34.42485573016925\nIteration 4280: Loss = 34.423230732116345\nIteration 4281: Loss = 34.421606127154675\nIteration 4282: Loss = 34.419981914861715\nIteration 4283: Loss = 34.4183580948155\nIteration 4284: Loss = 34.41673466659462\nIteration 4285: Loss = 34.41511162977816\nIteration 4286: Loss = 34.41348898394581\nIteration 4287: Loss = 34.41186672867774\nIteration 4288: Loss = 34.410244863554716\nIteration 4289: Loss = 34.40862338815797\nIteration 4290: Loss = 34.40700230206936\nIteration 4291: Loss = 34.40538160487119\nIteration 4292: Loss = 34.40376129614637\nIteration 4293: Loss = 34.402141375478315\nIteration 4294: Loss = 34.40052184245099\nIteration 4295: Loss = 34.39890269664886\nIteration 4296: Loss = 34.397283937656965\nIteration 4297: Loss = 34.395665565060845\nIteration 4298: Loss = 34.3940475784466\nIteration 4299: Loss = 34.392429977400845\nIteration 4300: Loss = 34.39081276151072\nIteration 4301: Loss = 34.38919593036391\nIteration 4302: Loss = 34.38757948354863\nIteration 4303: Loss = 34.3859634206536\nIteration 4304: Loss = 34.384347741268094\nIteration 4305: Loss = 34.38273244498191\nIteration 4306: Loss = 34.38111753138536\nIteration 4307: Loss = 34.37950300006928\nIteration 4308: Loss = 34.377888850625034\nIteration 4309: Loss = 34.37627508264453\nIteration 4310: Loss = 34.374661695720185\nIteration 4311: Loss = 34.373048689444936\nIteration 4312: Loss = 34.37143606341223\nIteration 4313: Loss = 34.369823817216066\nIteration 4314: Loss = 34.36821195045096\nIteration 4315: Loss = 34.36660046271192\nIteration 4316: Loss = 34.3649893535945\nIteration 4317: Loss = 34.36337862269476\nIteration 4318: Loss = 34.36176826960928\nIteration 4319: Loss = 34.36015829393516\nIteration 4320: Loss = 34.358548695270024\nIteration 4321: Loss = 34.356939473211995\nIteration 4322: Loss = 34.35533062735974\nIteration 4323: Loss = 34.353722157312404\nIteration 4324: Loss = 34.35211406266969\nIteration 4325: Loss = 34.35050634303175\nIteration 4326: Loss = 34.348898997999335\nIteration 4327: Loss = 34.34729202717364\nIteration 4328: Loss = 34.34568543015639\nIteration 4329: Loss = 34.344079206549836\nIteration 4330: Loss = 34.34247335595672\nIteration 4331: Loss = 34.34086787798031\nIteration 4332: Loss = 34.33926277222436\nIteration 4333: Loss = 34.33765803829318\nIteration 4334: Loss = 34.33605367579152\nIteration 4335: Loss = 34.33444968432468\nIteration 4336: Loss = 34.33284606349848\nIteration 4337: Loss = 34.3312428129192\nIteration 4338: Loss = 34.32963993219367\nIteration 4339: Loss = 34.328037420929185\nIteration 4340: Loss = 34.326435278733584\nIteration 4341: Loss = 34.32483350521517\nIteration 4342: Loss = 34.32323209998276\nIteration 4343: Loss = 34.3216310626457\nIteration 4344: Loss = 34.320030392813806\nIteration 4345: Loss = 34.3184300900974\nIteration 4346: Loss = 34.31683015410732\nIteration 4347: Loss = 34.31523058445489\nIteration 4348: Loss = 34.313631380751914\nIteration 4349: Loss = 34.31203254261074\nIteration 4350: Loss = 34.31043406964417\nIteration 4351: Loss = 34.30883596146553\nIteration 4352: Loss = 34.307238217688635\nIteration 4353: Loss = 34.30564083792778\nIteration 4354: Loss = 34.30404382179778\nIteration 4355: Loss = 34.302447168913915\nIteration 4356: Loss = 34.30085087889199\nIteration 4357: Loss = 34.29925495134827\nIteration 4358: Loss = 34.29765938589955\nIteration 4359: Loss = 34.296064182163086\nIteration 4360: Loss = 34.294469339756624\nIteration 4361: Loss = 34.292874858298426\nIteration 4362: Loss = 34.29128073740722\nIteration 4363: Loss = 34.28968697670223\nIteration 4364: Loss = 34.288093575803195\nIteration 4365: Loss = 34.28650053433026\nIteration 4366: Loss = 34.284907851904165\nIteration 4367: Loss = 34.283315528146076\nIteration 4368: Loss = 34.28172356267763\nIteration 4369: Loss = 34.280131955120986\nIteration 4370: Loss = 34.278540705098784\nIteration 4371: Loss = 34.27694981223413\nIteration 4372: Loss = 34.2753592761506\nIteration 4373: Loss = 34.27376909647231\nIteration 4374: Loss = 34.272179272823806\nIteration 4375: Loss = 34.27058980483011\nIteration 4376: Loss = 34.269000692116784\nIteration 4377: Loss = 34.267411934309806\nIteration 4378: Loss = 34.26582353103568\nIteration 4379: Loss = 34.26423548192133\nIteration 4380: Loss = 34.26264778659423\nIteration 4381: Loss = 34.2610604446823\nIteration 4382: Loss = 34.259473455813904\nIteration 4383: Loss = 34.257886819617944\nIteration 4384: Loss = 34.25630053572375\nIteration 4385: Loss = 34.25471460376116\nIteration 4386: Loss = 34.253129023360465\nIteration 4387: Loss = 34.25154379415242\nIteration 4388: Loss = 34.24995891576829\nIteration 4389: Loss = 34.24837438783978\nIteration 4390: Loss = 34.246790209999084\nIteration 4391: Loss = 34.24520638187885\nIteration 4392: Loss = 34.24362290311224\nIteration 4393: Loss = 34.24203977333282\nIteration 4394: Loss = 34.24045699217468\nIteration 4395: Loss = 34.238874559272354\nIteration 4396: Loss = 34.23729247426087\nIteration 4397: Loss = 34.23571073677566\nIteration 4398: Loss = 34.234129346452704\nIteration 4399: Loss = 34.23254830292841\nIteration 4400: Loss = 34.23096760583964\nIteration 4401: Loss = 34.22938725482375\nIteration 4402: Loss = 34.22780724951852\nIteration 4403: Loss = 34.22622758956225\nIteration 4404: Loss = 34.22464827459366\nIteration 4405: Loss = 34.22306930425196\nIteration 4406: Loss = 34.221490678176785\nIteration 4407: Loss = 34.21991239600827\nIteration 4408: Loss = 34.218334457387\nIteration 4409: Loss = 34.21675686195401\nIteration 4410: Loss = 34.21517960935083\nIteration 4411: Loss = 34.213602699219365\nIteration 4412: Loss = 34.212026131202094\nIteration 4413: Loss = 34.21044990494186\nIteration 4414: Loss = 34.20887402008202\nIteration 4415: Loss = 34.20729847626635\nIteration 4416: Loss = 34.20572327313913\nIteration 4417: Loss = 34.20414841034503\nIteration 4418: Loss = 34.20257388752922\nIteration 4419: Loss = 34.20099970433733\nIteration 4420: Loss = 34.19942586041542\nIteration 4421: Loss = 34.197852355410014\nIteration 4422: Loss = 34.1962791889681\nIteration 4423: Loss = 34.19470636073708\nIteration 4424: Loss = 34.19313387036485\nIteration 4425: Loss = 34.19156171749975\nIteration 4426: Loss = 34.18998990179055\nIteration 4427: Loss = 34.18841842288647\nIteration 4428: Loss = 34.18684728043722\nIteration 4429: Loss = 34.18527647409291\nIteration 4430: Loss = 34.18370600350411\nIteration 4431: Loss = 34.18213586832187\nIteration 4432: Loss = 34.180566068197656\nIteration 4433: Loss = 34.17899660278337\nIteration 4434: Loss = 34.177427471731384\nIteration 4435: Loss = 34.17585867469451\nIteration 4436: Loss = 34.174290211326\nIteration 4437: Loss = 34.17272208127956\nIteration 4438: Loss = 34.17115428420933\nIteration 4439: Loss = 34.16958681976988\nIteration 4440: Loss = 34.16801968761628\nIteration 4441: Loss = 34.16645288740395\n",
      "Iteration 4442: Loss = 34.16488641878882\nIteration 4443: Loss = 34.16332028142724\nIteration 4444: Loss = 34.16175447497603\nIteration 4445: Loss = 34.16018899909237\nIteration 4446: Loss = 34.15862385343396\nIteration 4447: Loss = 34.157059037658904\nIteration 4448: Loss = 34.155494551425754\nIteration 4449: Loss = 34.15393039439348\nIteration 4450: Loss = 34.1523665662215\nIteration 4451: Loss = 34.1508030665697\nIteration 4452: Loss = 34.14923989509833\nIteration 4453: Loss = 34.14767705146814\nIteration 4454: Loss = 34.14611453534031\nIteration 4455: Loss = 34.1445523463764\nIteration 4456: Loss = 34.14299048423845\nIteration 4457: Loss = 34.14142894858893\nIteration 4458: Loss = 34.13986773909072\nIteration 4459: Loss = 34.13830685540716\nIteration 4460: Loss = 34.13674629720199\nIteration 4461: Loss = 34.13518606413941\nIteration 4462: Loss = 34.13362615588402\nIteration 4463: Loss = 34.13206657210088\nIteration 4464: Loss = 34.130507312455464\nIteration 4465: Loss = 34.12894837661367\nIteration 4466: Loss = 34.127389764241826\nIteration 4467: Loss = 34.1258314750067\nIteration 4468: Loss = 34.12427350857546\nIteration 4469: Loss = 34.12271586461573\nIteration 4470: Loss = 34.121158542795534\nIteration 4471: Loss = 34.11960154278336\nIteration 4472: Loss = 34.11804486424806\nIteration 4473: Loss = 34.11648850685896\nIteration 4474: Loss = 34.11493247028578\nIteration 4475: Loss = 34.11337675419868\nIteration 4476: Loss = 34.11182135826825\nIteration 4477: Loss = 34.110266282165476\nIteration 4478: Loss = 34.10871152556178\nIteration 4479: Loss = 34.10715708812901\nIteration 4480: Loss = 34.105602969539405\nIteration 4481: Loss = 34.10404916946566\nIteration 4482: Loss = 34.10249568758088\nIteration 4483: Loss = 34.10094252355857\nIteration 4484: Loss = 34.09938967707267\nIteration 4485: Loss = 34.09783714779755\nIteration 4486: Loss = 34.09628493540794\nIteration 4487: Loss = 34.09473303957905\nIteration 4488: Loss = 34.093181459986475\nIteration 4489: Loss = 34.091630196306234\nIteration 4490: Loss = 34.09007924821476\nIteration 4491: Loss = 34.08852861538889\nIteration 4492: Loss = 34.08697829750588\nIteration 4493: Loss = 34.08542829424342\nIteration 4494: Loss = 34.083878605279565\nIteration 4495: Loss = 34.082329230292835\nIteration 4496: Loss = 34.08078016896213\nIteration 4497: Loss = 34.07923142096675\nIteration 4498: Loss = 34.07768298598645\nIteration 4499: Loss = 34.07613486370135\nIteration 4500: Loss = 34.074587053792\nIteration 4501: Loss = 34.07303955593935\nIteration 4502: Loss = 34.07149236982478\nIteration 4503: Loss = 34.06994549513006\nIteration 4504: Loss = 34.068398931537345\nIteration 4505: Loss = 34.06685267872924\nIteration 4506: Loss = 34.06530673638873\nIteration 4507: Loss = 34.063761104199216\nIteration 4508: Loss = 34.0622157818445\nIteration 4509: Loss = 34.06067076900877\nIteration 4510: Loss = 34.05912606537665\nIteration 4511: Loss = 34.05758167063317\nIteration 4512: Loss = 34.05603758446371\nIteration 4513: Loss = 34.05449380655413\nIteration 4514: Loss = 34.05295033659061\nIteration 4515: Loss = 34.05140717425979\nIteration 4516: Loss = 34.049864319248705\nIteration 4517: Loss = 34.04832177124476\nIteration 4518: Loss = 34.04677952993579\nIteration 4519: Loss = 34.04523759501\nIteration 4520: Loss = 34.04369596615603\nIteration 4521: Loss = 34.042154643062894\nIteration 4522: Loss = 34.04061362542\nIteration 4523: Loss = 34.03907291291717\nIteration 4524: Loss = 34.037532505244606\nIteration 4525: Loss = 34.03599240209292\nIteration 4526: Loss = 34.03445260315312\nIteration 4527: Loss = 34.032913108116595\nIteration 4528: Loss = 34.031373916675136\nIteration 4529: Loss = 34.02983502852094\nIteration 4530: Loss = 34.02829644334658\nIteration 4531: Loss = 34.026758160845034\nIteration 4532: Loss = 34.02522018070965\nIteration 4533: Loss = 34.023682502634195\nIteration 4534: Loss = 34.022145126312836\nIteration 4535: Loss = 34.020608051440085\nIteration 4536: Loss = 34.01907127771089\nIteration 4537: Loss = 34.01753480482057\nIteration 4538: Loss = 34.01599863246483\nIteration 4539: Loss = 34.01446276033977\nIteration 4540: Loss = 34.01292718814188\nIteration 4541: Loss = 34.011391915568026\nIteration 4542: Loss = 34.00985694231549\nIteration 4543: Loss = 34.008322268081905\nIteration 4544: Loss = 34.00678789256531\nIteration 4545: Loss = 34.00525381546415\nIteration 4546: Loss = 34.003720036477205\nIteration 4547: Loss = 34.00218655530368\nIteration 4548: Loss = 34.000653371643146\nIteration 4549: Loss = 33.999120485195576\nIteration 4550: Loss = 33.997587895661304\nIteration 4551: Loss = 33.99605560274107\nIteration 4552: Loss = 33.99452360613598\nIteration 4553: Loss = 33.99299190554752\nIteration 4554: Loss = 33.991460500677576\nIteration 4555: Loss = 33.98992939122839\nIteration 4556: Loss = 33.9883985769026\nIteration 4557: Loss = 33.986868057403235\nIteration 4558: Loss = 33.98533783243367\nIteration 4559: Loss = 33.9838079016977\nIteration 4560: Loss = 33.98227826489947\nIteration 4561: Loss = 33.98074892174351\nIteration 4562: Loss = 33.97921987193473\nIteration 4563: Loss = 33.97769111517841\nIteration 4564: Loss = 33.97616265118022\nIteration 4565: Loss = 33.974634479646184\nIteration 4566: Loss = 33.973106600282726\nIteration 4567: Loss = 33.97157901279665\nIteration 4568: Loss = 33.97005171689511\nIteration 4569: Loss = 33.96852471228562\nIteration 4570: Loss = 33.96699799867612\nIteration 4571: Loss = 33.96547157577489\nIteration 4572: Loss = 33.96394544329058\nIteration 4573: Loss = 33.962419600932215\nIteration 4574: Loss = 33.960894048409216\nIteration 4575: Loss = 33.95936878543134\nIteration 4576: Loss = 33.95784381170873\nIteration 4577: Loss = 33.95631912695192\nIteration 4578: Loss = 33.95479473087176\nIteration 4579: Loss = 33.95327062317953\nIteration 4580: Loss = 33.951746803586836\nIteration 4581: Loss = 33.95022327180567\nIteration 4582: Loss = 33.94870002754841\nIteration 4583: Loss = 33.947177070527765\nIteration 4584: Loss = 33.94565440045682\nIteration 4585: Loss = 33.94413201704904\nIteration 4586: Loss = 33.94260992001826\nIteration 4587: Loss = 33.94108810907866\nIteration 4588: Loss = 33.939566583944796\nIteration 4589: Loss = 33.93804534433159\nIteration 4590: Loss = 33.93652438995433\nIteration 4591: Loss = 33.93500372052865\nIteration 4592: Loss = 33.933483335770575\nIteration 4593: Loss = 33.931963235396466\nIteration 4594: Loss = 33.93044341912307\nIteration 4595: Loss = 33.92892388666749\nIteration 4596: Loss = 33.92740463774717\nIteration 4597: Loss = 33.92588567207993\nIteration 4598: Loss = 33.924366989383955\nIteration 4599: Loss = 33.92284858937779\nIteration 4600: Loss = 33.92133047178032\nIteration 4601: Loss = 33.919812636310816\nIteration 4602: Loss = 33.918295082688886\nIteration 4603: Loss = 33.916777810634514\nIteration 4604: Loss = 33.91526081986802\nIteration 4605: Loss = 33.913744110110095\nIteration 4606: Loss = 33.91222768108179\nIteration 4607: Loss = 33.91071153250451\nIteration 4608: Loss = 33.909195664100004\nIteration 4609: Loss = 33.907680075590385\nIteration 4610: Loss = 33.90616476669814\nIteration 4611: Loss = 33.90464973714605\nIteration 4612: Loss = 33.903134986657335\nIteration 4613: Loss = 33.90162051495549\nIteration 4614: Loss = 33.90010632176441\nIteration 4615: Loss = 33.89859240680834\nIteration 4616: Loss = 33.89707876981185\nIteration 4617: Loss = 33.89556541049989\nIteration 4618: Loss = 33.894052328597745\nIteration 4619: Loss = 33.89253952383105\nIteration 4620: Loss = 33.89102699592578\nIteration 4621: Loss = 33.889514744608306\nIteration 4622: Loss = 33.88800276960531\nIteration 4623: Loss = 33.886491070643814\nIteration 4624: Loss = 33.8849796474512\nIteration 4625: Loss = 33.88346849975524\nIteration 4626: Loss = 33.881957627283974\nIteration 4627: Loss = 33.880447029765854\nIteration 4628: Loss = 33.878936706929636\nIteration 4629: Loss = 33.877426658504454\nIteration 4630: Loss = 33.875916884219784\nIteration 4631: Loss = 33.87440738380541\nIteration 4632: Loss = 33.87289815699152\nIteration 4633: Loss = 33.871389203508585\nIteration 4634: Loss = 33.86988052308749\nIteration 4635: Loss = 33.868372115459366\nIteration 4636: Loss = 33.8668639803558\nIteration 4637: Loss = 33.865356117508625\nIteration 4638: Loss = 33.86384852665009\nIteration 4639: Loss = 33.862341207512735\nIteration 4640: Loss = 33.86083415982945\nIteration 4641: Loss = 33.85932738333349\nIteration 4642: Loss = 33.857820877758435\nIteration 4643: Loss = 33.85631464283819\nIteration 4644: Loss = 33.85480867830703\nIteration 4645: Loss = 33.85330298389956\nIteration 4646: Loss = 33.851797559350686\nIteration 4647: Loss = 33.8502924043957\nIteration 4648: Loss = 33.84878751877022\nIteration 4649: Loss = 33.84728290221019\nIteration 4650: Loss = 33.845778554451904\nIteration 4651: Loss = 33.84427447523197\nIteration 4652: Loss = 33.84277066428736\nIteration 4653: Loss = 33.84126712135536\nIteration 4654: Loss = 33.839763846173604\nIteration 4655: Loss = 33.83826083848006\nIteration 4656: Loss = 33.83675809801301\nIteration 4657: Loss = 33.835255624511106\nIteration 4658: Loss = 33.833753417713304\nIteration 4659: Loss = 33.832251477358895\nIteration 4660: Loss = 33.83074980318752\nIteration 4661: Loss = 33.82924839493915\nIteration 4662: Loss = 33.82774725235406\nIteration 4663: Loss = 33.82624637517288\nIteration 4664: Loss = 33.82474576313657\nIteration 4665: Loss = 33.82324541598642\nIteration 4666: Loss = 33.82174533346405\nIteration 4667: Loss = 33.820245515311385\nIteration 4668: Loss = 33.81874596127072\nIteration 4669: Loss = 33.81724667108465\nIteration 4670: Loss = 33.81574764449611\nIteration 4671: Loss = 33.81424888124835\nIteration 4672: Loss = 33.81275038108498\nIteration 4673: Loss = 33.8112521437499\nIteration 4674: Loss = 33.80975416898733\nIteration 4675: Loss = 33.80825645654187\nIteration 4676: Loss = 33.80675900615838\nIteration 4677: Loss = 33.80526181758211\nIteration 4678: Loss = 33.80376489055856\nIteration 4679: Loss = 33.80226822483365\nIteration 4680: Loss = 33.800771820153514\nIteration 4681: Loss = 33.79927567626471\nIteration 4682: Loss = 33.79777979291405\nIteration 4683: Loss = 33.796284169848704\nIteration 4684: Loss = 33.79478880681614\nIteration 4685: Loss = 33.79329370356418\nIteration 4686: Loss = 33.79179885984094\nIteration 4687: Loss = 33.79030427539486\nIteration 4688: Loss = 33.78880994997471\nIteration 4689: Loss = 33.78731588332958\nIteration 4690: Loss = 33.78582207520888\nIteration 4691: Loss = 33.78432852536233\nIteration 4692: Loss = 33.78283523353998\nIteration 4693: Loss = 33.78134219949218\nIteration 4694: Loss = 33.77984942296962\nIteration 4695: Loss = 33.77835690372329\nIteration 4696: Loss = 33.776864641504524\nIteration 4697: Loss = 33.775372636064944\nIteration 4698: Loss = 33.77388088715651\nIteration 4699: Loss = 33.77238939453146\nIteration 4700: Loss = 33.770898157942405\nIteration 4701: Loss = 33.76940717714223\nIteration 4702: Loss = 33.76791645188414\nIteration 4703: Loss = 33.76642598192167\nIteration 4704: Loss = 33.76493576700867\nIteration 4705: Loss = 33.76344580689926\nIteration 4706: Loss = 33.761956101347934\nIteration 4707: Loss = 33.76046665010947\nIteration 4708: Loss = 33.75897745293895\nIteration 4709: Loss = 33.75748850959178\nIteration 4710: Loss = 33.75599981982367\nIteration 4711: Loss = 33.754511383390664\nIteration 4712: Loss = 33.75302320004908\nIteration 4713: Loss = 33.75153526955557\nIteration 4714: Loss = 33.7500475916671\nIteration 4715: Loss = 33.74856016614092\nIteration 4716: Loss = 33.74707299273462\nIteration 4717: Loss = 33.7455860712061\nIteration 4718: Loss = 33.74409940131351\nIteration 4719: Loss = 33.74261298281539\nIteration 4720: Loss = 33.74112681547052\nIteration 4721: Loss = 33.739640899038044\nIteration 4722: Loss = 33.73815523327737\nIteration 4723: Loss = 33.73666981794822\nIteration 4724: Loss = 33.735184652810645\nIteration 4725: Loss = 33.73369973762496\nIteration 4726: Loss = 33.732215072151845\nIteration 4727: Loss = 33.730730656152225\nIteration 4728: Loss = 33.72924648938736\nIteration 4729: Loss = 33.727762571618804\nIteration 4730: Loss = 33.72627890260844\nIteration 4731: Loss = 33.72479548211842\nIteration 4732: Loss = 33.7233123099112\nIteration 4733: Loss = 33.721829385749565\nIteration 4734: Loss = 33.72034670939658\nIteration 4735: Loss = 33.71886428061563\nIteration 4736: Loss = 33.717382099170386\nIteration 4737: Loss = 33.715900164824816\nIteration 4738: Loss = 33.7144184773432\nIteration 4739: Loss = 33.712937036490125\nIteration 4740: Loss = 33.71145584203045\nIteration 4741: Loss = 33.709974893729374\nIteration 4742: Loss = 33.70849419135236\nIteration 4743: Loss = 33.70701373466517\nIteration 4744: Loss = 33.70553352343388\nIteration 4745: Loss = 33.70405355742487\nIteration 4746: Loss = 33.7025738364048\nIteration 4747: Loss = 33.70109436014062\nIteration 4748: Loss = 33.69961512839962\nIteration 4749: Loss = 33.69813614094934\nIteration 4750: Loss = 33.69665739755761\nIteration 4751: Loss = 33.695178897992605\nIteration 4752: Loss = 33.69370064202277\nIteration 4753: Loss = 33.69222262941682\nIteration 4754: Loss = 33.6907448599438\nIteration 4755: Loss = 33.68926733337303\nIteration 4756: Loss = 33.68779004947414\nIteration 4757: Loss = 33.68631300801702\nIteration 4758: Loss = 33.68483620877189\nIteration 4759: Loss = 33.68335965150924\nIteration 4760: Loss = 33.68188333599987\nIteration 4761: Loss = 33.68040726201485\nIteration 4762: Loss = 33.67893142932555\nIteration 4763: Loss = 33.67745583770364\nIteration 4764: Loss = 33.67598048692106\nIteration 4765: Loss = 33.67450537675008\nIteration 4766: Loss = 33.6730305069632\nIteration 4767: Loss = 33.67155587733326\nIteration 4768: Loss = 33.67008148763337\nIteration 4769: Loss = 33.668607337636914\nIteration 4770: Loss = 33.6671334271176\nIteration 4771: Loss = 33.665659755849404\nIteration 4772: Loss = 33.66418632360657\nIteration 4773: Loss = 33.66271313016367\nIteration 4774: Loss = 33.66124017529552\nIteration 4775: Loss = 33.65976745877726\nIteration 4776: Loss = 33.65829498038429\nIteration 4777: Loss = 33.65682273989231\nIteration 4778: Loss = 33.6553507370773\nIteration 4779: Loss = 33.65387897171551\nIteration 4780: Loss = 33.652407443583506\nIteration 4781: Loss = 33.65093615245812\nIteration 4782: Loss = 33.64946509811647\nIteration 4783: Loss = 33.64799428033595\nIteration 4784: Loss = 33.64652369889425\nIteration 4785: Loss = 33.64505335356934\nIteration 4786: Loss = 33.64358324413946\nIteration 4787: Loss = 33.64211337038313\nIteration 4788: Loss = 33.64064373207919\nIteration 4789: Loss = 33.63917432900671\nIteration 4790: Loss = 33.637705160945075\nIteration 4791: Loss = 33.63623622767394\nIteration 4792: Loss = 33.63476752897323\nIteration 4793: Loss = 33.63329906462317\nIteration 4794: Loss = 33.631830834404255\nIteration 4795: Loss = 33.630362838097255\nIteration 4796: Loss = 33.628895075483214\nIteration 4797: Loss = 33.62742754634347\nIteration 4798: Loss = 33.625960250459634\nIteration 4799: Loss = 33.62449318761358\nIteration 4800: Loss = 33.623026357587484\nIteration 4801: Loss = 33.62155976016377\nIteration 4802: Loss = 33.620093395125174\nIteration 4803: Loss = 33.61862726225466\nIteration 4804: Loss = 33.617161361335526\nIteration 4805: Loss = 33.6156956921513\nIteration 4806: Loss = 33.61423025448579\nIteration 4807: Loss = 33.61276504812311\nIteration 4808: Loss = 33.61130007284761\nIteration 4809: Loss = 33.60983532844394\nIteration 4810: Loss = 33.60837081469703\nIteration 4811: Loss = 33.60690653139205\nIteration 4812: Loss = 33.60544247831445\nIteration 4813: Loss = 33.603978655249975\nIteration 4814: Loss = 33.60251506198465\nIteration 4815: Loss = 33.60105169830473\nIteration 4816: Loss = 33.59958856399677\nIteration 4817: Loss = 33.59812565884758\nIteration 4818: Loss = 33.59666298264428\nIteration 4819: Loss = 33.595200535174214\nIteration 4820: Loss = 33.593738316225\nIteration 4821: Loss = 33.592276325584564\nIteration 4822: Loss = 33.590814563041064\nIteration 4823: Loss = 33.589353028382945\nIteration 4824: Loss = 33.5878917213989\nIteration 4825: Loss = 33.58643064187793\nIteration 4826: Loss = 33.584969789609275\nIteration 4827: Loss = 33.58350916438244\nIteration 4828: Loss = 33.58204876598721\nIteration 4829: Loss = 33.580588594213616\nIteration 4830: Loss = 33.579128648852\nIteration 4831: Loss = 33.577668929692926\nIteration 4832: Loss = 33.57620943652724\nIteration 4833: Loss = 33.57475016914605\nIteration 4834: Loss = 33.573291127340745\nIteration 4835: Loss = 33.571832310902955\nIteration 4836: Loss = 33.570373719624605\nIteration 4837: Loss = 33.56891535329783\nIteration 4838: Loss = 33.567457211715094\nIteration 4839: Loss = 33.56599929466909\nIteration 4840: Loss = 33.564541601952776\nIteration 4841: Loss = 33.56308413335937\nIteration 4842: Loss = 33.56162688868237\nIteration 4843: Loss = 33.56016986771552\nIteration 4844: Loss = 33.55871307025282\nIteration 4845: Loss = 33.55725649608855\nIteration 4846: Loss = 33.55580014501725\nIteration 4847: Loss = 33.55434401683371\nIteration 4848: Loss = 33.552888111332976\nIteration 4849: Loss = 33.55143242831036\nIteration 4850: Loss = 33.549976967561456\nIteration 4851: Loss = 33.54852172888208\nIteration 4852: Loss = 33.54706671206833\nIteration 4853: Loss = 33.54561191691656\nIteration 4854: Loss = 33.54415734322338\nIteration 4855: Loss = 33.542702990785656\nIteration 4856: Loss = 33.54124885940051\nIteration 4857: Loss = 33.53979494886532\nIteration 4858: Loss = 33.53834125897775\nIteration 4859: Loss = 33.536887789535676\nIteration 4860: Loss = 33.53543454033726\nIteration 4861: Loss = 33.53398151118091\nIteration 4862: Loss = 33.53252870186529\nIteration 4863: Loss = 33.53107611218932\nIteration 4864: Loss = 33.52962374195219\nIteration 4865: Loss = 33.52817159095331\nIteration 4866: Loss = 33.52671965899238\nIteration 4867: Loss = 33.52526794586933\nIteration 4868: Loss = 33.523816451384356\nIteration 4869: Loss = 33.5223651753379\nIteration 4870: Loss = 33.52091411753068\nIteration 4871: Loss = 33.51946327776363\nIteration 4872: Loss = 33.51801265583796\nIteration 4873: Loss = 33.51656225155513\nIteration 4874: Loss = 33.51511206471685\nIteration 4875: Loss = 33.51366209512508\nIteration 4876: Loss = 33.51221234258201\nIteration 4877: Loss = 33.51076280689015\nIteration 4878: Loss = 33.50931348785216\nIteration 4879: Loss = 33.50786438527105\nIteration 4880: Loss = 33.506415498950005\nIteration 4881: Loss = 33.504966828692496\nIteration 4882: Loss = 33.503518374302224\nIteration 4883: Loss = 33.502070135583175\nIteration 4884: Loss = 33.500622112339535\nIteration 4885: Loss = 33.499174304375764\nIteration 4886: Loss = 33.49772671149658\nIteration 4887: Loss = 33.496279333506926\nIteration 4888: Loss = 33.494832170212\nIteration 4889: Loss = 33.49338522141726\nIteration 4890: Loss = 33.491938486928404\nIteration 4891: Loss = 33.49049196655136\nIteration 4892: Loss = 33.489045660092316\nIteration 4893: Loss = 33.4875995673577\nIteration 4894: Loss = 33.486153688154204\nIteration 4895: Loss = 33.48470802228874\nIteration 4896: Loss = 33.483262569568474\nIteration 4897: Loss = 33.481817329800826\nIteration 4898: Loss = 33.48037230279344\nIteration 4899: Loss = 33.47892748835424\nIteration 4900: Loss = 33.47748288629134\nIteration 4901: Loss = 33.47603849641313\nIteration 4902: Loss = 33.47459431852826\nIteration 4903: Loss = 33.47315035244558\nIteration 4904: Loss = 33.47170659797422\nIteration 4905: Loss = 33.47026305492353\nIteration 4906: Loss = 33.468819723103095\nIteration 4907: Loss = 33.46737660232277\nIteration 4908: Loss = 33.46593369239263\nIteration 4909: Loss = 33.464490993123\nIteration 4910: Loss = 33.46304850432442\nIteration 4911: Loss = 33.461606225807714\nIteration 4912: Loss = 33.46016415738391\nIteration 4913: Loss = 33.45872229886429\nIteration 4914: Loss = 33.45728065006037\nIteration 4915: Loss = 33.45583921078392\nIteration 4916: Loss = 33.45439798084692\nIteration 4917: Loss = 33.45295696006161\nIteration 4918: Loss = 33.45151614824046\nIteration 4919: Loss = 33.450075545196185\nIteration 4920: Loss = 33.44863515074173\nIteration 4921: Loss = 33.44719496469027\nIteration 4922: Loss = 33.445754986855235\nIteration 4923: Loss = 33.44431521705028\nIteration 4924: Loss = 33.44287565508929\nIteration 4925: Loss = 33.441436300786414\nIteration 4926: Loss = 33.43999715395599\nIteration 4927: Loss = 33.43855821441264\nIteration 4928: Loss = 33.437119481971166\nIteration 4929: Loss = 33.43568095644667\nIteration 4930: Loss = 33.434242637654435\nIteration 4931: Loss = 33.43280452540999\nIteration 4932: Loss = 33.431366619529136\nIteration 4933: Loss = 33.42992891982785\nIteration 4934: Loss = 33.42849142612238\nIteration 4935: Loss = 33.42705413822917\nIteration 4936: Loss = 33.42561705596495\nIteration 4937: Loss = 33.42418017914663\nIteration 4938: Loss = 33.422743507591406\nIteration 4939: Loss = 33.421307041116634\nIteration 4940: Loss = 33.41987077953998\nIteration 4941: Loss = 33.41843472267926\nIteration 4942: Loss = 33.41699887035261\nIteration 4943: Loss = 33.41556322237832\nIteration 4944: Loss = 33.41412777857494\nIteration 4945: Loss = 33.41269253876125\nIteration 4946: Loss = 33.411257502756264\nIteration 4947: Loss = 33.40982267037921\nIteration 4948: Loss = 33.40838804144956\nIteration 4949: Loss = 33.40695361578702\nIteration 4950: Loss = 33.40551939321148\nIteration 4951: Loss = 33.404085373543126\nIteration 4952: Loss = 33.40265155660231\nIteration 4953: Loss = 33.401217942209655\nIteration 4954: Loss = 33.39978453018597\nIteration 4955: Loss = 33.39835132035233\nIteration 4956: Loss = 33.39691831253003\nIteration 4957: Loss = 33.395485506540574\nIteration 4958: Loss = 33.39405290220569\nIteration 4959: Loss = 33.39262049934733\nIteration 4960: Loss = 33.39118829778771\nIteration 4961: Loss = 33.38975629734923\nIteration 4962: Loss = 33.388324497854526\nIteration 4963: Loss = 33.38689289912646\nIteration 4964: Loss = 33.38546150098813\nIteration 4965: Loss = 33.38403030326282\nIteration 4966: Loss = 33.382599305774086\nIteration 4967: Loss = 33.38116850834567\nIteration 4968: Loss = 33.379737910801566\nIteration 4969: Loss = 33.378307512965954\nIteration 4970: Loss = 33.37687731466328\nIteration 4971: Loss = 33.37544731571818\nIteration 4972: Loss = 33.374017515955536\nIteration 4973: Loss = 33.37258791520042\nIteration 4974: Loss = 33.37115851327814\nIteration 4975: Loss = 33.36972931001423\nIteration 4976: Loss = 33.36830030523445\nIteration 4977: Loss = 33.36687149876478\nIteration 4978: Loss = 33.365442890431396\nIteration 4979: Loss = 33.364014480060725\nIteration 4980: Loss = 33.36258626747938\nIteration 4981: Loss = 33.36115825251422\nIteration 4982: Loss = 33.35973043499232\nIteration 4983: Loss = 33.35830281474097\nIteration 4984: Loss = 33.35687539158767\nIteration 4985: Loss = 33.355448165360144\nIteration 4986: Loss = 33.354021135886335\nIteration 4987: Loss = 33.35259430299441\nIteration 4988: Loss = 33.35116766651274\nIteration 4989: Loss = 33.349741226269906\nIteration 4990: Loss = 33.34831498209474\nIteration 4991: Loss = 33.346888933816274\nIteration 4992: Loss = 33.34546308126373\nIteration 4993: Loss = 33.344037424266574\nIteration 4994: Loss = 33.342611962654495\nIteration 4995: Loss = 33.341186696257346\nIteration 4996: Loss = 33.339761624905265\nIteration 4997: Loss = 33.33833674842857\nIteration 4998: Loss = 33.33691206665778\nIteration 4999: Loss = 33.33548757942365\nIteration 5000: Loss = 33.33406328655714\nIteration 5001: Loss = 33.33263918788941\nIteration 5002: Loss = 33.331215283251886\nIteration 5003: Loss = 33.329791572476154\nIteration 5004: Loss = 33.328368055394\nIteration 5005: Loss = 33.32694473183749\nIteration 5006: Loss = 33.32552160163884\nIteration 5007: Loss = 33.324098664630505\nIteration 5008: Loss = 33.32267592064515\nIteration 5009: Loss = 33.321253369515645\nIteration 5010: Loss = 33.319831011075074\nIteration 5011: Loss = 33.318408845156746\nIteration 5012: Loss = 33.31698687159415\nIteration 5013: Loss = 33.315565090221014\nIteration 5014: Loss = 33.31414350087125\nIteration 5015: Loss = 33.31272210337902\nIteration 5016: Loss = 33.31130089757865\nIteration 5017: Loss = 33.30987988330469\nIteration 5018: Loss = 33.308459060391925\nIteration 5019: Loss = 33.3070384286753\nIteration 5020: Loss = 33.305617987990026\nIteration 5021: Loss = 33.30419773817147\nIteration 5022: Loss = 33.30277767905523\nIteration 5023: Loss = 33.301357810477114\nIteration 5024: Loss = 33.29993813227315\nIteration 5025: Loss = 33.29851864427953\nIteration 5026: Loss = 33.297099346332686\nIteration 5027: Loss = 33.29568023826926\nIteration 5028: Loss = 33.294261319926086\nIteration 5029: Loss = 33.2928425911402\nIteration 5030: Loss = 33.291424051748855\nIteration 5031: Loss = 33.29000570158952\nIteration 5032: Loss = 33.28858754049983\nIteration 5033: Loss = 33.28716956831767\nIteration 5034: Loss = 33.285751784881114\nIteration 5035: Loss = 33.284334190028396\nIteration 5036: Loss = 33.28291678359805\nIteration 5037: Loss = 33.28149956542871\nIteration 5038: Loss = 33.2800825353593\nIteration 5039: Loss = 33.278665693228874\nIteration 5040: Loss = 33.277249038876754\nIteration 5041: Loss = 33.27583257214242\nIteration 5042: Loss = 33.27441629286557\nIteration 5043: Loss = 33.273000200886095\nIteration 5044: Loss = 33.2715842960441\nIteration 5045: Loss = 33.270168578179906\nIteration 5046: Loss = 33.268753047134\nIteration 5047: Loss = 33.26733770274708\nIteration 5048: Loss = 33.26592254486007\nIteration 5049: Loss = 33.26450757331407\nIteration 5050: Loss = 33.26309278795039\nIteration 5051: Loss = 33.26167818861053\nIteration 5052: Loss = 33.26026377513621\nIteration 5053: Loss = 33.25884954736934\nIteration 5054: Loss = 33.257435505152\nIteration 5055: Loss = 33.25602164832652\nIteration 5056: Loss = 33.25460797673539\nIteration 5057: Loss = 33.25319449022133\nIteration 5058: Loss = 33.25178118862723\nIteration 5059: Loss = 33.250368071796196\nIteration 5060: Loss = 33.248955139571514\nIteration 5061: Loss = 33.24754239179669\nIteration 5062: Loss = 33.246129828315404\nIteration 5063: Loss = 33.244717448971564\nIteration 5064: Loss = 33.243305253609236\nIteration 5065: Loss = 33.24189324207272\nIteration 5066: Loss = 33.240481414206485\nIteration 5067: Loss = 33.239069769855206\nIteration 5068: Loss = 33.237658308863764\nIteration 5069: Loss = 33.236247031077205\nIteration 5070: Loss = 33.23483593634082\nIteration 5071: Loss = 33.233425024500036\nIteration 5072: Loss = 33.232014295400525\nIteration 5073: Loss = 33.23060374888812\nIteration 5074: Loss = 33.22919338480887\nIteration 5075: Loss = 33.227783203009004\nIteration 5076: Loss = 33.22637320333496\nIteration 5077: Loss = 33.22496338563333\nIteration 5078: Loss = 33.22355374975097\nIteration 5079: Loss = 33.22214429553486\nIteration 5080: Loss = 33.22073502283221\nIteration 5081: Loss = 33.21932593149042\nIteration 5082: Loss = 33.21791702135705\nIteration 5083: Loss = 33.216508292279904\nIteration 5084: Loss = 33.215099744106936\nIteration 5085: Loss = 33.21369137668632\nIteration 5086: Loss = 33.21228318986639\nIteration 5087: Loss = 33.21087518349571\nIteration 5088: Loss = 33.209467357422994\nIteration 5089: Loss = 33.20805971149717\nIteration 5090: Loss = 33.20665224556737\nIteration 5091: Loss = 33.20524495948287\nIteration 5092: Loss = 33.20383785309319\nIteration 5093: Loss = 33.202430926248\nIteration 5094: Loss = 33.20102417879718\nIteration 5095: Loss = 33.199617610590785\nIteration 5096: Loss = 33.19821122147907\nIteration 5097: Loss = 33.19680501131248\nIteration 5098: Loss = 33.19539897994163\nIteration 5099: Loss = 33.19399312721734\nIteration 5100: Loss = 33.19258745299062\nIteration 5101: Loss = 33.191181957112654\nIteration 5102: Loss = 33.189776639434825\nIteration 5103: Loss = 33.18837149980871\nIteration 5104: Loss = 33.18696653808603\nIteration 5105: Loss = 33.18556175411876\nIteration 5106: Loss = 33.184157147759\nIteration 5107: Loss = 33.18275271885908\nIteration 5108: Loss = 33.18134846727148\nIteration 5109: Loss = 33.1799443928489\nIteration 5110: Loss = 33.17854049544419\nIteration 5111: Loss = 33.177136774910416\nIteration 5112: Loss = 33.17573323110082\nIteration 5113: Loss = 33.17432986386882\nIteration 5114: Loss = 33.17292667306802\nIteration 5115: Loss = 33.17152365855221\nIteration 5116: Loss = 33.17012082017539\nIteration 5117: Loss = 33.16871815779169\nIteration 5118: Loss = 33.167315671255466\nIteration 5119: Loss = 33.165913360421236\nIteration 5120: Loss = 33.16451122514372\nIteration 5121: Loss = 33.16310926527782\nIteration 5122: Loss = 33.16170748067859\nIteration 5123: Loss = 33.16030587120129\nIteration 5124: Loss = 33.15890443670137\nIteration 5125: Loss = 33.15750317703444\nIteration 5126: Loss = 33.15610209205631\nIteration 5127: Loss = 33.15470118162296\nIteration 5128: Loss = 33.153300445590574\nIteration 5129: Loss = 33.15189988381546\nIteration 5130: Loss = 33.15049949615417\nIteration 5131: Loss = 33.14909928246342\nIteration 5132: Loss = 33.14769924260006\nIteration 5133: Loss = 33.1462993764212\nIteration 5134: Loss = 33.14489968378406\nIteration 5135: Loss = 33.143500164546076\nIteration 5136: Loss = 33.142100818564856\nIteration 5137: Loss = 33.14070164569818\nIteration 5138: Loss = 33.13930264580401\nIteration 5139: Loss = 33.13790381874048\nIteration 5140: Loss = 33.136505164365936\nIteration 5141: Loss = 33.135106682538854\nIteration 5142: Loss = 33.133708373117926\nIteration 5143: Loss = 33.13231023596198\nIteration 5144: Loss = 33.130912270930075\nIteration 5145: Loss = 33.12951447788141\nIteration 5146: Loss = 33.12811685667537\nIteration 5147: Loss = 33.126719407171514\nIteration 5148: Loss = 33.12532212922958\nIteration 5149: Loss = 33.123925022709486\nIteration 5150: Loss = 33.12252808747133\nIteration 5151: Loss = 33.12113132337538\nIteration 5152: Loss = 33.119734730282055\nIteration 5153: Loss = 33.118338308051996\nIteration 5154: Loss = 33.11694205654598\nIteration 5155: Loss = 33.115545975625004\nIteration 5156: Loss = 33.11415006515017\nIteration 5157: Loss = 33.112754324982824\nIteration 5158: Loss = 33.111358754984444\nIteration 5159: Loss = 33.1099633550167\nIteration 5160: Loss = 33.10856812494142\nIteration 5161: Loss = 33.10717306462064\nIteration 5162: Loss = 33.105778173916526\nIteration 5163: Loss = 33.10438345269144\nIteration 5164: Loss = 33.102988900807915\nIteration 5165: Loss = 33.10159451812864\nIteration 5166: Loss = 33.10020030451653\nIteration 5167: Loss = 33.09880625983462\nIteration 5168: Loss = 33.0974123839461\nIteration 5169: Loss = 33.09601867671439\nIteration 5170: Loss = 33.094625138003046\nIteration 5171: Loss = 33.0932317676758\nIteration 5172: Loss = 33.09183856559658\nIteration 5173: Loss = 33.09044553162945\nIteration 5174: Loss = 33.08905266563864\nIteration 5175: Loss = 33.08765996748859\nIteration 5176: Loss = 33.08626743704389\nIteration 5177: Loss = 33.08487507416929\nIteration 5178: Loss = 33.08348287872971\nIteration 5179: Loss = 33.08209085059026\nIteration 5180: Loss = 33.08069898961621\nIteration 5181: Loss = 33.07930729567299\nIteration 5182: Loss = 33.077915768626205\nIteration 5183: Loss = 33.07652440834163\nIteration 5184: Loss = 33.0751332146852\nIteration 5185: Loss = 33.073742187523045\nIteration 5186: Loss = 33.07235132672143\nIteration 5187: Loss = 33.0709606321468\nIteration 5188: Loss = 33.06957010366576\nIteration 5189: Loss = 33.068179741145116\nIteration 5190: Loss = 33.06678954445179\nIteration 5191: Loss = 33.06539951345292\nIteration 5192: Loss = 33.06400964801577\nIteration 5193: Loss = 33.0626199480078\nIteration 5194: Loss = 33.06123041329661\nIteration 5195: Loss = 33.05984104375001\nIteration 5196: Loss = 33.058451839235914\nIteration 5197: Loss = 33.05706279962244\nIteration 5198: Loss = 33.0556739247779\nIteration 5199: Loss = 33.05428521457069\nIteration 5200: Loss = 33.052896668869444\nIteration 5201: Loss = 33.05150828754292\nIteration 5202: Loss = 33.050120070460075\nIteration 5203: Loss = 33.048732017489996\nIteration 5204: Loss = 33.04734412850195\nIteration 5205: Loss = 33.04595640336536\nIteration 5206: Loss = 33.04456884194984\nIteration 5207: Loss = 33.04318144412512\nIteration 5208: Loss = 33.04179420976114\nIteration 5209: Loss = 33.04040713872798\nIteration 5210: Loss = 33.03902023089588\nIteration 5211: Loss = 33.03763348613524\nIteration 5212: Loss = 33.036246904316656\nIteration 5213: Loss = 33.03486048531083\nIteration 5214: Loss = 33.03347422898869\nIteration 5215: Loss = 33.032088135221265\nIteration 5216: Loss = 33.0307022038798\nIteration 5217: Loss = 33.02931643483564\nIteration 5218: Loss = 33.027930827960354\nIteration 5219: Loss = 33.02654538312564\nIteration 5220: Loss = 33.02516010020336\nIteration 5221: Loss = 33.02377497906552\nIteration 5222: Loss = 33.02239001958433\nIteration 5223: Loss = 33.02100522163211\nIteration 5224: Loss = 33.019620585081384\nIteration 5225: Loss = 33.0182361098048\nIteration 5226: Loss = 33.016851795675194\nIteration 5227: Loss = 33.01546764256555\nIteration 5228: Loss = 33.01408365034899\nIteration 5229: Loss = 33.01269981889882\nIteration 5230: Loss = 33.01131614808852\nIteration 5231: Loss = 33.009932637791685\nIteration 5232: Loss = 33.0085492878821\nIteration 5233: Loss = 33.00716609823369\nIteration 5234: Loss = 33.005783068720554\nIteration 5235: Loss = 33.00440019921694\nIteration 5236: Loss = 33.003017489597255\nIteration 5237: Loss = 33.00163493973606\nIteration 5238: Loss = 33.000252549508076\nIteration 5239: Loss = 32.9988703187882\nIteration 5240: Loss = 32.99748824745144\nIteration 5241: Loss = 32.996106335373\nIteration 5242: Loss = 32.994724582428226\nIteration 5243: Loss = 32.993342988492614\nIteration 5244: Loss = 32.99196155344183\nIteration 5245: Loss = 32.99058027715169\nIteration 5246: Loss = 32.98919915949816\nIteration 5247: Loss = 32.98781820035737\nIteration 5248: Loss = 32.9864373996056\nIteration 5249: Loss = 32.985056757119274\nIteration 5250: Loss = 32.983676272775\nIteration 5251: Loss = 32.982295946449504\nIteration 5252: Loss = 32.98091577801969\nIteration 5253: Loss = 32.97953576736262\nIteration 5254: Loss = 32.9781559143555\nIteration 5255: Loss = 32.97677621887568\nIteration 5256: Loss = 32.975396680800664\nIteration 5257: Loss = 32.97401730000815\nIteration 5258: Loss = 32.97263807637593\nIteration 5259: Loss = 32.97125900978199\nIteration 5260: Loss = 32.96988010010445\nIteration 5261: Loss = 32.968501347221576\nIteration 5262: Loss = 32.96712275101183\nIteration 5263: Loss = 32.96574431135376\nIteration 5264: Loss = 32.96436602812611\nIteration 5265: Loss = 32.96298790120778\nIteration 5266: Loss = 32.96160993047779\nIteration 5267: Loss = 32.96023211581534\nIteration 5268: Loss = 32.95885445709975\nIteration 5269: Loss = 32.957476954210534\nIteration 5270: Loss = 32.956099607027305\nIteration 5271: Loss = 32.954722415429885\nIteration 5272: Loss = 32.9533453792982\nIteration 5273: Loss = 32.951968498512336\nIteration 5274: Loss = 32.950591772952535\nIteration 5275: Loss = 32.94921520249921\nIteration 5276: Loss = 32.94783878703288\nIteration 5277: Loss = 32.946462526434225\nIteration 5278: Loss = 32.945086420584104\nIteration 5279: Loss = 32.9437104693635\nIteration 5280: Loss = 32.94233467265355\nIteration 5281: Loss = 32.94095903033553\nIteration 5282: Loss = 32.9395835422909\nIteration 5283: Loss = 32.9382082084012\nIteration 5284: Loss = 32.936833028548186\nIteration 5285: Loss = 32.935458002613736\nIteration 5286: Loss = 32.93408313047987\nIteration 5287: Loss = 32.93270841202875\nIteration 5288: Loss = 32.931333847142724\nIteration 5289: Loss = 32.92995943570423\nIteration 5290: Loss = 32.928585177595885\nIteration 5291: Loss = 32.92721107270046\nIteration 5292: Loss = 32.925837120900844\nIteration 5293: Loss = 32.92446332208013\nIteration 5294: Loss = 32.92308967612147\nIteration 5295: Loss = 32.92171618290823\nIteration 5296: Loss = 32.9203428423239\nIteration 5297: Loss = 32.9189696542521\nIteration 5298: Loss = 32.917596618576624\nIteration 5299: Loss = 32.91622373518141\nIteration 5300: Loss = 32.91485100395051\nIteration 5301: Loss = 32.913478424768144\nIteration 5302: Loss = 32.91210599751867\nIteration 5303: Loss = 32.91073372208661\nIteration 5304: Loss = 32.90936159835658\nIteration 5305: Loss = 32.90798962621341\nIteration 5306: Loss = 32.906617805542005\nIteration 5307: Loss = 32.90524613622746\nIteration 5308: Loss = 32.903874618155\nIteration 5309: Loss = 32.90250325120999\nIteration 5310: Loss = 32.90113203527793\nIteration 5311: Loss = 32.89976097024449\nIteration 5312: Loss = 32.89839005599545\nIteration 5313: Loss = 32.89701929241675\nIteration 5314: Loss = 32.89564867939449\nIteration 5315: Loss = 32.894278216814854\nIteration 5316: Loss = 32.892907904564254\nIteration 5317: Loss = 32.89153774252916\nIteration 5318: Loss = 32.890167730596225\nIteration 5319: Loss = 32.88879786865225\nIteration 5320: Loss = 32.88742815658415\nIteration 5321: Loss = 32.886058594279014\nIteration 5322: Loss = 32.88468918162404\nIteration 5323: Loss = 32.88331991850659\nIteration 5324: Loss = 32.88195080481414\nIteration 5325: Loss = 32.88058184043435\nIteration 5326: Loss = 32.87921302525497\nIteration 5327: Loss = 32.87784435916393\nIteration 5328: Loss = 32.87647584204927\nIteration 5329: Loss = 32.87510747379919\nIteration 5330: Loss = 32.87373925430202\nIteration 5331: Loss = 32.87237118344625\nIteration 5332: Loss = 32.87100326112045\nIteration 5333: Loss = 32.86963548721341\nIteration 5334: Loss = 32.868267861613994\nIteration 5335: Loss = 32.86690038421124\nIteration 5336: Loss = 32.86553305489429\nIteration 5337: Loss = 32.86416587355249\nIteration 5338: Loss = 32.86279884007524\nIteration 5339: Loss = 32.86143195435214\nIteration 5340: Loss = 32.8600652162729\nIteration 5341: Loss = 32.85869862572737\nIteration 5342: Loss = 32.85733218260556\nIteration 5343: Loss = 32.85596588679758\nIteration 5344: Loss = 32.854599738193684\nIteration 5345: Loss = 32.8532337366843\nIteration 5346: Loss = 32.851867882159944\nIteration 5347: Loss = 32.85050217451131\nIteration 5348: Loss = 32.8491366136292\nIteration 5349: Loss = 32.847771199404555\nIteration 5350: Loss = 32.84640593172847\nIteration 5351: Loss = 32.84504081049215\nIteration 5352: Loss = 32.84367583558695\nIteration 5353: Loss = 32.842311006904374\nIteration 5354: Loss = 32.84094632433603\nIteration 5355: Loss = 32.83958178777368\nIteration 5356: Loss = 32.838217397109226\nIteration 5357: Loss = 32.83685315223469\nIteration 5358: Loss = 32.835489053042224\nIteration 5359: Loss = 32.83412509942414\nIteration 5360: Loss = 32.83276129127287\nIteration 5361: Loss = 32.83139762848098\nIteration 5362: Loss = 32.83003411094115\nIteration 5363: Loss = 32.82867073854623\nIteration 5364: Loss = 32.82730751118918\nIteration 5365: Loss = 32.8259444287631\nIteration 5366: Loss = 32.82458149116123\nIteration 5367: Loss = 32.82321869827692\nIteration 5368: Loss = 32.821856050003674\nIteration 5369: Loss = 32.82049354623512\nIteration 5370: Loss = 32.81913118686503\nIteration 5371: Loss = 32.81776897178728\nIteration 5372: Loss = 32.81640690089592\nIteration 5373: Loss = 32.815044974085076\nIteration 5374: Loss = 32.813683191249076\nIteration 5375: Loss = 32.81232155228232\nIteration 5376: Loss = 32.810960057079356\nIteration 5377: Loss = 32.80959870553488\nIteration 5378: Loss = 32.80823749754371\nIteration 5379: Loss = 32.80687643300078\nIteration 5380: Loss = 32.805515511801175\nIteration 5381: Loss = 32.80415473384009\nIteration 5382: Loss = 32.80279409901288\nIteration 5383: Loss = 32.801433607215\nIteration 5384: Loss = 32.800073258342046\nIteration 5385: Loss = 32.798713052289756\nIteration 5386: Loss = 32.797352988953975\nIteration 5387: Loss = 32.7959930682307\nIteration 5388: Loss = 32.79463329001604\nIteration 5389: Loss = 32.79327365420624\nIteration 5390: Loss = 32.79191416069767\nIteration 5391: Loss = 32.79055480938684\nIteration 5392: Loss = 32.78919560017039\nIteration 5393: Loss = 32.78783653294505\nIteration 5394: Loss = 32.786477607607736\nIteration 5395: Loss = 32.78511882405545\nIteration 5396: Loss = 32.78376018218533\nIteration 5397: Loss = 32.782401681894676\nIteration 5398: Loss = 32.78104332308085\nIteration 5399: Loss = 32.77968510564142\nIteration 5400: Loss = 32.778327029474\nIteration 5401: Loss = 32.7769690944764\nIteration 5402: Loss = 32.77561130054651\nIteration 5403: Loss = 32.77425364758236\nIteration 5404: Loss = 32.772896135482135\nIteration 5405: Loss = 32.77153876414412\nIteration 5406: Loss = 32.77018153346671\nIteration 5407: Loss = 32.768824443348464\nIteration 5408: Loss = 32.76746749368804\nIteration 5409: Loss = 32.76611068438423\nIteration 5410: Loss = 32.76475401533596\nIteration 5411: Loss = 32.76339748644226\nIteration 5412: Loss = 32.762041097602314\nIteration 5413: Loss = 32.76068484871542\nIteration 5414: Loss = 32.75932873968097\nIteration 5415: Loss = 32.75797277039853\nIteration 5416: Loss = 32.756616940767756\nIteration 5417: Loss = 32.75526125068845\nIteration 5418: Loss = 32.753905700060535\nIteration 5419: Loss = 32.752550288784036\nIteration 5420: Loss = 32.75119501675914\nIteration 5421: Loss = 32.749839883886125\nIteration 5422: Loss = 32.7484848900654\nIteration 5423: Loss = 32.74713003519751\nIteration 5424: Loss = 32.74577531918312\nIteration 5425: Loss = 32.744420741922994\nIteration 5426: Loss = 32.74306630331806\nIteration 5427: Loss = 32.74171200326933\nIteration 5428: Loss = 32.74035784167797\nIteration 5429: Loss = 32.73900381844525\nIteration 5430: Loss = 32.737649933472575\nIteration 5431: Loss = 32.73629618666145\nIteration 5432: Loss = 32.73494257791352\nIteration 5433: Loss = 32.73358910713057\nIteration 5434: Loss = 32.73223577421446\nIteration 5435: Loss = 32.7308825790672\nIteration 5436: Loss = 32.72952952159094\nIteration 5437: Loss = 32.72817660168791\nIteration 5438: Loss = 32.72682381926049\nIteration 5439: Loss = 32.725471174211165\nIteration 5440: Loss = 32.72411866644256\nIteration 5441: Loss = 32.72276629585739\nIteration 5442: Loss = 32.72141406235854\nIteration 5443: Loss = 32.720061965848956\nIteration 5444: Loss = 32.71871000623176\nIteration 5445: Loss = 32.717358183410134\nIteration 5446: Loss = 32.71600649728744\nIteration 5447: Loss = 32.71465494776713\nIteration 5448: Loss = 32.713303534752754\nIteration 5449: Loss = 32.711952258148045\nIteration 5450: Loss = 32.71060111785678\nIteration 5451: Loss = 32.70925011378293\nIteration 5452: Loss = 32.70789924583051\nIteration 5453: Loss = 32.70654851390372\nIteration 5454: Loss = 32.70519791790682\nIteration 5455: Loss = 32.703847457744246\nIteration 5456: Loss = 32.702497133320506\nIteration 5457: Loss = 32.70114694454025\nIteration 5458: Loss = 32.69979689130824\nIteration 5459: Loss = 32.69844697352935\nIteration 5460: Loss = 32.6970971911086\nIteration 5461: Loss = 32.695747543951086\nIteration 5462: Loss = 32.694398031962045\nIteration 5463: Loss = 32.69304865504683\nIteration 5464: Loss = 32.6916994131109\nIteration 5465: Loss = 32.690350306059855\nIteration 5466: Loss = 32.689001333799396\nIteration 5467: Loss = 32.68765249623532\nIteration 5468: Loss = 32.686303793273595\nIteration 5469: Loss = 32.68495522482025\nIteration 5470: Loss = 32.68360679078146\nIteration 5471: Loss = 32.68225849106349\nIteration 5472: Loss = 32.680910325572775\nIteration 5473: Loss = 32.6795622942158\nIteration 5474: Loss = 32.678214396899214\nIteration 5475: Loss = 32.676866633529755\nIteration 5476: Loss = 32.6755190040143\nIteration 5477: Loss = 32.674171508259796\nIteration 5478: Loss = 32.67282414617336\nIteration 5479: Loss = 32.671476917662204\nIteration 5480: Loss = 32.670129822633626\nIteration 5481: Loss = 32.66878286099509\nIteration 5482: Loss = 32.66743603265412\nIteration 5483: Loss = 32.6660893375184\nIteration 5484: Loss = 32.664742775495704\nIteration 5485: Loss = 32.66339634649393\nIteration 5486: Loss = 32.66205005042109\nIteration 5487: Loss = 32.660703887185285\nIteration 5488: Loss = 32.65935785669476\nIteration 5489: Loss = 32.65801195885788\nIteration 5490: Loss = 32.65666619358309\nIteration 5491: Loss = 32.65532056077896\nIteration 5492: Loss = 32.65397506035419\nIteration 5493: Loss = 32.65262969221757\nIteration 5494: Loss = 32.651284456278034\nIteration 5495: Loss = 32.649939352444584\nIteration 5496: Loss = 32.64859438062638\nIteration 5497: Loss = 32.64724954073265\nIteration 5498: Loss = 32.64590483267277\nIteration 5499: Loss = 32.64456025635621\nIteration 5500: Loss = 32.64321581169258\nIteration 5501: Loss = 32.64187149859153\nIteration 5502: Loss = 32.64052731696291\nIteration 5503: Loss = 32.63918326671662\nIteration 5504: Loss = 32.63783934776271\nIteration 5505: Loss = 32.63649556001132\nIteration 5506: Loss = 32.63515190337269\nIteration 5507: Loss = 32.63380837775719\nIteration 5508: Loss = 32.632464983075316\nIteration 5509: Loss = 32.631121719237626\nIteration 5510: Loss = 32.62977858615484\nIteration 5511: Loss = 32.628435583737755\nIteration 5512: Loss = 32.62709271189729\nIteration 5513: Loss = 32.62574997054447\nIteration 5514: Loss = 32.62440735959044\nIteration 5515: Loss = 32.62306487894643\nIteration 5516: Loss = 32.62172252852381\nIteration 5517: Loss = 32.620380308234054\nIteration 5518: Loss = 32.61903821798872\nIteration 5519: Loss = 32.61769625769949\nIteration 5520: Loss = 32.616354427278175\nIteration 5521: Loss = 32.61501272663667\nIteration 5522: Loss = 32.61367115568698\nIteration 5523: Loss = 32.61232971434123\nIteration 5524: Loss = 32.61098840251165\nIteration 5525: Loss = 32.60964722011056\nIteration 5526: Loss = 32.60830616705042\nIteration 5527: Loss = 32.60696524324378\nIteration 5528: Loss = 32.605624448603294\nIteration 5529: Loss = 32.60428378304174\nIteration 5530: Loss = 32.60294324647197\nIteration 5531: Loss = 32.601602838807004\nIteration 5532: Loss = 32.6002625599599\nIteration 5533: Loss = 32.598922409843865\nIteration 5534: Loss = 32.59758238837221\nIteration 5535: Loss = 32.596242495458334\nIteration 5536: Loss = 32.59490273101577\nIteration 5537: Loss = 32.59356309495813\n",
      "Iteration 5538: Loss = 32.59222358719914\nIteration 5539: Loss = 32.590884207652664\nIteration 5540: Loss = 32.58954495623261\nIteration 5541: Loss = 32.58820583285304\nIteration 5542: Loss = 32.58686683742813\nIteration 5543: Loss = 32.58552796987212\nIteration 5544: Loss = 32.584189230099376\nIteration 5545: Loss = 32.58285061802438\nIteration 5546: Loss = 32.5815121335617\nIteration 5547: Loss = 32.580173776626026\nIteration 5548: Loss = 32.57883554713215\nIteration 5549: Loss = 32.577497444994954\nIteration 5550: Loss = 32.57615947012944\nIteration 5551: Loss = 32.574821622450706\nIteration 5552: Loss = 32.57348390187397\nIteration 5553: Loss = 32.572146308314544\nIteration 5554: Loss = 32.57080884168782\nIteration 5555: Loss = 32.56947150190935\nIteration 5556: Loss = 32.56813428889474\nIteration 5557: Loss = 32.566797202559705\nIteration 5558: Loss = 32.5654602428201\nIteration 5559: Loss = 32.56412340959186\nIteration 5560: Loss = 32.56278670279101\nIteration 5561: Loss = 32.561450122333696\nIteration 5562: Loss = 32.56011366813616\nIteration 5563: Loss = 32.55877734011476\nIteration 5564: Loss = 32.557441138185936\nIteration 5565: Loss = 32.556105062266255\nIteration 5566: Loss = 32.55476911227235\nIteration 5567: Loss = 32.55343328812101\nIteration 5568: Loss = 32.55209758972908\nIteration 5569: Loss = 32.55076201701352\nIteration 5570: Loss = 32.54942656989141\nIteration 5571: Loss = 32.5480912482799\nIteration 5572: Loss = 32.54675605209628\nIteration 5573: Loss = 32.545420981257905\nIteration 5574: Loss = 32.544086035682255\nIteration 5575: Loss = 32.54275121528691\nIteration 5576: Loss = 32.54141651998953\nIteration 5577: Loss = 32.540081949707904\nIteration 5578: Loss = 32.53874750435991\nIteration 5579: Loss = 32.537413183863535\nIteration 5580: Loss = 32.536078988136836\nIteration 5581: Loss = 32.534744917098024\nIteration 5582: Loss = 32.533410970665344\nIteration 5583: Loss = 32.53207714875722\nIteration 5584: Loss = 32.53074345129209\nIteration 5585: Loss = 32.529409878188574\nIteration 5586: Loss = 32.52807642936533\nIteration 5587: Loss = 32.52674310474115\nIteration 5588: Loss = 32.52540990423493\nIteration 5589: Loss = 32.524076827765604\nIteration 5590: Loss = 32.522743875252324\nIteration 5591: Loss = 32.521411046614205\nIteration 5592: Loss = 32.52007834177057\nIteration 5593: Loss = 32.51874576064078\nIteration 5594: Loss = 32.51741330314431\nIteration 5595: Loss = 32.51608096920074\nIteration 5596: Loss = 32.514748758729766\nIteration 5597: Loss = 32.513416671651136\nIteration 5598: Loss = 32.51208470788474\nIteration 5599: Loss = 32.51075286735052\nIteration 5600: Loss = 32.50942114996858\nIteration 5601: Loss = 32.50808955565908\nIteration 5602: Loss = 32.50675808434227\nIteration 5603: Loss = 32.505426735938535\nIteration 5604: Loss = 32.504095510368316\nIteration 5605: Loss = 32.50276440755218\nIteration 5606: Loss = 32.501433427410795\nIteration 5607: Loss = 32.5001025698649\nIteration 5608: Loss = 32.498771834835345\nIteration 5609: Loss = 32.4974412222431\nIteration 5610: Loss = 32.49611073200917\nIteration 5611: Loss = 32.494780364054726\nIteration 5612: Loss = 32.493450118301006\nIteration 5613: Loss = 32.49211999466934\nIteration 5614: Loss = 32.490789993081165\nIteration 5615: Loss = 32.489460113457994\nIteration 5616: Loss = 32.488130355721474\nIteration 5617: Loss = 32.4868007197933\nIteration 5618: Loss = 32.48547120559531\nIteration 5619: Loss = 32.484141813049405\nIteration 5620: Loss = 32.4828125420776\nIteration 5621: Loss = 32.481483392602\nIteration 5622: Loss = 32.48015436454479\nIteration 5623: Loss = 32.47882545782828\nIteration 5624: Loss = 32.477496672374855\nIteration 5625: Loss = 32.47616800810699\nIteration 5626: Loss = 32.47483946494727\nIteration 5627: Loss = 32.473511042818394\nIteration 5628: Loss = 32.472182741643095\nIteration 5629: Loss = 32.470854561344254\nIteration 5630: Loss = 32.46952650184484\nIteration 5631: Loss = 32.46819856306789\nIteration 5632: Loss = 32.46687074493654\nIteration 5633: Loss = 32.46554304737407\nIteration 5634: Loss = 32.4642154703038\nIteration 5635: Loss = 32.462888013649135\nIteration 5636: Loss = 32.461560677333615\nIteration 5637: Loss = 32.46023346128087\nIteration 5638: Loss = 32.4589063654146\nIteration 5639: Loss = 32.4575793896586\nIteration 5640: Loss = 32.45625253393678\nIteration 5641: Loss = 32.45492579817313\nIteration 5642: Loss = 32.453599182291704\nIteration 5643: Loss = 32.45227268621674\nIteration 5644: Loss = 32.45094630987244\nIteration 5645: Loss = 32.449620053183224\nIteration 5646: Loss = 32.4482939160735\nIteration 5647: Loss = 32.44696789846783\nIteration 5648: Loss = 32.44564200029087\nIteration 5649: Loss = 32.444316221467325\nIteration 5650: Loss = 32.44299056192204\nIteration 5651: Loss = 32.441665021579915\nIteration 5652: Loss = 32.44033960036597\nIteration 5653: Loss = 32.439014298205294\nIteration 5654: Loss = 32.43768911502309\nIteration 5655: Loss = 32.43636405074463\nIteration 5656: Loss = 32.43503910529528\nIteration 5657: Loss = 32.433714278600526\nIteration 5658: Loss = 32.432389570585904\nIteration 5659: Loss = 32.43106498117708\nIteration 5660: Loss = 32.429740510299766\nIteration 5661: Loss = 32.42841615787982\nIteration 5662: Loss = 32.42709192384314\nIteration 5663: Loss = 32.42576780811575\nIteration 5664: Loss = 32.42444381062374\nIteration 5665: Loss = 32.423119931293314\nIteration 5666: Loss = 32.42179617005073\nIteration 5667: Loss = 32.42047252682239\nIteration 5668: Loss = 32.41914900153472\nIteration 5669: Loss = 32.41782559411429\nIteration 5670: Loss = 32.41650230448774\nIteration 5671: Loss = 32.415179132581805\nIteration 5672: Loss = 32.4138560783233\nIteration 5673: Loss = 32.41253314163913\nIteration 5674: Loss = 32.4112103224563\nIteration 5675: Loss = 32.40988762070189\nIteration 5676: Loss = 32.40856503630308\nIteration 5677: Loss = 32.40724256918714\nIteration 5678: Loss = 32.40592021928142\nIteration 5679: Loss = 32.40459798651336\nIteration 5680: Loss = 32.4032758708105\nIteration 5681: Loss = 32.40195387210045\nIteration 5682: Loss = 32.400631990310934\nIteration 5683: Loss = 32.39931022536973\nIteration 5684: Loss = 32.397988577204735\nIteration 5685: Loss = 32.39666704574392\nIteration 5686: Loss = 32.395345630915344\nIteration 5687: Loss = 32.394024332647156\nIteration 5688: Loss = 32.39270315086759\nIteration 5689: Loss = 32.391382085504965\nIteration 5690: Loss = 32.39006113648771\nIteration 5691: Loss = 32.388740303744314\nIteration 5692: Loss = 32.38741958720336\nIteration 5693: Loss = 32.38609898679351\nIteration 5694: Loss = 32.38477850244355\nIteration 5695: Loss = 32.38345813408231\nIteration 5696: Loss = 32.382137881638734\nIteration 5697: Loss = 32.38081774504183\nIteration 5698: Loss = 32.379497724220705\nIteration 5699: Loss = 32.378177819104565\nIteration 5700: Loss = 32.376858029622674\nIteration 5701: Loss = 32.37553835570441\nIteration 5702: Loss = 32.37421879727923\nIteration 5703: Loss = 32.37289935427665\nIteration 5704: Loss = 32.371580026626305\nIteration 5705: Loss = 32.37026081425792\nIteration 5706: Loss = 32.36894171710126\nIteration 5707: Loss = 32.36762273508623\nIteration 5708: Loss = 32.36630386814278\nIteration 5709: Loss = 32.36498511620097\nIteration 5710: Loss = 32.36366647919094\nIteration 5711: Loss = 32.36234795704291\nIteration 5712: Loss = 32.36102954968718\nIteration 5713: Loss = 32.359711257054165\nIteration 5714: Loss = 32.35839307907429\nIteration 5715: Loss = 32.35707501567817\nIteration 5716: Loss = 32.35575706679643\nIteration 5717: Loss = 32.35443923235979\nIteration 5718: Loss = 32.35312151229908\nIteration 5719: Loss = 32.35180390654519\nIteration 5720: Loss = 32.3504864150291\nIteration 5721: Loss = 32.349169037681904\nIteration 5722: Loss = 32.34785177443471\nIteration 5723: Loss = 32.34653462521878\nIteration 5724: Loss = 32.345217589965436\nIteration 5725: Loss = 32.343900668606054\nIteration 5726: Loss = 32.34258386107214\nIteration 5727: Loss = 32.34126716729527\nIteration 5728: Loss = 32.33995058720706\nIteration 5729: Loss = 32.338634120739286\nIteration 5730: Loss = 32.33731776782375\nIteration 5731: Loss = 32.336001528392345\nIteration 5732: Loss = 32.33468540237706\nIteration 5733: Loss = 32.33336938970997\nIteration 5734: Loss = 32.33205349032321\nIteration 5735: Loss = 32.33073770414902\nIteration 5736: Loss = 32.32942203111972\nIteration 5737: Loss = 32.32810647116769\nIteration 5738: Loss = 32.32679102422542\nIteration 5739: Loss = 32.325475690225474\nIteration 5740: Loss = 32.324160469100484\nIteration 5741: Loss = 32.322845360783184\nIteration 5742: Loss = 32.32153036520637\nIteration 5743: Loss = 32.32021548230294\nIteration 5744: Loss = 32.31890071200585\nIteration 5745: Loss = 32.31758605424817\nIteration 5746: Loss = 32.31627150896302\nIteration 5747: Loss = 32.314957076083616\nIteration 5748: Loss = 32.31364275554324\nIteration 5749: Loss = 32.31232854727529\nIteration 5750: Loss = 32.31101445121321\nIteration 5751: Loss = 32.30970046729054\nIteration 5752: Loss = 32.30838659544089\nIteration 5753: Loss = 32.307072835597985\nIteration 5754: Loss = 32.30575918769557\nIteration 5755: Loss = 32.30444565166753\nIteration 5756: Loss = 32.30313222744778\nIteration 5757: Loss = 32.30181891497036\nIteration 5758: Loss = 32.30050571416936\nIteration 5759: Loss = 32.29919262497898\nIteration 5760: Loss = 32.29787964733345\nIteration 5761: Loss = 32.29656678116713\nIteration 5762: Loss = 32.29525402641442\nIteration 5763: Loss = 32.29394138300985\nIteration 5764: Loss = 32.29262885088797\nIteration 5765: Loss = 32.29131642998345\nIteration 5766: Loss = 32.29000412023104\nIteration 5767: Loss = 32.28869192156553\nIteration 5768: Loss = 32.28737983392182\nIteration 5769: Loss = 32.286067857234904\nIteration 5770: Loss = 32.284755991439816\nIteration 5771: Loss = 32.283444236471695\nIteration 5772: Loss = 32.282132592265754\nIteration 5773: Loss = 32.280821058757276\nIteration 5774: Loss = 32.27950963588163\nIteration 5775: Loss = 32.27819832357426\nIteration 5776: Loss = 32.2768871217707\nIteration 5777: Loss = 32.27557603040654\nIteration 5778: Loss = 32.27426504941747\nIteration 5779: Loss = 32.27295417873924\nIteration 5780: Loss = 32.27164341830769\nIteration 5781: Loss = 32.27033276805873\nIteration 5782: Loss = 32.26902222792837\nIteration 5783: Loss = 32.26771179785265\nIteration 5784: Loss = 32.26640147776773\nIteration 5785: Loss = 32.26509126760984\nIteration 5786: Loss = 32.26378116731527\nIteration 5787: Loss = 32.2624711768204\nIteration 5788: Loss = 32.26116129606169\nIteration 5789: Loss = 32.259851524975666\nIteration 5790: Loss = 32.258541863498955\nIteration 5791: Loss = 32.25723231156822\nIteration 5792: Loss = 32.255922869120226\nIteration 5793: Loss = 32.25461353609182\nIteration 5794: Loss = 32.253304312419914\nIteration 5795: Loss = 32.251995198041506\nIteration 5796: Loss = 32.250686192893646\nIteration 5797: Loss = 32.2493772969135\nIteration 5798: Loss = 32.24806851003828\nIteration 5799: Loss = 32.24675983220528\nIteration 5800: Loss = 32.24545126335187\nIteration 5801: Loss = 32.24414280341549\nIteration 5802: Loss = 32.24283445233368\nIteration 5803: Loss = 32.241526210044036\nIteration 5804: Loss = 32.240218076484226\nIteration 5805: Loss = 32.23891005159199\nIteration 5806: Loss = 32.237602135305174\nIteration 5807: Loss = 32.23629432756167\nIteration 5808: Loss = 32.234986628299445\nIteration 5809: Loss = 32.23367903745657\nIteration 5810: Loss = 32.23237155497114\nIteration 5811: Loss = 32.231064180781374\nIteration 5812: Loss = 32.229756914825536\nIteration 5813: Loss = 32.22844975704199\nIteration 5814: Loss = 32.22714270736915\nIteration 5815: Loss = 32.225835765745515\nIteration 5816: Loss = 32.224528932109656\nIteration 5817: Loss = 32.22322220640022\nIteration 5818: Loss = 32.22191558855593\nIteration 5819: Loss = 32.22060907851557\nIteration 5820: Loss = 32.21930267621804\nIteration 5821: Loss = 32.21799638160224\nIteration 5822: Loss = 32.21669019460721\nIteration 5823: Loss = 32.215384115172036\nIteration 5824: Loss = 32.214078143235874\nIteration 5825: Loss = 32.21277227873797\nIteration 5826: Loss = 32.21146652161764\nIteration 5827: Loss = 32.210160871814246\nIteration 5828: Loss = 32.20885532926726\nIteration 5829: Loss = 32.2075498939162\nIteration 5830: Loss = 32.2062445657007\nIteration 5831: Loss = 32.20493934456039\nIteration 5832: Loss = 32.20363423043504\nIteration 5833: Loss = 32.20232922326447\nIteration 5834: Loss = 32.20102432298858\nIteration 5835: Loss = 32.19971952954732\nIteration 5836: Loss = 32.198414842880744\nIteration 5837: Loss = 32.19711026292896\nIteration 5838: Loss = 32.19580578963214\nIteration 5839: Loss = 32.19450142293054\nIteration 5840: Loss = 32.1931971627645\nIteration 5841: Loss = 32.191893009074406\nIteration 5842: Loss = 32.19058896180074\nIteration 5843: Loss = 32.18928502088403\nIteration 5844: Loss = 32.187981186264906\nIteration 5845: Loss = 32.18667745788403\nIteration 5846: Loss = 32.185373835682185\nIteration 5847: Loss = 32.184070319600195\nIteration 5848: Loss = 32.182766909578945\nIteration 5849: Loss = 32.18146360555941\nIteration 5850: Loss = 32.180160407482646\nIteration 5851: Loss = 32.178857315289754\nIteration 5852: Loss = 32.177554328921914\nIteration 5853: Loss = 32.1762514483204\nIteration 5854: Loss = 32.17494867342651\nIteration 5855: Loss = 32.17364600418167\nIteration 5856: Loss = 32.17234344052733\nIteration 5857: Loss = 32.17104098240502\nIteration 5858: Loss = 32.16973862975636\nIteration 5859: Loss = 32.16843638252302\nIteration 5860: Loss = 32.16713424064677\nIteration 5861: Loss = 32.16583220406939\nIteration 5862: Loss = 32.164530272732804\nIteration 5863: Loss = 32.16322844657894\nIteration 5864: Loss = 32.16192672554985\nIteration 5865: Loss = 32.16062510958761\nIteration 5866: Loss = 32.159323598634415\nIteration 5867: Loss = 32.15802219263248\nIteration 5868: Loss = 32.156720891524124\nIteration 5869: Loss = 32.1554196952517\nIteration 5870: Loss = 32.15411860375768\nIteration 5871: Loss = 32.15281761698457\nIteration 5872: Loss = 32.15151673487495\nIteration 5873: Loss = 32.15021595737148\nIteration 5874: Loss = 32.14891528441686\nIteration 5875: Loss = 32.1476147159539\nIteration 5876: Loss = 32.14631425192546\nIteration 5877: Loss = 32.14501389227445\nIteration 5878: Loss = 32.143713636943886\nIteration 5879: Loss = 32.142413485876816\nIteration 5880: Loss = 32.14111343901639\nIteration 5881: Loss = 32.139813496305784\nIteration 5882: Loss = 32.1385136576883\nIteration 5883: Loss = 32.13721392310724\nIteration 5884: Loss = 32.135914292506044\nIteration 5885: Loss = 32.13461476582817\nIteration 5886: Loss = 32.13331534301715\nIteration 5887: Loss = 32.132016024016615\nIteration 5888: Loss = 32.13071680877022\nIteration 5889: Loss = 32.12941769722172\nIteration 5890: Loss = 32.12811868931494\nIteration 5891: Loss = 32.12681978499372\nIteration 5892: Loss = 32.125520984202055\nIteration 5893: Loss = 32.124222286883935\nIteration 5894: Loss = 32.12292369298344\nIteration 5895: Loss = 32.12162520244471\nIteration 5896: Loss = 32.12032681521199\nIteration 5897: Loss = 32.11902853122954\nIteration 5898: Loss = 32.11773035044171\nIteration 5899: Loss = 32.11643227279292\nIteration 5900: Loss = 32.11513429822766\nIteration 5901: Loss = 32.11383642669046\nIteration 5902: Loss = 32.11253865812595\nIteration 5903: Loss = 32.111240992478805\nIteration 5904: Loss = 32.109943429693786\nIteration 5905: Loss = 32.108645969715695\nIteration 5906: Loss = 32.10734861248942\nIteration 5907: Loss = 32.1060513579599\nIteration 5908: Loss = 32.10475420607216\nIteration 5909: Loss = 32.10345715677125\nIteration 5910: Loss = 32.102160210002346\nIteration 5911: Loss = 32.100863365710644\nIteration 5912: Loss = 32.09956662384143\nIteration 5913: Loss = 32.098269984340035\nIteration 5914: Loss = 32.09697344715186\nIteration 5915: Loss = 32.095677012222396\nIteration 5916: Loss = 32.09438067949718\nIteration 5917: Loss = 32.093084448921786\nIteration 5918: Loss = 32.0917883204419\nIteration 5919: Loss = 32.09049229400327\nIteration 5920: Loss = 32.089196369551665\nIteration 5921: Loss = 32.087900547032966\nIteration 5922: Loss = 32.08660482639309\nIteration 5923: Loss = 32.08530920757804\nIteration 5924: Loss = 32.08401369053387\nIteration 5925: Loss = 32.082718275206695\nIteration 5926: Loss = 32.0814229615427\nIteration 5927: Loss = 32.08012774948815\nIteration 5928: Loss = 32.07883263898935\nIteration 5929: Loss = 32.07753762999266\nIteration 5930: Loss = 32.076242722444555\nIteration 5931: Loss = 32.074947916291514\nIteration 5932: Loss = 32.07365321148013\nIteration 5933: Loss = 32.07235860795702\nIteration 5934: Loss = 32.07106410566889\nIteration 5935: Loss = 32.06976970456251\nIteration 5936: Loss = 32.06847540458469\nIteration 5937: Loss = 32.067181205682324\nIteration 5938: Loss = 32.065887107802375\nIteration 5939: Loss = 32.06459311089185\nIteration 5940: Loss = 32.06329921489783\nIteration 5941: Loss = 32.06200541976745\nIteration 5942: Loss = 32.06071172544793\nIteration 5943: Loss = 32.059418131886524\nIteration 5944: Loss = 32.05812463903058\nIteration 5945: Loss = 32.05683124682748\nIteration 5946: Loss = 32.055537955224686\nIteration 5947: Loss = 32.05424476416971\nIteration 5948: Loss = 32.05295167361015\nIteration 5949: Loss = 32.05165868349364\nIteration 5950: Loss = 32.05036579376789\nIteration 5951: Loss = 32.049073004380666\nIteration 5952: Loss = 32.047780315279816\nIteration 5953: Loss = 32.04648772641322\nIteration 5954: Loss = 32.04519523772884\nIteration 5955: Loss = 32.04390284917469\nIteration 5956: Loss = 32.04261056069886\nIteration 5957: Loss = 32.04131837224949\nIteration 5958: Loss = 32.04002628377478\nIteration 5959: Loss = 32.038734295223\nIteration 5960: Loss = 32.037442406542475\nIteration 5961: Loss = 32.0361506176816\nIteration 5962: Loss = 32.034858928588825\nIteration 5963: Loss = 32.03356733921266\nIteration 5964: Loss = 32.03227584950168\nIteration 5965: Loss = 32.03098445940452\nIteration 5966: Loss = 32.02969316886988\nIteration 5967: Loss = 32.02840197784652\nIteration 5968: Loss = 32.027110886283246\nIteration 5969: Loss = 32.02581989412896\nIteration 5970: Loss = 32.02452900133257\nIteration 5971: Loss = 32.02323820784311\nIteration 5972: Loss = 32.02194751360964\nIteration 5973: Loss = 32.020656918581246\nIteration 5974: Loss = 32.019366422707144\nIteration 5975: Loss = 32.01807602593659\nIteration 5976: Loss = 32.01678572821884\nIteration 5977: Loss = 32.015495529503305\nIteration 5978: Loss = 32.01420542973939\nIteration 5979: Loss = 32.012915428876596\nIteration 5980: Loss = 32.01162552686444\nIteration 5981: Loss = 32.01033572365255\nIteration 5982: Loss = 32.009046019190585\nIteration 5983: Loss = 32.007756413428254\nIteration 5984: Loss = 32.00646690631537\nIteration 5985: Loss = 32.00517749780177\nIteration 5986: Loss = 32.00388818783735\nIteration 5987: Loss = 32.00259897637207\nIteration 5988: Loss = 32.001309863355964\nIteration 5989: Loss = 32.00002084873912\nIteration 5990: Loss = 31.998731932471664\nIteration 5991: Loss = 31.997443114503806\nIteration 5992: Loss = 31.996154394785805\nIteration 5993: Loss = 31.994865773267982\nIteration 5994: Loss = 31.993577249900717\nIteration 5995: Loss = 31.99228882463444\nIteration 5996: Loss = 31.99100049741966\nIteration 5997: Loss = 31.989712268206915\nIteration 5998: Loss = 31.98842413694684\nIteration 5999: Loss = 31.987136103590103\nIteration 6000: Loss = 31.985848168087415\nIteration 6001: Loss = 31.984560330389588\nIteration 6002: Loss = 31.98327259044746\nIteration 6003: Loss = 31.98198494821194\nIteration 6004: Loss = 31.98069740363401\nIteration 6005: Loss = 31.97940995666465\nIteration 6006: Loss = 31.97812260725499\nIteration 6007: Loss = 31.976835355356133\nIteration 6008: Loss = 31.97554820091929\nIteration 6009: Loss = 31.974261143895728\nIteration 6010: Loss = 31.97297418423674\nIteration 6011: Loss = 31.971687321893704\nIteration 6012: Loss = 31.97040055681806\nIteration 6013: Loss = 31.969113888961267\nIteration 6014: Loss = 31.967827318274903\nIteration 6015: Loss = 31.966540844710533\nIteration 6016: Loss = 31.96525446821984\nIteration 6017: Loss = 31.963968188754535\nIteration 6018: Loss = 31.962682006266387\nIteration 6019: Loss = 31.961395920707236\nIteration 6020: Loss = 31.960109932028953\nIteration 6021: Loss = 31.95882404018349\nIteration 6022: Loss = 31.957538245122848\nIteration 6023: Loss = 31.95625254679909\nIteration 6024: Loss = 31.954966945164315\nIteration 6025: Loss = 31.953681440170715\nIteration 6026: Loss = 31.952396031770512\nIteration 6027: Loss = 31.95111071991598\nIteration 6028: Loss = 31.949825504559474\nIteration 6029: Loss = 31.94854038565338\nIteration 6030: Loss = 31.947255363150152\nIteration 6031: Loss = 31.945970437002313\nIteration 6032: Loss = 31.94468560716242\nIteration 6033: Loss = 31.94340087358309\nIteration 6034: Loss = 31.94211623621702\nIteration 6035: Loss = 31.940831695016925\nIteration 6036: Loss = 31.939547249935607\nIteration 6037: Loss = 31.938262900925913\nIteration 6038: Loss = 31.93697864794073\nIteration 6039: Loss = 31.93569449093305\nIteration 6040: Loss = 31.93441042985584\nIteration 6041: Loss = 31.933126464662198\nIteration 6042: Loss = 31.931842595305245\nIteration 6043: Loss = 31.93055882173816\nIteration 6044: Loss = 31.92927514391418\nIteration 6045: Loss = 31.92799156178659\nIteration 6046: Loss = 31.92670807530874\nIteration 6047: Loss = 31.925424684434034\nIteration 6048: Loss = 31.92414138911592\nIteration 6049: Loss = 31.92285818930791\nIteration 6050: Loss = 31.921575084963575\nIteration 6051: Loss = 31.920292076036528\nIteration 6052: Loss = 31.919009162480453\nIteration 6053: Loss = 31.917726344249083\nIteration 6054: Loss = 31.916443621296185\nIteration 6055: Loss = 31.91516099357561\nIteration 6056: Loss = 31.913878461041268\nIteration 6057: Loss = 31.91259602364708\nIteration 6058: Loss = 31.911313681347064\nIteration 6059: Loss = 31.910031434095263\nIteration 6060: Loss = 31.9087492818458\nIteration 6061: Loss = 31.90746722455285\nIteration 6062: Loss = 31.90618526217061\nIteration 6063: Loss = 31.904903394653363\nIteration 6064: Loss = 31.903621621955445\nIteration 6065: Loss = 31.902339944031223\nIteration 6066: Loss = 31.901058360835133\nIteration 6067: Loss = 31.899776872321677\nIteration 6068: Loss = 31.89849547844539\nIteration 6069: Loss = 31.897214179160862\nIteration 6070: Loss = 31.89593297442275\nIteration 6071: Loss = 31.894651864185754\nIteration 6072: Loss = 31.893370848404622\nIteration 6073: Loss = 31.89208992703418\nIteration 6074: Loss = 31.890809100029283\nIteration 6075: Loss = 31.88952836734484\nIteration 6076: Loss = 31.88824772893583\nIteration 6077: Loss = 31.886967184757275\nIteration 6078: Loss = 31.885686734764246\nIteration 6079: Loss = 31.884406378911873\nIteration 6080: Loss = 31.883126117155335\nIteration 6081: Loss = 31.881845949449858\nIteration 6082: Loss = 31.880565875750744\nIteration 6083: Loss = 31.87928589601333\nIteration 6084: Loss = 31.87800601019299\nIteration 6085: Loss = 31.876726218245185\nIteration 6086: Loss = 31.875446520125408\nIteration 6087: Loss = 31.874166915789203\nIteration 6088: Loss = 31.87288740519218\nIteration 6089: Loss = 31.871607988289977\nIteration 6090: Loss = 31.87032866503831\nIteration 6091: Loss = 31.869049435392938\nIteration 6092: Loss = 31.86777029930967\nIteration 6093: Loss = 31.866491256744357\nIteration 6094: Loss = 31.865212307652918\nIteration 6095: Loss = 31.863933451991333\nIteration 6096: Loss = 31.862654689715598\nIteration 6097: Loss = 31.861376020781794\nIteration 6098: Loss = 31.860097445146028\nIteration 6099: Loss = 31.85881896276448\nIteration 6100: Loss = 31.857540573593386\nIteration 6101: Loss = 31.856262277588996\nIteration 6102: Loss = 31.85498407470766\nIteration 6103: Loss = 31.85370596490574\nIteration 6104: Loss = 31.852427948139667\nIteration 6105: Loss = 31.85115002436592\nIteration 6106: Loss = 31.849872193541028\nIteration 6107: Loss = 31.848594455621576\nIteration 6108: Loss = 31.847316810564205\nIteration 6109: Loss = 31.846039258325593\nIteration 6110: Loss = 31.844761798862468\n",
      "Iteration 6111: Loss = 31.843484432131614\nIteration 6112: Loss = 31.84220715808988\nIteration 6113: Loss = 31.84092997669413\nIteration 6114: Loss = 31.839652887901334\nIteration 6115: Loss = 31.838375891668456\nIteration 6116: Loss = 31.837098987952537\nIteration 6117: Loss = 31.83582217671066\nIteration 6118: Loss = 31.83454545789998\nIteration 6119: Loss = 31.833268831477685\nIteration 6120: Loss = 31.83199229740099\nIteration 6121: Loss = 31.830715855627215\nIteration 6122: Loss = 31.829439506113676\nIteration 6123: Loss = 31.828163248817773\nIteration 6124: Loss = 31.82688708369694\nIteration 6125: Loss = 31.825611010708673\nIteration 6126: Loss = 31.82433502981052\nIteration 6127: Loss = 31.823059140960034\nIteration 6128: Loss = 31.821783344114884\nIteration 6129: Loss = 31.820507639232755\nIteration 6130: Loss = 31.819232026271376\nIteration 6131: Loss = 31.817956505188533\nIteration 6132: Loss = 31.81668107594206\nIteration 6133: Loss = 31.815405738489865\nIteration 6134: Loss = 31.814130492789854\nIteration 6135: Loss = 31.81285533880003\nIteration 6136: Loss = 31.811580276478406\nIteration 6137: Loss = 31.81030530578309\nIteration 6138: Loss = 31.809030426672198\nIteration 6139: Loss = 31.807755639103913\nIteration 6140: Loss = 31.80648094303646\nIteration 6141: Loss = 31.805206338428125\nIteration 6142: Loss = 31.803931825237225\nIteration 6143: Loss = 31.80265740342214\nIteration 6144: Loss = 31.8013830729413\nIteration 6145: Loss = 31.80010883375317\nIteration 6146: Loss = 31.79883468581628\nIteration 6147: Loss = 31.797560629089187\nIteration 6148: Loss = 31.79628666353052\nIteration 6149: Loss = 31.795012789098937\nIteration 6150: Loss = 31.793739005753157\nIteration 6151: Loss = 31.79246531345195\nIteration 6152: Loss = 31.791191712154113\nIteration 6153: Loss = 31.789918201818498\nIteration 6154: Loss = 31.788644782404035\nIteration 6155: Loss = 31.78737145386967\nIteration 6156: Loss = 31.786098216174416\nIteration 6157: Loss = 31.784825069277296\nIteration 6158: Loss = 31.783552013137434\nIteration 6159: Loss = 31.782279047713963\nIteration 6160: Loss = 31.781006172966087\nIteration 6161: Loss = 31.779733388853035\nIteration 6162: Loss = 31.778460695334115\nIteration 6163: Loss = 31.777188092368647\nIteration 6164: Loss = 31.775915579916003\nIteration 6165: Loss = 31.774643157935646\nIteration 6166: Loss = 31.77337082638704\nIteration 6167: Loss = 31.772098585229703\nIteration 6168: Loss = 31.770826434423224\nIteration 6169: Loss = 31.769554373927203\nIteration 6170: Loss = 31.768282403701324\nIteration 6171: Loss = 31.76701052370529\nIteration 6172: Loss = 31.765738733898875\nIteration 6173: Loss = 31.764467034241875\nIteration 6174: Loss = 31.763195424694146\nIteration 6175: Loss = 31.761923905215593\nIteration 6176: Loss = 31.76065247576616\nIteration 6177: Loss = 31.75938113630586\nIteration 6178: Loss = 31.7581098867947\nIteration 6179: Loss = 31.75683872719279\nIteration 6180: Loss = 31.75556765746027\nIteration 6181: Loss = 31.7542966775573\nIteration 6182: Loss = 31.753025787444116\nIteration 6183: Loss = 31.75175498708099\nIteration 6184: Loss = 31.750484276428256\nIteration 6185: Loss = 31.749213655446265\nIteration 6186: Loss = 31.74794312409542\nIteration 6187: Loss = 31.746672682336204\nIteration 6188: Loss = 31.745402330129107\nIteration 6189: Loss = 31.744132067434663\nIteration 6190: Loss = 31.742861894213487\nIteration 6191: Loss = 31.74159181042622\nIteration 6192: Loss = 31.740321816033532\nIteration 6193: Loss = 31.739051910996178\nIteration 6194: Loss = 31.737782095274923\nIteration 6195: Loss = 31.736512368830592\nIteration 6196: Loss = 31.73524273162405\nIteration 6197: Loss = 31.733973183616218\nIteration 6198: Loss = 31.732703724768047\nIteration 6199: Loss = 31.73143435504056\nIteration 6200: Loss = 31.730165074394797\nIteration 6201: Loss = 31.72889588279185\nIteration 6202: Loss = 31.727626780192853\nIteration 6203: Loss = 31.726357766559016\nIteration 6204: Loss = 31.725088841851555\nIteration 6205: Loss = 31.723820006031733\nIteration 6206: Loss = 31.722551259060893\nIteration 6207: Loss = 31.72128260090039\nIteration 6208: Loss = 31.720014031511635\nIteration 6209: Loss = 31.71874555085609\nIteration 6210: Loss = 31.717477158895246\nIteration 6211: Loss = 31.71620885559065\nIteration 6212: Loss = 31.714940640903894\nIteration 6213: Loss = 31.713672514796603\nIteration 6214: Loss = 31.712404477230464\nIteration 6215: Loss = 31.711136528167202\nIteration 6216: Loss = 31.709868667568575\nIteration 6217: Loss = 31.70860089539639\nIteration 6218: Loss = 31.707333211612518\nIteration 6219: Loss = 31.706065616178847\nIteration 6220: Loss = 31.70479810905732\nIteration 6221: Loss = 31.70353069020993\nIteration 6222: Loss = 31.702263359598703\nIteration 6223: Loss = 31.70099611718572\nIteration 6224: Loss = 31.699728962933097\nIteration 6225: Loss = 31.698461896802996\nIteration 6226: Loss = 31.697194918757628\nIteration 6227: Loss = 31.695928028759237\nIteration 6228: Loss = 31.69466122677013\nIteration 6229: Loss = 31.69339451275263\nIteration 6230: Loss = 31.69212788666912\nIteration 6231: Loss = 31.690861348482034\nIteration 6232: Loss = 31.689594898153842\nIteration 6233: Loss = 31.688328535647035\nIteration 6234: Loss = 31.6870622609242\nIteration 6235: Loss = 31.685796073947905\nIteration 6236: Loss = 31.68452997468081\nIteration 6237: Loss = 31.68326396308559\nIteration 6238: Loss = 31.68199803912497\nIteration 6239: Loss = 31.680732202761735\nIteration 6240: Loss = 31.6794664539587\nIteration 6241: Loss = 31.678200792678695\nIteration 6242: Loss = 31.67693521888465\nIteration 6243: Loss = 31.67566973253948\nIteration 6244: Loss = 31.674404333606187\nIteration 6245: Loss = 31.67313902204781\nIteration 6246: Loss = 31.671873797827388\nIteration 6247: Loss = 31.670608660908073\nIteration 6248: Loss = 31.669343611252984\nIteration 6249: Loss = 31.668078648825336\nIteration 6250: Loss = 31.666813773588373\nIteration 6251: Loss = 31.665548985505364\nIteration 6252: Loss = 31.664284284539647\nIteration 6253: Loss = 31.66301967065458\nIteration 6254: Loss = 31.661755143813586\nIteration 6255: Loss = 31.660490703980106\nIteration 6256: Loss = 31.659226351117635\nIteration 6257: Loss = 31.657962085189713\nIteration 6258: Loss = 31.656697906159913\nIteration 6259: Loss = 31.655433813991866\nIteration 6260: Loss = 31.654169808649215\nIteration 6261: Loss = 31.652905890095674\nIteration 6262: Loss = 31.651642058294996\nIteration 6263: Loss = 31.650378313210958\nIteration 6264: Loss = 31.649114654807395\nIteration 6265: Loss = 31.647851083048174\nIteration 6266: Loss = 31.646587597897195\nIteration 6267: Loss = 31.64532419931844\nIteration 6268: Loss = 31.64406088727588\nIteration 6269: Loss = 31.642797661733557\nIteration 6270: Loss = 31.641534522655554\nIteration 6271: Loss = 31.64027147000599\nIteration 6272: Loss = 31.63900850374903\nIteration 6273: Loss = 31.637745623848858\nIteration 6274: Loss = 31.63648283026972\nIteration 6275: Loss = 31.635220122975912\nIteration 6276: Loss = 31.633957501931754\nIteration 6277: Loss = 31.632694967101617\nIteration 6278: Loss = 31.6314325184499\nIteration 6279: Loss = 31.630170155941048\nIteration 6280: Loss = 31.628907879539558\nIteration 6281: Loss = 31.62764568920995\nIteration 6282: Loss = 31.6263835849168\nIteration 6283: Loss = 31.625121566624717\nIteration 6284: Loss = 31.623859634298356\nIteration 6285: Loss = 31.6225977879024\nIteration 6286: Loss = 31.621336027401593\nIteration 6287: Loss = 31.620074352760703\nIteration 6288: Loss = 31.61881276394454\nIteration 6289: Loss = 31.617551260917953\nIteration 6290: Loss = 31.61628984364585\nIteration 6291: Loss = 31.61502851209315\nIteration 6292: Loss = 31.61376726622483\nIteration 6293: Loss = 31.612506106005917\nIteration 6294: Loss = 31.611245031401452\nIteration 6295: Loss = 31.609984042376546\nIteration 6296: Loss = 31.608723138896313\nIteration 6297: Loss = 31.607462320925936\nIteration 6298: Loss = 31.606201588430626\nIteration 6299: Loss = 31.604940941375652\nIteration 6300: Loss = 31.60368037972629\nIteration 6301: Loss = 31.60241990344787\nIteration 6302: Loss = 31.601159512505806\nIteration 6303: Loss = 31.59989920686545\nIteration 6304: Loss = 31.598638986492304\nIteration 6305: Loss = 31.597378851351838\nIteration 6306: Loss = 31.596118801409585\nIteration 6307: Loss = 31.594858836631115\nIteration 6308: Loss = 31.59359895698205\nIteration 6309: Loss = 31.592339162428015\nIteration 6310: Loss = 31.591079452934732\nIteration 6311: Loss = 31.589819828467913\nIteration 6312: Loss = 31.588560288993317\nIteration 6313: Loss = 31.587300834476764\nIteration 6314: Loss = 31.586041464884104\nIteration 6315: Loss = 31.584782180181197\nIteration 6316: Loss = 31.58352298033398\nIteration 6317: Loss = 31.58226386530843\nIteration 6318: Loss = 31.58100483507053\nIteration 6319: Loss = 31.579745889586317\nIteration 6320: Loss = 31.578487028821897\nIteration 6321: Loss = 31.57722825274336\nIteration 6322: Loss = 31.57596956131688\nIteration 6323: Loss = 31.574710954508635\nIteration 6324: Loss = 31.57345243228488\nIteration 6325: Loss = 31.57219399461187\nIteration 6326: Loss = 31.57093564145591\nIteration 6327: Loss = 31.569677372783367\nIteration 6328: Loss = 31.568419188560625\nIteration 6329: Loss = 31.567161088754105\nIteration 6330: Loss = 31.565903073330283\nIteration 6331: Loss = 31.564645142255635\nIteration 6332: Loss = 31.563387295496725\nIteration 6333: Loss = 31.562129533020116\nIteration 6334: Loss = 31.560871854792442\nIteration 6335: Loss = 31.55961426078034\nIteration 6336: Loss = 31.558356750950523\nIteration 6337: Loss = 31.5570993252697\nIteration 6338: Loss = 31.555841983704656\nIteration 6339: Loss = 31.554584726222185\nIteration 6340: Loss = 31.553327552789135\nIteration 6341: Loss = 31.552070463372395\nIteration 6342: Loss = 31.55081345793888\nIteration 6343: Loss = 31.549556536455544\nIteration 6344: Loss = 31.548299698889384\nIteration 6345: Loss = 31.54704294520743\nIteration 6346: Loss = 31.545786275376763\nIteration 6347: Loss = 31.544529689364477\nIteration 6348: Loss = 31.54327318713772\nIteration 6349: Loss = 31.54201676866369\nIteration 6350: Loss = 31.540760433909572\nIteration 6351: Loss = 31.539504182842656\nIteration 6352: Loss = 31.53824801543022\nIteration 6353: Loss = 31.536991931639605\nIteration 6354: Loss = 31.535735931438165\nIteration 6355: Loss = 31.53448001479331\nIteration 6356: Loss = 31.53322418167249\nIteration 6357: Loss = 31.531968432043172\nIteration 6358: Loss = 31.530712765872888\nIteration 6359: Loss = 31.529457183129182\nIteration 6360: Loss = 31.52820168377964\nIteration 6361: Loss = 31.526946267791896\nIteration 6362: Loss = 31.525690935133603\nIteration 6363: Loss = 31.524435685772477\nIteration 6364: Loss = 31.52318051967624\nIteration 6365: Loss = 31.52192543681267\nIteration 6366: Loss = 31.520670437149587\nIteration 6367: Loss = 31.51941552065482\nIteration 6368: Loss = 31.518160687296266\nIteration 6369: Loss = 31.516905937041837\nIteration 6370: Loss = 31.515651269859482\nIteration 6371: Loss = 31.514396685717212\nIteration 6372: Loss = 31.513142184583042\nIteration 6373: Loss = 31.511887766425033\nIteration 6374: Loss = 31.510633431211303\nIteration 6375: Loss = 31.50937917890997\nIteration 6376: Loss = 31.50812500948921\nIteration 6377: Loss = 31.506870922917244\nIteration 6378: Loss = 31.5056169191623\nIteration 6379: Loss = 31.504362998192665\nIteration 6380: Loss = 31.503109159976667\nIteration 6381: Loss = 31.501855404482647\nIteration 6382: Loss = 31.50060173167899\nIteration 6383: Loss = 31.49934814153413\nIteration 6384: Loss = 31.498094634016514\nIteration 6385: Loss = 31.496841209094658\nIteration 6386: Loss = 31.495587866737075\nIteration 6387: Loss = 31.494334606912336\nIteration 6388: Loss = 31.493081429589047\nIteration 6389: Loss = 31.49182833473584\nIteration 6390: Loss = 31.49057532232139\nIteration 6391: Loss = 31.48932239231441\nIteration 6392: Loss = 31.488069544683633\nIteration 6393: Loss = 31.486816779397852\nIteration 6394: Loss = 31.48556409642587\nIteration 6395: Loss = 31.48431149573655\nIteration 6396: Loss = 31.48305897729876\nIteration 6397: Loss = 31.481806541081433\nIteration 6398: Loss = 31.480554187053514\nIteration 6399: Loss = 31.479301915184\nIteration 6400: Loss = 31.478049725441913\nIteration 6401: Loss = 31.476797617796304\nIteration 6402: Loss = 31.475545592216292\nIteration 6403: Loss = 31.474293648670987\nIteration 6404: Loss = 31.473041787129556\nIteration 6405: Loss = 31.471790007561196\nIteration 6406: Loss = 31.470538309935147\nIteration 6407: Loss = 31.469286694220678\nIteration 6408: Loss = 31.468035160387096\nIteration 6409: Loss = 31.466783708403728\nIteration 6410: Loss = 31.465532338239946\nIteration 6411: Loss = 31.464281049865168\nIteration 6412: Loss = 31.463029843248826\nIteration 6413: Loss = 31.461778718360392\nIteration 6414: Loss = 31.460527675169395\nIteration 6415: Loss = 31.45927671364536\nIteration 6416: Loss = 31.458025833757873\nIteration 6417: Loss = 31.456775035476543\nIteration 6418: Loss = 31.45552431877103\nIteration 6419: Loss = 31.454273683611007\nIteration 6420: Loss = 31.453023129966173\nIteration 6421: Loss = 31.451772657806302\nIteration 6422: Loss = 31.45052226710117\nIteration 6423: Loss = 31.44927195782058\nIteration 6424: Loss = 31.448021729934403\nIteration 6425: Loss = 31.446771583412513\nIteration 6426: Loss = 31.44552151822483\nIteration 6427: Loss = 31.44427153434131\nIteration 6428: Loss = 31.443021631731938\nIteration 6429: Loss = 31.44177181036674\nIteration 6430: Loss = 31.440522070215753\nIteration 6431: Loss = 31.43927241124908\nIteration 6432: Loss = 31.43802283343684\nIteration 6433: Loss = 31.43677333674918\nIteration 6434: Loss = 31.435523921156292\nIteration 6435: Loss = 31.434274586628405\nIteration 6436: Loss = 31.433025333135756\nIteration 6437: Loss = 31.43177616064866\nIteration 6438: Loss = 31.43052706913741\nIteration 6439: Loss = 31.429278058572383\nIteration 6440: Loss = 31.428029128923946\nIteration 6441: Loss = 31.426780280162532\nIteration 6442: Loss = 31.4255315122586\nIteration 6443: Loss = 31.424282825182637\nIteration 6444: Loss = 31.42303421890515\nIteration 6445: Loss = 31.421785693396707\nIteration 6446: Loss = 31.420537248627895\nIteration 6447: Loss = 31.41928888456931\nIteration 6448: Loss = 31.41804060119164\nIteration 6449: Loss = 31.416792398465542\nIteration 6450: Loss = 31.41554427636175\nIteration 6451: Loss = 31.414296234851005\nIteration 6452: Loss = 31.413048273904092\nIteration 6453: Loss = 31.411800393491834\nIteration 6454: Loss = 31.410552593585074\nIteration 6455: Loss = 31.409304874154692\nIteration 6456: Loss = 31.408057235171604\nIteration 6457: Loss = 31.406809676606766\nIteration 6458: Loss = 31.405562198431138\nIteration 6459: Loss = 31.40431480061575\nIteration 6460: Loss = 31.40306748313163\nIteration 6461: Loss = 31.40182024594987\nIteration 6462: Loss = 31.400573089041565\nIteration 6463: Loss = 31.399326012377863\nIteration 6464: Loss = 31.39807901592992\nIteration 6465: Loss = 31.39683209966897\nIteration 6466: Loss = 31.39558526356623\nIteration 6467: Loss = 31.39433850759298\nIteration 6468: Loss = 31.39309183172051\nIteration 6469: Loss = 31.391845235920165\nIteration 6470: Loss = 31.390598720163297\nIteration 6471: Loss = 31.389352284421317\nIteration 6472: Loss = 31.38810592866565\nIteration 6473: Loss = 31.38685965286775\nIteration 6474: Loss = 31.385613456999124\nIteration 6475: Loss = 31.38436734103128\nIteration 6476: Loss = 31.383121304935784\nIteration 6477: Loss = 31.381875348684222\nIteration 6478: Loss = 31.38062947224822\nIteration 6479: Loss = 31.37938367559942\nIteration 6480: Loss = 31.378137958709498\nIteration 6481: Loss = 31.376892321550187\nIteration 6482: Loss = 31.37564676409323\nIteration 6483: Loss = 31.374401286310384\nIteration 6484: Loss = 31.373155888173482\nIteration 6485: Loss = 31.371910569654357\nIteration 6486: Loss = 31.370665330724872\nIteration 6487: Loss = 31.36942017135694\nIteration 6488: Loss = 31.368175091522488\nIteration 6489: Loss = 31.36693009119348\nIteration 6490: Loss = 31.365685170341923\nIteration 6491: Loss = 31.36444032893983\nIteration 6492: Loss = 31.363195566959266\nIteration 6493: Loss = 31.36195088437233\nIteration 6494: Loss = 31.36070628115113\nIteration 6495: Loss = 31.359461757267812\nIteration 6496: Loss = 31.358217312694585\nIteration 6497: Loss = 31.35697294740363\nIteration 6498: Loss = 31.35572866136722\nIteration 6499: Loss = 31.354484454557596\nIteration 6500: Loss = 31.353240326947102\n[[307.13984848]\n [274.85374396]\n [247.35694147]\n ...\n [ 31.35572866]\n [ 31.35448445]\n [ 31.35324033]]\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "# Napomena: za broj za num_iter <= 6000 se dobija manja predikcija nego sto bi trebalo,\n",
    "# a za num_iter >= 7000 veca. Pretpostavljam da to moze da bude samo iz dva razloga:\n",
    "#     1) Korak alpha je veliki i nakon dolaska do minimuma on ga preskace i nastavlja\n",
    "#        dalje sa pretragom, zbog cega se napusta lokalni optimum\n",
    "#     2) underfit/overfit\n",
    "alpha = 10e-8 * 2\n",
    "num_iter = 6500\n",
    "eps = 10e-5\n",
    "\n",
    "w, loss_history, it_break = gradient_descent(X, y, w_init, alpha, num_iter, eps)\n",
    "print(loss_history)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [],
   "source": [
    "w_analytic = train_analytic(X, y)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Trained model:\nf_w(x) = 0.002 + 0.129*x0 + 0.003*x1 + -0.152*x2 + 0.01*x3 + 0.016*x4 + 0.041*x5\nTrained model (analytic):\nf_w(x) = -0.0 + 1.0*x0 + 0.0*x1 + -0.0*x2 + 0.0*x3 + 0.0*x4 + 0.0*x5"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "print(f'Trained model:')\n",
    "show_model(w)\n",
    "\n",
    "print()\n",
    "print(f'Trained model (analytic):')\n",
    "show_model(w_analytic)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(range(it_break), loss_history[:it_break, 0], color='aqua')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Loss')\n",
    "\n",
    "plt.title('Loss function with respect to number of iterations')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [],
   "source": [
    "cylinders = int(input('Cylinders: '))\n",
    "displacement = float(input('Displacement: '))\n",
    "horsepower = float(input('Horsepower: '))\n",
    "weight = float(input('Weight: '))\n",
    "acceleration = float(input('Acceleration: '))\n",
    "model_year = float(input('Model-year: '))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Input instance:\n[[1.000e+00]\n [8.000e+00]\n [3.100e+02]\n [1.300e+02]\n [3.515e+03]\n [1.210e+01]\n [7.000e+01]]\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "x = np.array([1, cylinders, displacement, horsepower, \n",
    "              weight, acceleration, model_year]).reshape(-1, 1)\n",
    "print('Input instance:')\n",
    "print(x)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Mpg = 21.94899728482704 (gradient descent)\nMpg = 8.00000002056549 (analytic)\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "predicted_mpg_gd = hypothesis(w, x)\n",
    "predicted_mpg_analytic = hypothesis(w_analytic, x)\n",
    "\n",
    "print(f'Mpg = {predicted_mpg_gd} (gradient descent)')\n",
    "print(f'Mpg = {predicted_mpg_analytic} (analytic)')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}